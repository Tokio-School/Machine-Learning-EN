{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa1e653-43cc-498e-89d7-fcd9d96454bd",
   "metadata": {},
   "source": [
    "# Detection of Anomalies: Synthetic datasets\n",
    "M041- Exercise 1\n",
    "\n",
    "## What are we going to do?\n",
    "- We will create a dataset for anomaly detection with normal and abnormal cases\n",
    "- We will model a Gaussian distribution on the normal data\n",
    "- Using validation, we will determine the probability threshold for detecting outliers\n",
    "- We will evaluate the final accuracy of the model on the test subset\n",
    "- We will graphically represent the behaviour of the model at each step\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fa1b3-a351-4231-8c86-ae3da364eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use this cell to import all the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import from_levels_and_colors\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plot_n = 1\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5fc30-05e3-47cf-b356-6c4d456f6289",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset for detection of anomalies\n",
    "\n",
    "To solve this exercise, we first need to create a dataset with normal data and another with anomalous data (outliers). In this case, the datasets will be 2D with only 2 features, instead of a large number of n features, to facilitate visualising them in a 2D representation.\n",
    "\n",
    "Initially we are going to create 2 independent datasets, one representing normal data and the other representing outliers. We will then combine them into 3 final subsets of training, validation, and test, as usual, with the particularity that in this case the anomalous data will only be distributed in the validation and test subsets.\n",
    "\n",
    "Complete the following cell to create separate initial datasets with normal data and outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d9f3f-f4a9-48b5-bc91-e539f54164d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate two independent synthetic datasets with normal and outlier data\n",
    "\n",
    "m = 300\n",
    "n = 2\n",
    "outliers_ratio = 0.15    # Percentage of outliers vs. normal data, modifiable\n",
    "m_outliers = int(m * outliers_ratio)\n",
    "m_normal = m - m_outliers\n",
    "x_lim = (-5, 5)\n",
    "y_lim = (-5, 5)\n",
    "\n",
    "print('No. of examples: {}, ratio of anomalous examples: {}%, no. of normal and anomalous data: {}/{}'.format(m, outliers_ratio * 100, m_normal, m_outliers))\n",
    "print('Number of features: {}'.format(n))\n",
    "\n",
    "# Create both datasets\n",
    "normal_dataset = make_blobs(n_samples=m_normal, centers=np.array([[1.5, 1.5]]), cluster_std=1.0, random_state=42)\n",
    "normal_dataset = normal_dataset[0]    # We discard the rest of the information and retain only the positions of the examples\n",
    "outliers_dataset = np.random.uniform(low=(x_lim[0], y_lim[0]), high=(x_lim[1], y_lim[1]), size=(m_outliers, 2))\n",
    "\n",
    "# We plot the initial data\n",
    "plt.figure(plot_n)\n",
    "\n",
    "plt.title('Original dataset: normal data and outliers')\n",
    "\n",
    "plt.scatter(normal_dataset[:, 0], normal_dataset[:, 1], s=10, color='b')\n",
    "plt.scatter(outliers_dataset[:, 0], outliers_dataset[:, 1], s=10, colour='r')\n",
    "\n",
    "plt.xlim(x_lim)\n",
    "plt.ylim(y_lim)\n",
    "plt.legend(('Normal', ‘Outliers'))\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22baf4a-d6ab-4cd7-9072-9ffafca2b219",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "### Normalisation\n",
    "\n",
    "Before we continue, let's preprocess the data by normalisation, as we usually do. Since our *X* will be composed of both normal and outlier data, we will normalise them at the same time.\n",
    "\n",
    "In this case, we do not insert a bias first column of 1s to the dataset, so we normalise all the columns.\n",
    "\n",
    "Complete the following code cell to normalise the data, retrieving your normalisation function from previous exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd4008-bf7d-42a0-90e4-9bc4f1317923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the data of both datasets with the same normalisation parameters\n",
    "\n",
    "def normalize(x, mu, std):\n",
    "    return [...]\n",
    "\n",
    "# Find the mean and standard deviation of the features of the original datasets\n",
    "# Concatenate both datasets into a common X beforehand, making sure to use the correct axis\n",
    "X = [...]\n",
    "mu_normalize = [...]\n",
    "std = [...]\n",
    "\n",
    "print('Original datasets:')\n",
    "print(normal_dataset.shape, outliers_dataset.shape)\n",
    "\n",
    "print('Mean and standard deviation of the features:')\n",
    "print(mu_normalize)\n",
    "print(mu_normalize.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('Normalized datasets:')\n",
    "normal_dataset_norm = normalize(normal_dataset, mu_normalize, std)\n",
    "outliers_dataset_norm = normalize(outliers_dataset, mu_normalize, std)\n",
    "\n",
    "print(normal_dataset_norm.shape)\n",
    "print(outliers_dataset_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8812f-ee9a-4d7c-beef-828a96530121",
   "metadata": {},
   "source": [
    "### Divide the dataset into training, validation, and test subsets\n",
    "\n",
    "We will now subdivide the original datasets into training, validation, and test subsets.\n",
    "\n",
    "To do this, we split the normal data dataset according to the usual ratios, and assign half of the anomalous data to the validation and test subsets.\n",
    "\n",
    "Complete the following code cell to create these subsets:\n",
    "\n",
    "*NOTE:* Remember that you can do this manually or with Scikit-learn methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef12f5-cbdd-4c0b-bf75-7f09e7edd453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the datasets into training, validation, and test subsets with the normal and outlier data divided between the latter 2 subsets\n",
    "\n",
    "ratios = [66, 33, 33]\n",
    "print('Ratios:\\n', ratios, ratios[0] + ratios[1] + ratios[2])\n",
    "\n",
    "r = [0, 0]\n",
    "# Tip: the round() function and the x.shape attribute can be useful to you\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('Cutoff rates:\\n', r)\n",
    "\n",
    "# Split the normal data dataset into the 3 subsets following the indicated ratios\n",
    "X_train, X_val, X_test = [...]\n",
    "\n",
    "# Assign the label Y = 0 to all the data from the normal data dataset and Y = 1 to the outliers\n",
    "# Create 1D arrays whose length corresponds to the number of examples in each subset with the value of 0.\n",
    "Y_train = [...]\n",
    "\n",
    "# Now concatenate half of the anomalous data to the validation subset and the other half to the test subset\n",
    "val_outliers_dataset, test_outliers_dataset = [...]\n",
    "\n",
    "X_val = [...]\n",
    "X_test = [...]\n",
    "# The final result for X_val and X_test will be 2D vectors of (m_normals * ratio [validation or test] + m_outliers / 2, n)\n",
    "\n",
    "# Finally, as we have done before, concatenate to Y_val and Y_test a 1D array with length corresponding to the number of anomalous examples in each subset (half of m_outliers)\n",
    "# Each array, this time, has values of 1 (float) in each element\n",
    "Y_val = [...]\n",
    "Y_test = [...]\n",
    "# The final result for Y_val and Y_test will be 1D vectors of (m_normal * ratio [validation or test], 1) of 0’s and (m_outliers/ 2, 1) of 1s.\n",
    "\n",
    "# We check the created subsets\n",
    "print('Training, validation, and test subset sizes:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978ddc2-48cd-4d30-b1e1-63ca2992370f",
   "metadata": {},
   "source": [
    "## Random rearrangement of data\n",
    "\n",
    "Finally, we will finish preprocessing the datasets by randomly reordering them.\n",
    "\n",
    "Complete the following code cell to randomly reorder them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb217c-872e-4024-b397-29ab32e2bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the training, validation, and test subsets individually\n",
    "\n",
    "print('First 10 rows and 2 columns of X and vector Y:')\n",
    "print('Training subset:')\n",
    "print(X_train[:10,:2])\n",
    "print(Y_train[:10,:2])\n",
    "print('Validation subset:')\n",
    "print(X_val[:10,:2])\n",
    "print(Y_val[:10,:2])\n",
    "print('Test subset:')\n",
    "print(X_test[:10,:2])\n",
    "print(Y_test[:10,:2])\n",
    "\n",
    "print('Reorder X and Y:')\n",
    "# Use an initial random state of 42, in order to maintain reproducibility\n",
    "X_train, Y_train = [...]\n",
    "X_val, Y_val = [...]\n",
    "X_test, Y_test = [...]\n",
    "\n",
    "print('First 10 rows and 2 columns of X and vector Y:')\n",
    "print('Training subset:')\n",
    "print(X_train[:10,:2])\n",
    "print(Y_train[:10,:2])\n",
    "print('Validation subset:')\n",
    "print(X_val[:10,:2])\n",
    "print(Y_val[:10,:2])\n",
    "print('Test subset:')\n",
    "print(X_test[:10,:2])\n",
    "print(Y_test[:10,:2])\n",
    "\n",
    "print('Training, validation, and test subset sizes:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565e020-b836-4348-b36a-099b6d724dd9",
   "metadata": {},
   "source": [
    "# Graphical representation\n",
    "\n",
    "Finally, we plot our 3 subsets on a 2D graph.\n",
    "\n",
    "Complete the following code cell to randomly reorder them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c184a-f1d9-45e3-a669-b42f3e04899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the 3 subsets on a 2D graph\n",
    "\n",
    "# You can adjust matplotlib’s parameters, such as the range of dimensions and size of the points\n",
    "plt.figure(plot_n)\n",
    "\n",
    "plt.title('Subsets with normal and outlier data')\n",
    "\n",
    "cmap, norm = from_levels_and_colors([0., 0.5, 1.1], ['blue', 'red'])\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=25, c=Y_train, marker='o', cmap=cmap, norm=norm)\n",
    "plt.scatter(X_val[:, 0], X_val[:, 1], s=25, c=Y_val, marker='s', cmap=cmap, norm=norm)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=25, c=Y_test, marker='*', cmap=cmap, norm=norm)\n",
    "\n",
    "plt.xlim(x_lim)\n",
    "plt.ylim(y_lim)\n",
    "plt.legend(('Training'', 'Validation'', 'Test'))\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf4573b-9463-4326-a745-075e04ca0c92",
   "metadata": {},
   "source": [
    "## Model a Gaussian distribution\n",
    "\n",
    "Let's train the model, which in this case will mean first modelling the presumed Gaussian distribution that the normal data follows.\n",
    "\n",
    "Initially we model only normal data to find out which distribution they follow, which data are normal or acceptable and which do not follow this distribution and should be considered outliers.\n",
    "\n",
    "A multivariate Gaussian distribution is defined by 2 parameters: the mean $\\mu$ and the covariance matrix $\\Sigma$. $\\mu$ is a 1D vector of size (*n*,) and $\\Sigma$ is a 2D vector/square matrix (*n*, *n*).\n",
    "\n",
    "Recall from the module and exercise on SVM with Gaussian filter that the Gaussian or multivariate normal distribution has a rounded or oval shape, where $\\mu$ represents the location of the central point of the distribution in space and $\\Sigma$ represents the shape of the distribution, more or less pronounced or beaked.\n",
    "\n",
    "*NOTE:* Although the normal or Gaussian distribution is one of the most common distributions in nature, in a real project we should first check whether our data follows a normal distribution or whether we have to model it with another distribution, following the same steps.\n",
    "\n",
    "$\\mu$ y $\\Sigma$ can be calculated as:\n",
    "\n",
    "$$ \\mu = \\frac{1}{m} \\sum\\limits_{i=0}^{m} x^i; $$\n",
    "$$ \\Sigma = \\frac{1}{m} \\sum\\limits_{i=0}^{m} (x^i - \\mu)(x^i - \\mu)^T; $$\n",
    "\n",
    "Follow the instructions below to model the Gaussian distribution and obtain its parameters $\\mu$ and $\\Sigma$, and then calculate the probability that a point is normal or anomalous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74446758-c229-4cdf-a681-af8995fd7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Model the Gaussian distribution and obtain its mu and Sigma\n",
    "\n",
    "# Calculate the mean and Sigma of X_train\n",
    "# To do this, you can use Numpy's mean and covariance matrix functions with the appropriate axis\n",
    "mu = [...]\n",
    "sigma = [...]\n",
    "\n",
    "# Compute the multivariate normal distribution of the X_train normal training data with these parameters\n",
    "dist_normal = multivariate_normal(mean=mu, cov=sigma)\n",
    "\n",
    "print('Dimensions of the mean and covariance matrix of the training subset:')\n",
    "print(mu.shape, sigma.shape)\n",
    "print('Mean:')\n",
    "print(mu)\n",
    "print('Covariance matrix:')\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4bacbc-c078-40bf-b16d-77323d4a115f",
   "metadata": {},
   "source": [
    "We will plot the density function of the normal data distribution with probability slices next to the normal data dataset.\n",
    "\n",
    "Probability density function:\n",
    "\n",
    "$$ pdf(x) = \\frac{1}{\\Sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2}(\\frac{x - \\mu}{\\Sigma})^2} $$\n",
    "\n",
    "Follow the instructions in the next cell for this:\n",
    "\n",
    "*NOTE*: You can rely on Matplotlib’s [contourf function](https://matplotlib.org/stable/gallery/images_contours_and_fields/contourf_demo.html#sphx-glr-gallery-images-contours-and-fields-contourf-demo-py) and on the examples in [scipy.stats.multivariate_normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html#scipy-stats-multivariate-normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6636514-d2d8-4091-833d-487f38f048b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the density function and normal data\n",
    "\n",
    "fig1, ax2 = plt.subplots([...])\n",
    "\n",
    "[...]\n",
    "\n",
    "# Add a coloured bar with the distribution probability\n",
    "[...]\n",
    "\n",
    "# Add a title and labels to each dimension\n",
    "[...]\n",
    "\n",
    "# Also plot the data of the training subset X_train as points on the same graph\n",
    "[...]\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b19fd-72f1-4edb-a334-dd732036dad5",
   "metadata": {},
   "source": [
    "## Determine the probability threshold for detecting anomalous cases\n",
    "\n",
    "We will now determine the probability threshold ϵ from which we will determine whether a new case is normal or anomalous. If an example is too different from the normal data, if it is far from the normal data, if the probability that it follows the same distribution as the normal data is below this threshold, we can declare it as anomalous.\n",
    "\n",
    "To find this threshold, we will use the validation subset, with normal and anomalous data, and like the validation for regularisation in supervised learning, we will estimate multiple values of the threshold ϵ, keeping the one that best classifies between normal and anomalous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd1133-2549-4243-b6ce-c77c6ee7732b",
   "metadata": {},
   "source": [
    "To begin with, we will plot the distribution, the validation subset, and several possible values of $\\epsilon$ as cutoff boundaries.\n",
    "\n",
    "To do this, follow the instructions to complete the following cell:\n",
    "\n",
    "*NOTE*: For contour lines, use the [contour function](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.contour.html#matplotlib.axes.Axes.contour), also used in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11053b88-8801-4821-84dc-1cbb6b7c874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the distribution, the validation subset, and several possible values of epsilon\n",
    "\n",
    "# Generate some values for epsilon\n",
    "epsilon_evaluated = np.linspace(0., 0.5, num=10)\n",
    "\n",
    "# Retrieve the code of the previous cell and add an outline for each epsilon value\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664998f6-5071-442b-a872-099ba11a040d",
   "metadata": {},
   "source": [
    "To have more visibility on the evaluation of our dataset and to finally check the value of $\\epsilon$ we will compute the probabilities that each outlier in the validation subset follows the distribution of the normal data.\n",
    "\n",
    "Follow the instructions in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d4788-e3b3-4572-92ae-231215ee4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the probabilities that the validation outliers follow the training distribution\n",
    "\n",
    "# Filter out data from the validation subset that are outliers\n",
    "# Remember, outliers have a Y_val = 1.\n",
    "X_val_outliers = [...]\n",
    "\n",
    "# Calculate their probabilities that they follow the normal distribution\n",
    "p_val_outliers = dist_normal.pdf(X_val_outliers)\n",
    "\n",
    "print('Probabilities of the first 10 validation outliers following the normal distribution:')\n",
    "print(p_val_outliers[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793fdb3-06c2-4b33-bae4-5f141fee82a1",
   "metadata": {},
   "source": [
    "These probabilities should be quite low, so that they are almost all below the probability or cutoff threshold of $\\epsilon$.\n",
    "\n",
    "Check that this is the case and also check that the vast majority of non-anomalous data in the validation subset have clearly higher probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96200dfa-352d-414a-96aa-4f3fc864a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the probabilities that the non-anomalous validation data follows the training distribution\n",
    "\n",
    "# Filter the normal data from the validation subset\n",
    "# Remember, normal data has a Y_val = 0.\n",
    "X_val_normal = [...]\n",
    "\n",
    "# Calculate their probabilities that they follow the normal distribution\n",
    "p_val_normal = dist_normal.pdf(X_val_normal)\n",
    "\n",
    "print('Probabilities of the first 10 validation outliers following the normal distribution:')\n",
    "print(p_val_normal[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058d2bd-663c-4c5f-a460-e4ff1d1daffb",
   "metadata": {},
   "source": [
    "To appreciate this more easily, represent both probabilities graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74926a21-ebea-4ba1-83a4-13ff65b7c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot both probabilities over all normal and outlier data as a line and dot plot\n",
    "\n",
    "plt.figure(plot_n)\n",
    "\n",
    "# Use a legend and series with different colours\n",
    "[...]\n",
    "\n",
    "plot_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c9ed2d-ea69-45e8-ab30-6231ab3ec462",
   "metadata": {},
   "source": [
    "Finally, we will evaluate a linear space of possible values of $\\epsilon$ and find the most optimal one to declare a datum as anomalous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bedbd7-a692-45cc-ba7c-659dd2e941c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate multiple epsilon values and find the most optimal one to classify data as normal or anomalous\n",
    "\n",
    "# Generate a linear space of epsilon values with greater accuracy\n",
    "epsilon_evaluated = np.linspace(0., 1., num=1e2)    # You can reduce the accuracy to speed up the computation\n",
    "\n",
    "# Values to find their optimal\n",
    "epsilon = 1e6    # Initial epsilon value to be optimised\n",
    "f1_val = 0.    # F1_score of the classification\n",
    "for e in epsilon_evaluated:\n",
    "    # Assign Y = 1 to values whose probability is less than epsilon and 0 to the rest\n",
    "    Y_val_pred = np.where([...])\n",
    "    \n",
    "    # Find the F1-score for that classification with Y_val as the known value\n",
    "    score = f1_score([...])\n",
    "    \n",
    "    if score > f1_val:\n",
    "        f1_val = score\n",
    "        epsilon = e\n",
    "\n",
    "print('Epsilon optimal in validation subset:', epsilon)\n",
    "print('F1-score:', f1_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f74abc-7bf8-4f60-b9dd-9a8e5bdd269d",
   "metadata": {},
   "source": [
    "Representa de nuevo líneas de contorno para las $\\epsilon$ evaluadas sobre una gráfica 2D con los datos de validación, normales y anómalos en distintos colores, y la $\\epsilon$ óptima con un color diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f93b9-322e-45c0-8db1-72401c7a2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot contour lines again for the evaluated ϵ on a 2D plot with the validation, normal, and outlier data in different colours, and the optimal $\\epsilon$ in a different colour:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379a47d-5cb4-44b5-a4de-5285987b29ed",
   "metadata": {},
   "source": [
    "## Evaluar la precisión final del modelo\n",
    "\n",
    "Para finalizar nuestro entrenamiento, vamos a comprobar la precisión final del modelo sobre el subset de test, como hacemos habitualmente.\n",
    "\n",
    "Para ello, haremos una comprobación matemática y visual de dichos datos.\n",
    "\n",
    "Sigue las instrucciones para completar la siguiente celda y representar gráficamente los datos normales y anómalos del subset de test junto a la distribución de datos normales del subset de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817aabb9-40d5-4d9e-8fb5-18d3729e0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa el subset de test junto a la distribución de datos del subset de entrenamiento\n",
    "# Incluye la línea de contorno para la epsilon escogida\n",
    "\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f2bf1-9fdc-4e2c-a2e5-483c0c3cd74c",
   "metadata": {},
   "source": [
    "Ahora calcula las métricas de evaluación de clasificación para evaluar la clasificación entre datos normales y anómalos que hace el modelo sobre el subset de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8702ac8-7565-4356-9c3b-9d9c85b58ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula las métricas de evaluación de clasificación del modelo para el subset de test\n",
    "\n",
    "# Asigna Y = 1. a valores cuya probabilidad sea inferior a epsilon y 0. al resto\n",
    "Y_test_pred = np.where([...])\n",
    "\n",
    "# Haya el F1-score para la clasificación con Y_test como valor conocido\n",
    "f1_test = f1_score([...])\n",
    "\n",
    "print('F1-score para el subset de test:', f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a2158-146a-45d7-a106-0b5291f2a29d",
   "metadata": {},
   "source": [
    "Analiza gráficamente qué datos del subset de test clasifica correcta e incorrectamente el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea09ac-99ee-43f5-b860-91837a6f6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa errores y aciertos en el subset de test junto a la distribución y el corte de epsilon\n",
    "\n",
    "# Asigna z = 1. para acierto y z = 0. para fallo\n",
    "# Acierto: Y_test == Y_test_pred\n",
    "z = [...]\n",
    "\n",
    "# Representa la gráfica\n",
    "# Utiliza colores diferentes para los datos normales y anómalos y formas diferentes para los aciertos y fallos\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55549e42-0611-4676-869a-5efd331581ab",
   "metadata": {},
   "source": [
    "*¿Crees que el modelo hace un buen trabajo al detectar las anomalías?*\n",
    "\n",
    "*¿Hay algún dato que tú clasificarías de forma diferente?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0756999-3a84-4dcb-b5cd-501c8856cb1f",
   "metadata": {},
   "source": [
    "Por último, representa gráficamente todos los datos, de los 3 subsets, de forma conjunta, junto a la distribución y el corte $\\epsilon$, para analizar la distribución de datos normales y anómalos y el funcionamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5d4cd-c824-41af-86cc-e4ba97fad4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa los datos normales y anómalos junto a la distribución y el corte de epsilon\n",
    "# Representa los 3 subsets: entrenamiento, validación y test\n",
    "# Distingue los 3 subsets con marcadores de formas diferentes\n",
    "# Utiliza colores diferentes para datos normales y anómalos conocidos originalmente\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
