{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9426a6-87f3-40a6-8c5f-941f0a4be60d",
   "metadata": {},
   "source": [
    "# Linear Regression: Synthetic example with Scikit-learn\n",
    "M2U3 - Exercise 6\n",
    "\n",
    "## What are we going to do?\n",
    "- We will solve a multivariate linear regression model using Scikit-learn\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa052fc-51d9-40b1-be66-4effb2edae9f",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Once we developed a hands-on implementation of the multivariate linear regression algorithm with Numpy exclusively, we have been able to see in depth the steps to follow, how the internal mathematical algorithm works, and how all the hyperparameters affect it.\n",
    "\n",
    "Having a good understanding of how these ML models work, let's see how to use them with the functions of the Scikit-learn ML framework.\n",
    "\n",
    "In this exercise you will have a blank template with the steps we have followed in previous exercises, which you will have to complete with your code following those steps, but this time using Scikit-learn methods.\n",
    "\n",
    "In each cell we will suggest a Scikit-learn function that you can use. We won't give you more information about it here, because we want you to look it up for yourself in the documentation: how it works, the algorithms it implements (some of them will be slightly different from the ones we have seen in the course, don't worry as the important thing is the base), arguments, examples, etc.\n",
    "\n",
    "It sounds like a truism, but I'm sure you will agree with us that the ability to find relevant information in the documentation at all times is extremely important, and it can often cost us a little more than it should :).\n",
    "\n",
    "Also take the opportunity to dive deeper into the documentation and discover interesting aspects of the framework. We will continue to work with it in subsequent exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3107a3-7a0f-485e-a2c4-35203f3cf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import all the necessary modules into this cell\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524eb8a5-4cfe-45f2-86de-fa7468cd3674",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset for linear regression\n",
    "\n",
    "- Add a modifiable bias and error term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c14a0e9-879f-4521-b42f-00cc29bb34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a synthetic dataset for linear regression with Scikit-learn\n",
    "# You can use the sklearn.datasets.make_regression() function\n",
    "# Remember to always use a given random start state to maintain reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9628d70-ce18-48c5-bb29-c4f9e13ee149",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "- Randomly reorder the data..\n",
    "- Normalise the data..\n",
    "- Dvide the data into training and test subsets.\n",
    "\n",
    "*Note*: Why did we use only 2 training and test subsets this time, with no validation subset? Because we will use *k-fold* for our cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44e3fe-a738-499d-b60b-0d0064fe2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the data\n",
    "# You can use the sklearn.utils.shuffle() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8d39a-37c7-4ed3-a2f3-11718fbfc491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the examples\n",
    "# You can use the sklearn.preprocessing.StandardScaler() class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b5894-1ecd-4de6-b358-428ed42fb854",
   "metadata": {},
   "source": [
    "*Note*: This scaling is equivalent to the basic normalisation we have seen throughout the course. Another more convenient but more complex to comprehend normalisation for more advanced models would be the one implemented in *sklearn.preprocessing.normalize*.\n",
    "\n",
    "You can find all the available preprocessing classes and functions here: [Sklearn docs: 6.3. Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "And a graphical comparison: [Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eec2eb-d249-46ad-a9a2-30a55b7c8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide the dataset into training and test subsets\n",
    "# You can use the sklearn.model_selection.train_test_split() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db3368-6f2d-4cb2-82da-cb02adacf13f",
   "metadata": {},
   "source": [
    "## Train an initial model\n",
    "\n",
    "- Train an initial model on the training subset without regularisation.\n",
    "- Test the suitability of the model.\n",
    "- Check if it suffers from deviation or overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ab79a-eec7-4c38-951c-995309a6105a",
   "metadata": {},
   "source": [
    "To train a simple multivariate linear regression model, you can use the [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) class\n",
    "\n",
    "You can consult a complete training example: [Linear Regression Example](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a136bc-35fb-4d46-959a-7bcd18fe3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a baseline linear regression model on the training subset without regularisation\n",
    "# Adjust the intercept/bias term and do not normalise the features, as we have already normalised them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb8363-0fd5-4950-9862-507ea4f6065a",
   "metadata": {},
   "source": [
    "Check the suitability of the model applied to this dataset. To do this you can use:\n",
    "- The R^2 coefficient of determination method [LinearRegression.score()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score)\n",
    "- The function [sklearn.metrics.mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) (returns the MSE or RMSE)\n",
    "- Other [regression metrics](https://scikit-learn.org/stable/modules/classes.html#regression-metrics)\n",
    "\n",
    "Try several of the methods to get to know them better and see their possible differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124b483-05c4-4452-b5d2-a173c10b400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the suitability of the model by evaluating it on the test set\n",
    "# Test 3 of the preceding metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c63d5-d604-4fde-aea1-db964f92b837",
   "metadata": {},
   "source": [
    "To check whether there might be bias or overfitting, we can calculate e.g., the RMSE on the predictions of the training subset and on those of the test subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48c4a0-86c1-4c95-a816-57d877cf2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check if the evaluation on both subsets is similar with the RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe51468-adab-4332-b4ce-cd37dc7629c9",
   "metadata": {},
   "source": [
    "## Find the optimal *k-fold* or cross-validation regularisation\n",
    "\n",
    "- Train a model for each regularisation value to be considered.\n",
    "- Train and evaluate them on a K-fold training subset division.\n",
    "- Choose the optimal model and its regularisation.\n",
    "\n",
    "We are now going to use a more complex linear regression algorithm, the [sklearn.linear_model.Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) class, which allows us to set an L2 regularisation parameter.\n",
    "\n",
    "In this function, the regularisation argument is called *alpha*, although it should not be confused with the learning rate.\n",
    "\n",
    "The regularisation we have seen during the course is the one implemented by most Scikit-learn algorithms, its common name being \"L2\" or \"L2-norm\".\n",
    "\n",
    "Consider L2 regularisation parameters in the logarithmic range: 0.1, 0.01, 0.001, 0.0001, etc.\n",
    "\n",
    "You can follow this link: [K-fold](https://scikit-learn.org/stable/modules/cross_validation.html#k-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740c1c0-a47c-4e99-8aac-90dda330b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a different model for each alpha on a different K-fold\n",
    "\n",
    "# Use a Numpy function to create a logarithmic space of >5 values between [0, 1e-3]\n",
    "alphas = [...]\n",
    "\n",
    "# Create k K-fold splits\n",
    "kfolds = [...]\n",
    "\n",
    "# Iterate over the splits for your models and evaluate them on the generated CV subset\n",
    "linear_models = []\n",
    "best_model = None\n",
    "for train, cv in kfolds.split(X):\n",
    "    # Train a model on the training subset\n",
    "    # Remember to set the corresponding alpha/regularisation parameter, adjust the bias, and do not normalise\n",
    "    # Evaluate it on the CV subset using its model.score()\n",
    "    # Save the model with the best score for the best_model variable and display the alpha of the best model\n",
    "    alpha = [...]\n",
    "    print('L2 regularization:', alpha)\n",
    "    \n",
    "    model = [...]\n",
    "    \n",
    "    linear_models.append(model)\n",
    "    \n",
    "    # If the model is better than the best model so far...\n",
    "    best_model = [...]\n",
    "    \n",
    "    print('L2 regularisation of the best model so far:', alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f97d9f-170e-44ff-917e-6a81e5186827",
   "metadata": {},
   "source": [
    "## Finally, evaluate the model on the test subset\n",
    "\n",
    "- Display the coefficients and intercept of the best model.\n",
    "- Evaluate the best model on the initial test subset.\n",
    "- Calculate the residuals on the test subset and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4a0c4-24f4-44cc-99a1-bbee993fbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model on the initial test subset\n",
    "\n",
    "# Display the coefficients and intercept of the best trained model\n",
    "print('Intercept coefficients of the trained model:')\n",
    "print([...])    # Make predictions about the test subset\n",
    "\n",
    "# Realiza las predicciones sobre el subset de test\n",
    "y_test_pred = [...]\n",
    "\n",
    "# Calculate the model evaluation metrics: RMSE and coefficient of determination R^2\n",
    "rmse = [...]\n",
    "r2_score = [...]\n",
    "\n",
    "print('Root mean square error(RMSE): %.2f' % rmse)\n",
    "print('Coefficient of determination: %.2f' % r2_score)\n",
    "\n",
    "# Calculate the residuals on the test subset\n",
    "residuals = [...]\n",
    "\n",
    "# Plot them graphically\n",
    "plt.figure(1)\n",
    "\n",
    "# Fill in your code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73c084-040e-4cdb-8bb0-ec6dabcd0f0e",
   "metadata": {},
   "source": [
    "## Make predictions about new examples\n",
    "\n",
    "- Generate a new example, following the same pattern as the original dataset.\n",
    "- Normalise its features.\n",
    "- Generate a prediction for this new example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113c4eb-6a5b-4a22-963b-7f57bc3611b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make predictions about a new manually created example\n",
    "\n",
    "# Create the new example\n",
    "X_pred = [...]\n",
    "\n",
    "# Normalise its features\n",
    "X_pred = [...]\n",
    "\n",
    "# Generate a prediction for this new example\n",
    "y_pred = [...]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
