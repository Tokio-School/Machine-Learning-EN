{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3064ed2b-d40e-4c47-92be-8f482d0894a1",
   "metadata": {},
   "source": [
    "# Linear Regression: Comparison of regularisation values\n",
    "M2U3 - Exercise 3\n",
    "\n",
    "## What are we going to do?\n",
    "- Create a synthetic dataset for multivariate linear regression with a random error term\n",
    "- We will train 3 different linear regression models on this dataset with different *lambda* values\n",
    "- We will compare the effect of the *lambda* value on the model, its accuracy, and its residuals\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0c848-4a74-4625-81f8-16026ae359fd",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset with error term for training and final testing\n",
    "\n",
    "VWe will start, as usual, by creating a synthetic dataset for linear regression, with bias and error terms, either manually or with Scikit-learn methods.\n",
    "\n",
    "This time we are going to create 2 datasets, one for training and one for final test, following the same pattern but with different sizes. We will train the models with the first dataset and then check with the second dataset how they would behave on data that they have not \"seen\" previously in the training process, which are completely new to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e16d5-eb44-448a-9173-fc54c9abd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a synthetic dataset manually, with bias term and error term\n",
    "\n",
    "m = 100\n",
    "n = 1\n",
    "\n",
    "X_train = [...]\n",
    "X_test = [...]    # The size of the test dataset should be 25% of the original\n",
    "\n",
    "Theta_true = [...]\n",
    "\n",
    "error = 0.35\n",
    "\n",
    "Y_train = [...]\n",
    "Y_test = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta to be estimated and its dimensions:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Check X_train, X_test, Y_train e Y_test\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289d719-6d79-4078-a8e9-e1e45071b25c",
   "metadata": {},
   "source": [
    "## Train 3 different models with different *lambda* values\n",
    "\n",
    "We will train 3 different models on this dataset with different *lambda* values.\n",
    "\n",
    "To do this, start by copying your cells with the code that implements the regularised cost function and gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fcd813-f806-4750-bdc9-d76be0e5d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy here the cells or the code to implement 2 functions with regularised cost function\n",
    "# and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff72fa1-6b3f-4ee8-a5ed-6437c7fb595a",
   "metadata": {},
   "source": [
    "Let's train the models. To do this, remember that with Jupyter you can simply modify the code cells and the variables will remain in the Jupyter kernel memory.\n",
    "\n",
    "Therefore, you can e.g., modify the name of the following variables, changing \"1\" to \"2\" y \"3\", and simply re-execute the cell to store the results of the 3 models, while the variables of the previous models are still available.\n",
    "\n",
    "If you run into any difficulties, you can also copy the code cell several times and have 3 cells to train 3 models with different variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e952b-eb2b-473e-9afb-120b18b005eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test your implementation by training a model on the previously created synthetic dataset\n",
    "\n",
    "# Create an initial theta with a given constant value (not randomly this time).\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = [1e-3, 1e-1, 1e1]    # We use 3 different values\n",
    "e = 1e-3\n",
    "iter_ = 1e3    # Check that your function can support float values or modify it\n",
    "\n",
    "print('Hyperparameters used:')\n",
    "print('Alpha:', alpha, 'Max error:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "# Use lambda_[i], within the range [0, 1, 2] for each model\n",
    "j_hist_1, theta_final_1 = gradient_descent([...])\n",
    "\n",
    "print('Training time (s):', time.time() - t)\n",
    "\n",
    "# TODO: complete\n",
    "print('\\nLast 10 values of the cost function')\n",
    "print(j_hist_1[...])\n",
    "print('\\Final cost:')\n",
    "print(j_hist_1[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta_final_1)\n",
    "\n",
    "print('True values of Theta and difference with trained valuess:')\n",
    "print(Theta_true)\n",
    "print(theta_final_1 - Theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f32f2d-32ac-4b58-954a-f1e3aec73aa8",
   "metadata": {},
   "source": [
    "## Graphically check the effect of lambda on the models\n",
    "\n",
    "Now let's check the 3 models against each other.\n",
    "\n",
    "Let's start by checking the final cost, a representation of their accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c7326-96ee-4d20-b2d8-a4f3042e0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show the final cost of the 3 models:\n",
    "\n",
    "print('Final cost of the 3 models:')\n",
    "print(j_hist_1[...])\n",
    "print(j_hist_2[...])\n",
    "print(j_hist_3[...])\n",
    "\n",
    "# Visually represent the cost vs. lambda values with a line and dot plot\n",
    "plt.plot([...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef947036-e97c-4d1f-9270-b359c0077acd",
   "metadata": {},
   "source": [
    "*How does a higher *lambda* value affect the final cost in this dataset?*\n",
    "\n",
    "Let's plot the training and test datasets, to check that they follow a similar pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04402c0-858d-4285-ae96-b728e48ef7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot X_train vs Y_train, and X_test vs Y_test graphically.\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "plt.title([...])\n",
    "plt.xlabel([...])\n",
    "plt.ylabel([...])\n",
    "\n",
    "# Remember to use different colours\n",
    "plt.scatter([...])\n",
    "plt.scatter([...])\n",
    "\n",
    "# Create a legend for the different series and their colours\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ff5d0-e1b9-41b9-8701-b22c889209fb",
   "metadata": {},
   "source": [
    "We will now check the predictions of each model on the training dataset, to see how well the line fits the training values in each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab90ac-ce83-4e49-b3ed-1e604de010e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the predictions for each model on X_train\n",
    "\n",
    "Y_train_pred1 = [...]\n",
    "Y_train_pred2 = [...]\n",
    "Y_train_pred3 = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919048cf-dd27-476e-af86-2f403e77bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For each model, graphically represent its predictions about X_test\n",
    "\n",
    "# If you get an error with other notebook charts, use the bottom line of plt.figure() or comment it out\n",
    "plt.figure(2)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\n",
    "fig.suptitle([...])\n",
    "\n",
    "# Use different colours for each model\n",
    "\n",
    "ax1.plot()\n",
    "ax1.scatter()\n",
    "\n",
    "ax2.plot()\n",
    "ax2.scatter()\n",
    "\n",
    "ax3.plot()\n",
    "ax3.scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f424f3a-cbac-4f50-a51b-42369e76fb59",
   "metadata": {},
   "source": [
    "Since the training dataset has an error term, there may be significant differences between the data in the training dataset and the test dataset. You can play with various values of this term to increase or decrease the difference..\n",
    "\n",
    "Let's check what happens to the predictions when we plot them on the test dataset, on data that the models have not seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74c55b-d431-458e-aeb9-9acec635a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate predictions for each model on X_test\n",
    "\n",
    "Y_test_pred1 = [...]\n",
    "Y_test_pred2 = [...]\n",
    "Y_test_pred3 = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f231a2-1c22-4fdf-8d3f-eee83d35fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For each model, graphically represent its predictions about X_test\n",
    "\n",
    "# SIf you get an error with other notebook charts, use the bottom line of plt.figure() or comment it out\n",
    "plt.figure(3)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\n",
    "fig.suptitle([...])\n",
    "\n",
    "# Use different colours for each model\n",
    "\n",
    "ax1.plot()\n",
    "ax1.scatter()\n",
    "\n",
    "ax2.plot()\n",
    "ax2.scatter()\n",
    "\n",
    "ax3.plot()\n",
    "ax3.scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47348a65-925c-4009-8cc9-b3bee04e261d",
   "metadata": {},
   "source": [
    "What happens? In some cases, depending on the parameters used, it may be more or less easy to discern it.\n",
    "\n",
    "Cuando el modelo tiene un factor de regulación *lambda* bajo o nulo, se ajusta demasiado a los datos con los que se entrena, consiguiendo una curva muy pegada y una precisión máxima... sólo sobre dicho dataset.\n",
    "\n",
    "Sin embargo, en la vida real, posteriormente pueden llegar datos sobre los que no hemos entrenado el modelo que tengan alguna pequeña variación sobre los datos originales.\n",
    "\n",
    "En dicha situación vamos a preferir un valor de *lambda* más alto, que nos permita tener una precisión mayor para los nuevos datos, aunque perdamos algo de precisión para los datos del dataset de entrenamiento.\n",
    "\n",
    "Por tanto, buscamos que un modelo \"generalice\" y pueda hacer buenas predicciones sobre nuevos datos, en lugar de que simplemente \"memorice\" los resultados que ya ha visto.\n",
    "\n",
    "Podemos por tanto pensar en la regularización como un alumno que tiene las preguntas del examen antes de presentarse:\n",
    "- Si luego le caen dichas preguntas, va a tener una nota (o precisión) muy alta, puesto que ya \"ha visto\" las preguntas previamente.\n",
    "- Si luego las preguntas son diferentes, puede tener una nota bastante alta, en función de lo similares que sean.\n",
    "- Sin embargo, si las preguntas son totalmente diferentes, va a tener una nota muy baja, porque no es que hubiera estudiado mucho la asignatura, sino que sus notas eran altas sólo por saber el resultado de antemano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3f7fb-ef8e-41b5-bb52-59239dbdb0db",
   "metadata": {},
   "source": [
    "## Comprobar los residuos sobre el subset de test final\n",
    "\n",
    "Representa los residuos para los 3 modelos gráficamente. De esta forma vas a poder comparar tus 3 modelos sobre los 2 datasets.\n",
    "\n",
    "Calcúlalos tanto para el dataset de entrenamiento como para el de testing. Puedes hacerlo con celdas diferentes para poder apreciar sus diferencias a la vez.\n",
    "\n",
    "*Consejos*:\n",
    "- Cuidado con las escalas de los ejes X e Y a la hora de hacer las comparaciones.\n",
    "- Para poder verlos a la vez, puedes crear 3 subgráficas horizontales, en lugar de verticales, con \"plt.subplots(1, 3)\".\n",
    "- Utiliza colores diferentes para cada uno de los 3 modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c52d96-c5d4-40af-bed5-e6169479a2a6",
   "metadata": {},
   "source": [
    "Si no aprecias claramente dichos efectos sobre tus datasets, puedes probar a modificar los valores iniciales:\n",
    "- Con un nº de ejemplos *m* mayor, para que los modelos puedan ser más precisos.\n",
    "- Con un término de error mayor, para que haya más diferencia o variación entre ejemplos.\n",
    "- Con un tamaño del dataset de test sobre el de entrenamiento menor, para que haya más diferencias entre ambos datasets (al tener más datos, los valores pueden suavizarse más).\n",
    "- Etc."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
