{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd78b6e-f0b2-463d-a0fc-3cb7028b8fae",
   "metadata": {},
   "source": [
    "# Linear Regression: Normalisation\n",
    "M2U3 - Exercise 1\n",
    "\n",
    "## What are we going to do?\n",
    "- We will create a synthetic dataset with features in different value ranges\n",
    "- We will train a linear regression model on the original dataset\n",
    "- We will normalise the original dataset\n",
    "- We will train another linear regression model on the normalised dataset\n",
    "- We will make a comparison between the training of both models, normalised and non-normalised\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08edc5e4-f421-43a6-bc2a-81b519b3d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d98df0-1c0f-4d3a-96fa-421c510fbc48",
   "metadata": {},
   "source": [
    "## Creation of a synthetic dataset\n",
    "\n",
    "We are going to manually create a synthetic dataset for linear regression.\n",
    "\n",
    "Create a synthetic dataset with an error term of 10% of the value over *Y* and an *X* approx. in the range (-1, 1), this time manually, not with the specific Scikit-learn methods, with the code used in previous exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852d3d6-6ab9-4eea-b570-f46e578e5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy code from previous exercises to generate a dataset with a bias term and an error term\n",
    "\n",
    "m = 1000\n",
    "n = 4\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_verd = [...]\n",
    "\n",
    "error = 0.1\n",
    "\n",
    "Y = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ce2dc-fb9c-48d8-b8e2-b467a6b34888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values and dimensions of the vectors\n",
    "print('Theta and its dimensions to be estimateds:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca2797d-f580-4ad5-8eb9-ec8dbe307a8d",
   "metadata": {},
   "source": [
    "We will now modify the dataset to ensure that each feature, each column of *X*, has a different order of magnitude and mean.\n",
    "\n",
    "To do this, multiply each column of *X* (except the first one, the bias, which must be all 1’s) by a different range and add a different bias value to it.\n",
    "\n",
    "The value we then add up will be the mean of that feaure or column, and the value by which we multiply its range or scale.\n",
    "\n",
    "P. ej., $X_1 = X_1 * 10^3 + 3.1415926$, where `10^3` would be the mean and `3,1415926` the scale of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838c8d9-b6c7-4372-8ee5-d204d02d67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For each column of X, multiply it by a range of values and add a different mean to it\n",
    "\n",
    "# The arrays of ranges and averages must be of length n\n",
    "# Create an array with the ranges of values, e.g.: 1e0, 1e3, 1e-2, 1e5\n",
    "ranges = [...]\n",
    "\n",
    "averages = [...]\n",
    "\n",
    "X = [...]\n",
    "\n",
    "print('X with different averages and scales')\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231cbb2-6163-4d66-98f6-388a59c08761",
   "metadata": {},
   "source": [
    "Remember that you can run Jupyter cells in a different order from their position in the document. The brackets to the left of the cells will mark the order of execution, and the variables will always keep their values after the last executed cell, **¡so be careful!**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1620b3-2c86-49f6-a244-af617f1acd6b",
   "metadata": {},
   "source": [
    "## Training and evaluation of the model\n",
    "\n",
    "Once again, we will train a multivariate linear regression model. This time, we are going to train it first on the original, non-normalised dataset, and then retrain it on the normalised dataset, in order to compare both models and training processes and see the effects of normalisation.\n",
    "\n",
    "To do this you must copy the cells or code from previous exercises and train a multivariate linear regression model, optimized by gradient descent, on the original dataset.\n",
    "\n",
    "You must also copy the cells that test the training of the model, representing the cost function vs. the number of iterations.\n",
    "\n",
    "You do not need to make predictions about this data or evaluate the model’s residuals. In order to compare them, we will do so only on the basis of the final cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed296163-cc10-42b6-9b94-ff5d661891f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a linear regression model and plot the evolution of its cost function\n",
    "# Use the non-normalised X\n",
    "# Add the suffix \"_no_norm\" to the Theta and j_hist variables returned by your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a01c7-aa0e-46ad-85fe-0c95f9513732",
   "metadata": {},
   "source": [
    "## Data normalisation\n",
    "\n",
    "We are going to normalise the data from the original dataset.\n",
    "\n",
    "To do this, we are going to create a normalisation function that applies the necessary transformation, according to the formula:\n",
    "\n",
    "$x_j = \\frac{x_j - \\mu_{j}}{\\sigma_{j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29339c02-1f97-44bc-b6b9-b8fe446c6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a normalisation function to a common range and with a mean of 0\n",
    "\n",
    "def normalize(x, mu, std):\n",
    "    \"\"\" Normalise a dataset with X examples\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the examples, no bias term\n",
    "    mu -- Numpy 1D vector with the mean of each feature/column\n",
    "    std -- Numpy 1D vector with the standard deviation of each feature/column\n",
    "    \n",
    "    Return:\n",
    "    x_norm -- Numpy 2D array with the examples, and their normalised features\n",
    "    \"\"\"\n",
    "    return [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4bb8d-62ad-4a82-94fc-b653dbdce5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the original dataset using your normalisation function\n",
    "\n",
    "# # Find the mean and standard deviation of the features of X (columns), except the first column (bias).\n",
    "mu = [...]\n",
    "std = [...]\n",
    "\n",
    "print('original X:')\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('Mean and standard deviation of the features':')\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('normalised X:')\n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std)    # Normalise only column 1 and the subsequent columns, not column 0\n",
    "print(X_norm)\n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a478df-2bfe-44d0-a8dd-9803bc10d7d2",
   "metadata": {},
   "source": [
    "*BONUS:*\n",
    "1. Calculate the means and standard deviations of *X_norm* according to its features/columns.\n",
    "1. Compare them with those of *X*, *mu*, and *std*\n",
    "1. Plot the distributions of *X* and *X_norm* in a bar graph or box plot (you can use multiple Matplotlib subplots)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2282464-871d-4f5a-b37d-1fddc565deab",
   "metadata": {},
   "source": [
    "## Retraining the model and comparison of results\n",
    "\n",
    "Now retrain the model on the normalised dataset. Check the final cost and the iteration at which it converged.\n",
    "\n",
    "To do this, you can go back to the training cells of the model and check the evolution of the cost function and modify the *X* used for *X_norm*.\n",
    "\n",
    "In many cases, because it is such a simple model, there may be no noticeable improvement. Depending on the capacity of your working environment, try using a higher number of features and slightly increasing the error term of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504bb63c-dab8-4ef1-9290-ec401dd48e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a linear regression model and plot the evolution of its cost function\n",
    "# Use the normalised X\n",
    "# Add the suffix \"_norm\" to the Theta and j_hist variables returned by your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7ab5a-96a6-4cfd-965e-87cb06074c57",
   "metadata": {},
   "source": [
    "*QUESTION: : Is there any difference in the accuracy and training time of the model on non-normalised data and the model on normalised data? If you increase the error term and the difference in means and ranges between the features, does it make more of a difference?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db510f96-5b5a-405c-9b18-53335a1380d7",
   "metadata": {},
   "source": [
    "## Beware of the original Theta\n",
    "\n",
    "For the original dataset, before normalisation, the relationship $Y = X \\times \\Theta$ was fulfilled\n",
    "\n",
    "However, we have now modified the *X* term of this function..\n",
    "\n",
    "Therefore, check what happens if you want to recompute *Y* using the normalized *X*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c9276-898b-4075-8f85-14a6433a08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check for differences between the original Y and the Y computed using the normalized X\n",
    "\n",
    "# Check the value of Y by multiplying X_norm and Theta_true\n",
    "Y_norm = [...]\n",
    "\n",
    "# Check for differences between Y_norm and Y\n",
    "diff = Y_norm - Y\n",
    "\n",
    "print('Difference between Y_norm and Y (first 10 rows):')\n",
    "print(diff[:10])\n",
    "\n",
    "# Plot the difference between the Ys vs X on a dot plot\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d7eb8f-449c-4314-a6e0-a2c96da46f05",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "\n",
    "Similarly, what happens when we are going to use the model to make predictions?\n",
    "\n",
    "Generate a new dataset *X_pred* following the same method you used for the original *X* dataset, incorporating the bias term, multiplying its features by a range and adding different values to them, without finally normalising the dataset.\n",
    "\n",
    "Also calculate its *Y_pred_true* (without error term), as the true value of Y to try to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58266bfc-775e-4b71-968d-efec6049e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a new dataset with fewer examples and the same number of features as the original dataset\n",
    "# Make sure it has a normalized mean or range across features/columns\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "Y_pred_true = np.matmul(X_pred, Theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c7760-e745-4c87-9f65-d38379bd1c69",
   "metadata": {},
   "source": [
    "Now check if there is any difference between the *Y_pred_true* and the *Y_pred* that your model predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfa630-2986-48a8-98a7-1e5c2397b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check the differences between the actual Y and the predicted Y\n",
    "\n",
    "Y_pred = np.matmul(X_pred, theta)\n",
    "\n",
    "diff = Y_pred_true - Y_pred\n",
    "\n",
    "print('Differences between actual Y and predicted Y:')\n",
    "print(diff[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982753d9-e862-44a1-b7f1-9ed679b30e77",
   "metadata": {},
   "source": [
    "Since the predictions are not correct otherwise, we should first normalise the new *X_pred* before generating the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb3bc9-8049-4c83-ac44-b745617db797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the X_pred\n",
    "\n",
    "X_pred[...] = normalize(X_pred[...], mu, std)\n",
    "\n",
    "print(X_pred[:10,:])\n",
    "print(X_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2adea-14ff-45bf-ac88-bb684a598abd",
   "metadata": {},
   "source": [
    "This time we have not generated a new, different variable by normalisation, but it remains the variable *X_pred*.\n",
    "\n",
    "You can then rerun the previous cell to, now that *X_pred* is normalised, check if there is any difference between the actual *Y* and the predicted *Y*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94370b2-64ca-4445-8405-804216ebfd0b",
   "metadata": {},
   "source": [
    "So always remember:\n",
    "- The *theta* calculated when training the model will always be relative to the normalised dataset, and cannot be used for the original dataset, since with the same *Y* and a different *X*, Theta must change.\n",
    "- To make predictions on new examples, we first have to normalise them as well, using the same values for the means and standard deviations that we originally used to train the model."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
