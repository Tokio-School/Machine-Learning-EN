{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd78b6e-f0b2-463d-a0fc-3cb7028b8fae",
   "metadata": {},
   "source": [
    "# Linear Regression: Normalisation\n",
    "M2U3 - Exercise 1\n",
    "\n",
    "## What are we going to do?\n",
    "- We will create a synthetic dataset with features in different value ranges\n",
    "- We will train a linear regression model on the original dataset\n",
    "- We will normalise the original dataset\n",
    "- We will train another linear regression model on the normalised dataset\n",
    "- We will make a comparison between the training of both models, normalised and non-normalised\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08edc5e4-f421-43a6-bc2a-81b519b3d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d98df0-1c0f-4d3a-96fa-421c510fbc48",
   "metadata": {},
   "source": [
    "## Creation of a synthetic dataset\n",
    "\n",
    "We are going to manually create a synthetic dataset for linear regression.\n",
    "\n",
    "Create a synthetic dataset with an error term of 10% of the value over *Y* and an *X* approx. in the range (-1, 1), this time manually, not with the specific Scikit-learn methods, with the code used in previous exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852d3d6-6ab9-4eea-b570-f46e578e5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy code from previous exercises to generate a dataset with a bias term and an error term\n",
    "\n",
    "m = 1000\n",
    "n = 4\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_verd = [...]\n",
    "\n",
    "error = 0.1\n",
    "\n",
    "Y = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ce2dc-fb9c-48d8-b8e2-b467a6b34888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values and dimensions of the vectors\n",
    "print('Theta and its dimensions to be estimateds:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca2797d-f580-4ad5-8eb9-ec8dbe307a8d",
   "metadata": {},
   "source": [
    "We will now modify the dataset to ensure that each feature, each column of *X*, has a different order of magnitude and mean.\n",
    "\n",
    "To do this, multiply each column of *X* (except the first one, the bias, which must be all 1’s) by a different range and add a different bias value to it.\n",
    "\n",
    "The value we then add up will be the mean of that feaure or column, and the value by which we multiply its range or scale.\n",
    "\n",
    "P. ej., $X_1 = X_1 * 10^3 + 3.1415926$, where `10^3` would be the mean and `3,1415926` the scale of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838c8d9-b6c7-4372-8ee5-d204d02d67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For each column of X, multiply it by a range of values and add a different mean to it\n",
    "\n",
    "# The arrays of ranges and averages must be of length n\n",
    "# Create an array with the ranges of values, e.g.: 1e0, 1e3, 1e-2, 1e5\n",
    "ranges = [...]\n",
    "\n",
    "averages = [...]\n",
    "\n",
    "X = [...]\n",
    "\n",
    "print('X with different averages and scales')\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231cbb2-6163-4d66-98f6-388a59c08761",
   "metadata": {},
   "source": [
    "Remember that you can run Jupyter cells in a different order from their position in the document. The brackets to the left of the cells will mark the order of execution, and the variables will always keep their values after the last executed cell, **¡so be careful!**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1620b3-2c86-49f6-a244-af617f1acd6b",
   "metadata": {},
   "source": [
    "## Training and evaluation of the model\n",
    "\n",
    "Once again, we will train a multivariate linear regression model. This time, we are going to train it first on the original, non-normalised dataset, and then retrain it on the normalised dataset, in order to compare both models and training processes and see the effects of normalisation.\n",
    "\n",
    "To do this you must copy the cells or code from previous exercises and train a multivariate linear regression model, optimized by gradient descent, on the original dataset.\n",
    "\n",
    "You must also copy the cells that test the training of the model, representing the cost function vs. the number of iterations.\n",
    "\n",
    "You do not need to make predictions about this data or evaluate the model’s residuals. In order to compare them, we will do so only on the basis of the final cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed296163-cc10-42b6-9b94-ff5d661891f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a linear regression model and plot the evolution of its cost function\n",
    "# Use the non-normalised X\n",
    "# Add the suffix \"_no_norm\" to the Theta and j_hist variables returned by your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a01c7-aa0e-46ad-85fe-0c95f9513732",
   "metadata": {},
   "source": [
    "## Data normalisation\n",
    "\n",
    "We are going to normalise the data from the original dataset.\n",
    "\n",
    "To do this, we are going to create a normalisation function that applies the necessary transformation, according to the formula:\n",
    "\n",
    "$x_j = \\frac{x_j - \\mu_{j}}{\\sigma_{j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29339c02-1f97-44bc-b6b9-b8fe446c6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a normalisation function to a common range and with a mean of 0\n",
    "\n",
    "def normalize(x, mu, std):\n",
    "    \"\"\" Normalise a dataset with X examples\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the examples, no bias term\n",
    "    mu -- Numpy 1D vector with the mean of each feature/column\n",
    "    std -- Numpy 1D vector with the standard deviation of each feature/column\n",
    "    \n",
    "    Return:\n",
    "    x_norm -- Numpy 2D array with the examples, and their normalised features\n",
    "    \"\"\"\n",
    "    return [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4bb8d-62ad-4a82-94fc-b653dbdce5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the original dataset using your normalisation function\n",
    "\n",
    "# # Find the mean and standard deviation of the features of X (columns), except the first column (bias).\n",
    "mu = [...]\n",
    "std = [...]\n",
    "\n",
    "print('original X:')\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('Mean and standard deviation of the features':')\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('normalised X:')\n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std)    # Normalise only column 1 and the subsequent columns, not column 0\n",
    "print(X_norm)\n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a478df-2bfe-44d0-a8dd-9803bc10d7d2",
   "metadata": {},
   "source": [
    "*BONUS:*\n",
    "1. Calculate the means and standard deviations of *X_norm* according to its features/columns.\n",
    "1. Compare them with those of *X*, *mu*, and *std*\n",
    "1. 3.	Plot the distributions of *X* and *X_norm* in a bar graph or box plot (you can use multiple Matplotlib subplots)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2282464-871d-4f5a-b37d-1fddc565deab",
   "metadata": {},
   "source": [
    "## Reentrenamiento del modelo y comparación de resultados\n",
    "\n",
    "Ahora reentrena el modelo sobre el dataset normalizado. Comprueba el coste final y la iteración en la que ha convergido.\n",
    "\n",
    "Para ello, puedes volver a las celdas de entrenar el modelo y comprobar la evolución de la función de coste y modificar la *X* utilizada por *X_norm*.\n",
    "\n",
    "En muchos casos, al ser un modelo tan simple, puede que no se aprecie ninguna mejora. En función de la capacidad de tu entorno de trabajo, prueba a utilizar un nº mayor de características y en aumentar ligeramente el término de error del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504bb63c-dab8-4ef1-9290-ec401dd48e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo de regresión lineal y representa gráficamente la evolución de su función de coste\n",
    "# Usa la X normalizada\n",
    "# Añádele el sufijo \"_norm\" a las variables Theta y j_hist que devuelve tu modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7ab5a-96a6-4cfd-965e-87cb06074c57",
   "metadata": {},
   "source": [
    "*PREGUNTA: ¿Ha habido diferencias en la precisión y tiempo de entrenamiento entre el modelo sobre datos no normalizados y el modelo sobre datos normalizados? Si incrementas el término de error y la diferencia de medias y rangos entre las características, ¿se aprecia mayor diferencia?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db510f96-5b5a-405c-9b18-53335a1380d7",
   "metadata": {},
   "source": [
    "## Cuidado con la Theta original\n",
    "\n",
    "Para el dataset original, antes de normalizarlo, se cumplía la relación $Y = X \\times \\Theta$.\n",
    "\n",
    "Sin embargo, ahora hemos modificado la *X* de dicha función.\n",
    "\n",
    "Por tanto, comprueba qué sucede si quieres volver a computar la *Y* usando la *X* normalizada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c9276-898b-4075-8f85-14a6433a08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba si hay diferencias entre la Y original y la Y usando la X normalizada\n",
    "\n",
    "# Comprueba el valor de Y al multiplicar X_norm y Theta_verd\n",
    "Y_norm = [...]\n",
    "\n",
    "# Comprueba si hay diferencias entre Y_norm e Y\n",
    "diff = Y_norm - Y\n",
    "\n",
    "print('Diferencias entre Y_norm e Y (primeras 10 filas):')\n",
    "print(diff[:10])\n",
    "\n",
    "# Representa en un gráfico de puntos la diferencia entre Ys vs X\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d7eb8f-449c-4314-a6e0-a2c96da46f05",
   "metadata": {},
   "source": [
    "### Realizar predicciones\n",
    "\n",
    "Del mismo modo, ¿qué sucede cuando vamos a utilizar el modelo para realizar predicciones?\n",
    "\n",
    "Genera un nuevo conjunto de datos *X_pred* siguiendo el mismo método que usaste para el dataset *X* original, incorporando el término de bias, multiplicando sus características por un rango y sumándoles valores diferentes, sin normalizarlo finalmente.\n",
    "\n",
    "También calcula su *Y_pred_verd* (sin término de error), como valor verdadero de *Y* a intentar predecir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58266bfc-775e-4b71-968d-efec6049e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un nuevo dataset de menor nº de ejemplos e igual nº de características que el dataset original\n",
    "# Asegúrate de que tiene una media o rango normalizado entre características/columnas\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "Y_pred_verd = np.matmul(X_pred, Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c7760-e745-4c87-9f65-d38379bd1c69",
   "metadata": {},
   "source": [
    "Ahora comprueba si habría alguna diferencia entre la *Y_pred_verd* y la *Y_pred* que predeciría tu modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfa630-2986-48a8-98a7-1e5c2397b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba las diferencias entre la Y real y la Y predicha\n",
    "\n",
    "Y_pred = np.matmul(X_pred, theta)\n",
    "\n",
    "diff = Y_pred_verd - Y_pred\n",
    "\n",
    "print('Diferencias entre la Y real y la Y predicha:')\n",
    "print(diff[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982753d9-e862-44a1-b7f1-9ed679b30e77",
   "metadata": {},
   "source": [
    "Dado que las predicciones no son correctas sino, deberíamos previamente normalizar la nueva *X_pred* antes de generar las predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb3bc9-8049-4c83-ac44-b745617db797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normaliza la X_pred\n",
    "\n",
    "X_pred[...] = normalize(X_pred[...], mu, std)\n",
    "\n",
    "print(X_pred[:10,:])\n",
    "print(X_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2adea-14ff-45bf-ac88-bb684a598abd",
   "metadata": {},
   "source": [
    "En esta ocasión no hemos generado una nueva variable diferente al normalizar, sino que sigue siendo la variable *X_pred*.\n",
    "\n",
    "Así puedes reejecutar la celda anterior para, ahora que *X_pred* está normalizada, comprobar si hay alguna diferencia entre la *Y* real y la *Y* predicha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94370b2-64ca-4445-8405-804216ebfd0b",
   "metadata": {},
   "source": [
    "Por tanto, recuerda siempre:\n",
    "- La *theta* calculada al entrenar el modelo será relativa siempre al dataset normalizado, y no se podrá usar para el dataset original, ya que a igual *Y* y distinta *X*, *Theta* debe cambiar.\n",
    "- Para hacer predicciones sobre nuevos ejemplos, antes tenemos que normalizarlos también, usando los mismos valores de medias y desviaciones típicas que usamos originalmente para entrenar el modelo."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
