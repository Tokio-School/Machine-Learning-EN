{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99297ac-4801-4c79-a2fa-901355a5801c",
   "metadata": {},
   "source": [
    "# Linear Regression: Validation, final evaluation, and metrics\n",
    "M2U3 - Exercise 4\n",
    "\n",
    "## What are we going to do?\n",
    "- Create a synthetic dataset for multivariate linear regression\n",
    "- Preprocess the data\n",
    "- We will train the model on the training subset and check its suitability\n",
    "- We will find the optimal *lambda* hyperparameter for the validation subset\n",
    "- We will evaluate the model on the test subset\n",
    "- We will make predictions about new future examples\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d2b23-f655-4774-b2b6-3033d0267eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6445a2e-fe04-4a8b-ba15-768cc351d1af",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset for linear regression\n",
    "\n",
    "We will start, as usual, by creating a synthetic dataset for this exercise.\n",
    "\n",
    "This time, for the error term, use a non-symmetric range, different from [-1, 1], such as [-a, b], with parameters *a* and *b* that you can control. In this way we can modify this distribution at later points to force a greater difference between the training and validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39806ebb-3466-4adb-aa59-8b4d50a90f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a synthetic dataset manually, with a bias term and an error term\n",
    "\n",
    "m = 1000\n",
    "n = 3\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_true = [...]\n",
    "\n",
    "error = 0.2\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta to be estimated and its dimensions:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b3e08-c416-451b-addd-c5e1aae0c95d",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "We will preprocess the data completely, to leave it ready to train the model.\n",
    "\n",
    "To preprocess the data, we will follow the steps below:\n",
    "- Randomly rearrange the data.\n",
    "- Normalise the data.\n",
    "- Divide the dataset into training, validation, and test subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e736d3-b931-4a7b-8fd2-01e1a5aeb615",
   "metadata": {},
   "source": [
    "### Randomly rearrange the dataset\n",
    "\n",
    "This time we are going to use a synthetic dataset created using random data. Therefore, it will not be necessary to rearrange the data, as it is already randomized and disorganized by default.\n",
    "\n",
    "However, we may often encounter real datasets whose data has a certain order or pattern, which can confound our training.\n",
    "\n",
    "Therefore, before starting to process the data, the first thing we need to do is to randomly reorder it, especially before splitting it into training, validation, and test subsets.\n",
    "\n",
    "*Note*: Very important! Remember to always reorder the *X* and *Y* examples and results in the same order, so that each example is assigned the same result before and after reordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d872e2-2c2c-426c-a8c8-ac367323ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the dataset\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Use an initial random state of 42, in order to maintain reproducibility\n",
    "print('Reorder X and Y:')\n",
    "X, Y = [...]\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569a638-eabc-498a-8c8f-bc80b33f1784",
   "metadata": {},
   "source": [
    "Check that *X* and *Y* have the correct dimensions and a different order than before.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3297962-8236-4036-917e-c3019db5c0de",
   "metadata": {},
   "source": [
    "## Normalise the dataset\n",
    "\n",
    "Once the data has been randomly reordered, we will proceed with the normalisation of the *X* examples dataset.\n",
    "\n",
    "To do this, copy the code cells from the previous exercises to normalise it.\n",
    "\n",
    "*Note*: In previous exercises we used 2 different code cells, one to define the normalisation function and one to normalise the dataset. You can combine both cells into one cell to save this preprocessing in a reusable cell for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ceb351-1296-4c46-bfb4-c17b66dc896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the dataset with a normalisation function\n",
    "\n",
    "def normalize(x, mu=None, std=None):\n",
    "    \"\"\" Normalises a dataset with X examples\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the examples, without bias term\n",
    "    mu -- Numpy 1D vector with the mean of each feature/column\n",
    "    std -- Numpy 1D vector with the standard deviation of each feature/column\n",
    "    \n",
    "    Return:\n",
    "    x_norm -- 2D ndarray with the examples, and their normalised features\n",
    "    mu, std -- if mu and std are None, compute and return those parameters. If not, use these parameters to normalise x without returning them\n",
    "    \"\"\"\n",
    "    return [...]\n",
    "\n",
    "# Find the mean and standard deviation of the X features (columns), except for the first one (bias)\n",
    "mu = [...]\n",
    "std = [...]\n",
    "\n",
    "print('Original X (first 10 rows and columns):')\n",
    "print(X[:10, :10])\n",
    "print(X.shape)\n",
    "\n",
    "print('Mean and standard deviation of the features:')\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('Normalised X:')\n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std)    # Normalization only for column 1 and the subsequent columns, not for column 0\n",
    "print(X_norm)\n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5baf16d-a9c5-4fea-a90c-491c328bb84c",
   "metadata": {},
   "source": [
    "### Divide the dataset into training, validation, and test subsets\n",
    "\n",
    "Finally, we will divide the dataset into the 3 subsets to be used.\n",
    "\n",
    "For this purpose, we will use a ratio of 60%/20%/20%, as we start with 1000 examples.\n",
    "As we said, for a different number of examples, we can modify the ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7ce09-47f7-4179-abcb-2ca6333f3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide the X and Y dataset into the 3 subsets according to the indicated ratios\n",
    "\n",
    "ratio = [60,20,20]\n",
    "print('Ratio:\\n', ratio, ratio[0] + ratio[1] + ratio[2])\n",
    "\n",
    "# Calculate the cutoff indices for X and Y\n",
    "# Tip: the round() function and the x.shape attribute may be useful to you\n",
    "r = [0, 0]\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('cutoff indices:\\n', r)\n",
    "\n",
    "# Tip: the np.array_split() function may be useful to you\n",
    "X_train, X_val, X_test = [...]\n",
    "Y_train, Y_val, Y_test = [...]\n",
    "\n",
    "print('Size of the subsets:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e23119-bc3d-41d4-9b54-214b818660f4",
   "metadata": {},
   "source": [
    "## Train an initial model on the training subset\n",
    "\n",
    "Before we begin optimizing the lambda hyperparameter, we will train an initial unregularized model on the training subset to check its performance and suitability, and to be sure that it makes sense to train a multivariate linear regression model on this dataset, as its features might not be suitable; there might be low correlation between them, they might not follow a linear relationship, etc.\n",
    "\n",
    "To do this, follow these steps:\n",
    "- Train an initial model, without regularization, with *lambda* at 0.\n",
    "- Plot the history of the cost function to check its evolution.\n",
    "- Retrain the model if necessary, e.g., by varying the learning rate *alpha*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99cdeeb-b962-45e3-bd5c-72551c65cd40",
   "metadata": {},
   "source": [
    "Copy the cells from previous exercises in which you implemented the regularized cost and gradient descent functions, and copy the cell where you trained the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948972f-ed3e-4400-99b3-6bc6b34c52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cells with the regularised cost and gradient descent functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee29de0-a7d9-44d4-a6a8-b681ad2d3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cell where we trained the previous model\n",
    "# Train your model on the unregularised training subset and get the final cost and the history of its evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b0c09-3c4a-4629-b971-14f229defe50",
   "metadata": {},
   "source": [
    "Check the training of the model as in previous exercises, plotting the evolution of the cost function versus the number of iterations, and copying the corresponding code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e34e6f-140a-48a4-9b7c-f1a05d957ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the evolution of the cost function vs. the number of iterations\n",
    "\n",
    "plt.figure(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962d837-1c0f-44ec-b480-6d1756769ebc",
   "metadata": {},
   "source": [
    "As we said before, review the training of your model and modify some parameters to retrain it to improve its performance, if necessary: the learning rate, the convergence point, the maximum number of iterations, etc., except for the *lambda* regularisation parameter, which must be set to 0.\n",
    "\n",
    "*Note*: This point is important, as these hyperparameters will generally be the same ones that we will use for the remainder of the optimisation of the model, so now is the time to find the right values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb720a0c-39e7-4c2e-8590-1751eb7a8e85",
   "metadata": {},
   "source": [
    "### Check for deviation or overfitting, *bias* or *variance*\n",
    "\n",
    "There is a test we can quickly do to check whether our initial model clearly suffers from deviation, variance, or has a more or less acceptable performance.\n",
    "\n",
    "We will plot the evolution of the cost function of 2 models, one trained on the first *n* examples of the training subset and the other trained on the first *n* examples of the validation subset.\n",
    "\n",
    "Since the training subset and the validation subset are not the same size, use only the same number of examples from the training subset as the total number of examples in the validation subset.\n",
    "\n",
    "To do this, train 2 models under equal conditions by copying the corresponding code cells again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415fa4e-9622-4a44-92b8-e801775128cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Establish a common theta_ini and hyperparameters for both models, to train them both on equal terms\n",
    "\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta initial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = 0.\n",
    "e = 1e-3\n",
    "iter_ = 1e3\n",
    "\n",
    "print('Hyperparameters used:')\n",
    "print('Alpha:', alpha, 'Max error:', e, 'Nº iter', iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afa142-838d-4bb6-a666-fd9da0d40343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a model without regularisation on the first n values of X_train, where n is the no. of\n",
    "# examples available in X_val\n",
    "# Use j_hist_train and theta_train as variable names to distinguish them from the other model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294b17e-4a6b-4482-b44e-8523c31a4c38",
   "metadata": {},
   "source": [
    "*Note*: Check that *theta_ini* has not been modified, or modify your code so that both models use the same *theta_ini*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037ce66-0c05-41b0-8c7b-6123d8c8c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In the same way, train a model without regularisation on X_val with the same parameters\n",
    "# Remember to use j_hist_val and theta_val as variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c039e-b4b2-4070-b525-6df346df468e",
   "metadata": {},
   "source": [
    "Now plot both evolutions on the same graph, with different colours, so that they can be compared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da673f-623f-4bb9-805e-24f299e91e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the cost evolution in both datasets on a line graph for comparison\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.title()\n",
    "plt.xlabel()\n",
    "plt.ylabel()\n",
    "\n",
    "# Use different colours for both series, and provide a legend to distinguish them\n",
    "plt.plot()\n",
    "plt.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b09e13-f75e-472c-9d99-3a58e137219e",
   "metadata": {},
   "source": [
    "With a random synthetic dataset it is difficult to overfit, as the original data will follow the same pattern, but by proceeding in this way we will be able to identify the following problems:\n",
    "\n",
    "- If the final cost in both subsets is high, there may be a problem with deviation or *bias*.\n",
    "- If the final cost for both subsets is very different from each other, there may be a problem with overfitting or *variance*, especially when the cost for the training subset is much lower than the cost for the validation subset,\n",
    "\n",
    "Recall the significance of deviation and overfitting:\n",
    "- Deviation occurs when the model cannot fit the curve of the dataset well enough, either because the features are not correct (or others are missing), or because the data has too much error, or because the model follows a different relationship, or is too simple.\n",
    "- Overfitting occurs when the model fits the dataset curve very well, too well, too closely to the examples on which it has been trained, and when it has to predict on new outcomes it does not do so correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228ba68-fb7c-459a-9d0f-e681a9594a68",
   "metadata": {},
   "source": [
    "### Test the suitability of the model\n",
    "\n",
    "As mentioned above, another reason to train an initial model is to check whether it makes sense to train a multivariate linear regression model on such a dataset.\n",
    "\n",
    "If we see that the model suffers from overfitting, we can always correct it with regularisation. However, if we see that it suffers from high deviation, i.e., that the final cost is very high, it may be that our type of model or the features chosen are not suitable for this problem.\n",
    "\n",
    "In this case, we found that the error is low enough to make further training of this multivariate linear regression model promising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94efa4-9bd6-410f-b921-d0bec64d1dd2",
   "metadata": {},
   "source": [
    "## Find the optimal *lambda* hyperparameter on the validation subset\n",
    "\n",
    "Now, in order to find the optimal *lambda*, we will train a different model for each *lambda* value to be considered on the training subset, and check its accuracy on the validation subset.\n",
    "\n",
    "We will plot the final error or cost of each model vs. the *lambda* value used, to see which model has a lower error or cost on the validation subset.\n",
    "\n",
    "In this way, we train all models on the same subset and under equal conditions (except *lambda*), and we evaluate them on a subset of data they have not seen previously, which has not been used to train them.\n",
    "\n",
    "The validation subset is therefore not used to train the model, but only to evaluate the optimal *lambda* value. Except for the previous point, where we made a quick initial assessment of the possible occurrence of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd793c-bc7e-4e69-9826-23498fddcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a model for each different *lambda* value on X_train and evaluate it on X_val\n",
    "\n",
    "lambdas = [0., 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1e0, 3e0, 1e1]\n",
    "# BONUS: Generate an array of lambdas with 10 values on a logarithmic scale between 10^-3 and 10, alternating between values whose first non-zero decimal is a 1 or a 3, like this list\n",
    "\n",
    "# Complete the code to train a different model for each value of lambda on X_train\n",
    "# Store your theta and final error/cost\n",
    "# Afterwards, evaluate its total cost on the validation subset\n",
    "\n",
    "# Store this information in the following arrays, of the same size as the lambda arrays\n",
    "j_train = [...]\n",
    "j_val = [...]\n",
    "theta_val = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7788b-7d4b-4ce7-a715-565d86aebd66",
   "metadata": {},
   "source": [
    "Once all models have been trained, on a line graph plot their final cost on the training subset and the final cost on the validation subset vs. the *lambda* value used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2871a-8f2c-4593-84e4-107a2be2c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the final error for each value of lambda\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "# Fill in your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3216c6-47d6-4b40-a8b4-1ec094f0a8f7",
   "metadata": {},
   "source": [
    "Once these final errors are plotted, we can automatically choose the model with the optimal *lambda* value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8b282-aee5-41a5-ae16-4e713d172003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose the optimal model and lambda value, with the lowest error on the validation subset\n",
    "\n",
    "# Iterate over the theta and lambda of all the models and choose the one with the lowest cost on the validation subset\n",
    "\n",
    "j_final = [...]\n",
    "theta_final = [...]\n",
    "lambda_final = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4f7a4-caaf-4e1c-b2ef-ff8cedea8881",
   "metadata": {},
   "source": [
    "Once all the above steps have been implemented, we have our trained model and its hyperparameters optimised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555a61a-22f0-4e65-9646-d6a5ba2b9210",
   "metadata": {},
   "source": [
    "## Finally, evaluate the model on the test subset\n",
    "\n",
    "Finally, we have found our optimal *theta* and *lambda* hyperparameter coefficients, so we now have a trained, ready-to-be-used model.\n",
    "\n",
    "However, although we have calculated its error or final cost on the validation subset, we have used this subset to select the model or to \"finish training it\". Therefore, we have not yet tested how this model will work on data it has never seen before.\n",
    "\n",
    "To do this, we will finally evaluate it on the test subset, on a subset that we have not yet used to either train the model or to select its hyperparameters. A separate subset that the model training has not yet seen.\n",
    "\n",
    "Therefore, we will calculate the total error or cost on the test subset and graphically check the residuals of the model on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7ab30-bc8b-4a48-83be-fdaffad5bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the error of the model on the test subset using the cost function with the corresponding\n",
    "# theta and lambda\n",
    "\n",
    "j_test = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79b916-6359-4d68-b4ec-79175145cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the predictions of the model on the test subset, its residuals, and plot them\n",
    "\n",
    "Y_test_pred = [...]\n",
    "\n",
    "residuals = [...]\n",
    "\n",
    "plt.figure(4)\n",
    "\n",
    "# Fill in your code\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5fffa-6f8e-4c90-ba0b-87d178f663c4",
   "metadata": {},
   "source": [
    "In this way we can get a more realistic idea of how accurate our model is and how it will behave with new examples in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e93c93-b0e0-4f8f-afd6-e3e806fb55ea",
   "metadata": {},
   "source": [
    "## Make predictions about new future examples\n",
    "\n",
    "With our model trained, optimised, and evaluated, all that remains is to put it to work by making predictions with new examples.\n",
    "\n",
    "To do this, we will:\n",
    "- Generate a new example, following the same pattern as the original dataset.\n",
    "- Normalise its features before predictions can be made about them.\n",
    "- Generate a prediction for this new example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d7548-a5b0-4e89-ba76-3f7a61f09a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Generate a new example following the original pattern, with a bias term and a random error term\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "# Normalise its features (except the bias term) to the original means and standard deviations\n",
    "X_pred = [...]\n",
    "\n",
    "# Generate a prediction for this new example\n",
    "Y_pred = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda6132-a3cd-4929-a8b5-3b6e176ee48a",
   "metadata": {},
   "source": [
    "## Data Preprocessing with Scikit-learn\n",
    "\n",
    "Finally, find and use the functions available in Scikit-learn to preprocess data:\n",
    "1. [Randomly reordering](https://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html?highlight=shuffle#sklearn.utils.shuffle)\n",
    "1. [Normalising/scaling](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)\n",
    "1. [Dividing the data into the 3 corresponding subsets](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=split#sklearn.model_selection.train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa80db3-5c42-4e02-b3f7-b15413bcf832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: : Use Scikit-learn functions to randomly reorder, normalise, and split the data into training, validation, and test subsets\n",
    "# Use the original X instead of X_norm\n",
    "\n",
    "X_reord = [...]\n",
    "\n",
    "X_escalated = [...]\n",
    "\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9623f78-2089-4020-8444-f12f06c14633",
   "metadata": {},
   "source": [
    "*BONUS*: ¿Can you correct your code to apply these standard operations in as few lines as possible and leave it ready for reuse every time?"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
