{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b71ed5-8c08-462d-a3f3-bdfaa4b03bb5",
   "metadata": {},
   "source": [
    "# Linear Regression: Regularisation\n",
    "M2U3 - Exercise 2\n",
    "\n",
    "## What are we going to do?\n",
    "- We will implement a regularised cost function for multivariate linear regression\n",
    "- We will implement the regularisation for gradient descent\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b9db2-3c8c-4412-8a3e-059b0a4bf801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8fa69-2cd8-4002-ab2c-13a5ae4ff396",
   "metadata": {},
   "source": [
    "## Creation of a synthetic dataset\n",
    "\n",
    "To test your implementation of a regularised gradient descent and cost function, retrieve your cells from the previous notebooks on synthetic datasets and generate a dataset for this exercise.\n",
    "\n",
    "Don't forget to add a bias term to *X* and an error term to *Y*, initialized to 0 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b400b5-27b7-4ed9-b950-1475b0d722a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Manually generate a synthetic dataset, with a bias term and an error term initialised to 0\n",
    "\n",
    "m = 1000\n",
    "n = 3\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_true = [...]\n",
    "\n",
    "error = 0.\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta to be estimated and its dimensions:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1e349-ffff-4df9-9597-17e6b71a7806",
   "metadata": {},
   "source": [
    "## Regularised cost function\n",
    "\n",
    "We will now modify our implementation of the cost function from the previous exercise to add the regularisation term.\n",
    "\n",
    "Recall that the regularised cost function is:\n",
    "\n",
    "$$ h_\\theta(x^i) = Y = X \\times \\Theta^T $$\n",
    "$$J_\\theta = \\frac{1}{2m} [\\sum\\limits_{i=0}^{m} (h_\\theta(x^i)-y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f2237-8286-40db-bf9c-14e5da6b298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the regularised cost function according to the following template\n",
    "\n",
    "def regularized_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computes the cost function for the considered dataset and coefficients.\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the values of the independent variables from the examples, of size m x n\n",
    "    y -- Numpy 1D array with the dependent/target variable, of size m x 1\n",
    "    theta -- Numpy 1D array with the weights of the model coefficients, of size 1 x n (row vector)\n",
    "    \n",
    "    Named arguments:\n",
    "    lambda -- float with the regularisation parameter\n",
    "    \n",
    "    Return:\n",
    "    j -- float with the cost for this theta array\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Remember to check the dimensions of the matrix multiplication to perform it correctly\n",
    "    # Remember not to regularize the coefficient of the bias parameter (first value of theta)\n",
    "    j = [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd42e2-4677-4aab-b76c-f48a2d89c52b",
   "metadata": {},
   "source": [
    "*NOTE:* Check that the function simply returns a float value, and not an array or matrix. Use the `ndarray.resize((size0, size1))` method if you need to change the dimensions of any array before you multiply it with `np.matmul()` and make sure the result dimensions match, or returns `j[0,0]` as the `float` value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71362cca-7836-4e76-b05e-1f8036b2566a",
   "metadata": {},
   "source": [
    "As the synthetic dataset has the error term set at 0, the result of the cost function for the *Theta_true* with parameter *lambda* = 0 must be exactly 0.\n",
    "\n",
    "As before, as we move away with different values of θ, the cost should increase. Similarly, the higher the *lambda* regularisation parameter, the higher the penalty and cost, and the higher the *Theta* value, the higher the penalty and cost as well.\n",
    "\n",
    "Check your implementation in these 5 scenarios:\n",
    "1. Using *Theta_true* and with *lambda* at 0, the cost should still be 0.\n",
    "1. With *lambda* still at 0, as the value of *theta* moves away from *Theta_true*, the cost should increase.\n",
    "1. Using *Theta_true* and with a *lambda* other than 0, the cost must now be greater than 0.\n",
    "1. With a *lambda* other than 0, for a *theta* other than *Theta_true*, the cost must be higher than with *lambda* equal to 0.\n",
    "1. With a *lambda* other than 0, the higher the values of the coefficients of *theta* (positive or negative), the higher the penalty and the higher the cost.\n",
    "\n",
    "Recall that the value of lambda must always be positive and generally less than 0: `[0, 1e-1, 3e-1, 1e-2, 3e-2, ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11d5ff-ea2c-4dbb-b8f0-b01801dec7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check the implementation of your regularised cost function in these scenarios\n",
    "\n",
    "theta = Theta_true    # Modify and test various values of theta\n",
    "\n",
    "j = regularized_cost_function(X, Y, theta)\n",
    "\n",
    "print('Cost of the model:')\n",
    "print(j)\n",
    "print('Tested Theta and actual Theta:')\n",
    "print(theta)\n",
    "print(Theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3de9e-cea7-4544-bc01-d36e56fb8a00",
   "metadata": {},
   "source": [
    "Record your experiments and results in this cell (in Markdown or code):\n",
    "1. Experiment 1\n",
    "1. Experiment 2\n",
    "1. Experiment 3\n",
    "1. Experiment 4\n",
    "1. Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9b45f-a805-471e-bcf2-ed4b978e77a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regularised gradient descent\n",
    "\n",
    "Now we will also regularise the training by gradient descent. We will modify the *Theta* updates so that they now also contain the *lambda* regularisation parameter:\n",
    "\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i + \\frac{\\lambda}{m} \\theta_j]; \\space j \\in [1, n] $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$\n",
    "\n",
    "Remember to build again on your previous implementation of the gradient descent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bbcb9-5f11-4284-8d6c-2c673ef60051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the function that trains the regularised gradient descent model\n",
    "\n",
    "def regularized_gradient_descent(x, y, theta, alpha, lambda_=0., e, iter_):\n",
    "    \"\"\" Trains the model by optimising its cost function using gradient descent\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the values of the independent variables from the examples, of size m x n\n",
    "    y -- Numpy 1D array with the dependent/target variable, of size m x 1\n",
    "    theta -- Numpy 1D array with the weights of the model coefficients, of size 1 x n (row vector)\n",
    "    alpha -- float, training rate\n",
    "    \n",
    "    Argumentos nombrados (keyword):\n",
    "    lambda -- float con el parámetro de regularización\n",
    "    e -- float, diferencia mínima entre iteraciones para declarar que el entrenamiento ha convergido finalmente\n",
    "    iter_ -- int/float, nº de iteraciones\n",
    "    \n",
    "    Devuelve:\n",
    "    j_hist -- list/array con la evolución de la función de coste durante el entrenamiento\n",
    "    theta -- array de Numpy con el valor de theta en la última iteración\n",
    "    \"\"\"\n",
    "    # TODO: declara unos valores por defecto para e e iter_ en los argumentos nombrados (keyword) de la función\n",
    "    \n",
    "    iter_ = int(iter_)    # Si has declarado iter_ en notación científica (1e3) o float (1000.), conviértelo\n",
    "    \n",
    "    # Inicializa j_hist como una list o un array de Numpy. Recuerda que no sabemos qué tamaño tendrá finalmente\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...]    # Obtén m y n a partir de las dimensiones de X\n",
    "    \n",
    "    for k in [...]:    # Itera sobre el nº de iteraciones máximo\n",
    "        # Declara una theta para cada iteración como \"deep copy\" de theta, ya que debemos actualizarla valor a valor\n",
    "        theta_iter = [...]\n",
    "        \n",
    "        for j in [...]:    # Itera sobre el nº de características\n",
    "            # Actualiza theta_iter para cada característica, según la derivada de la función de coste\n",
    "            # Incluye el ratio de entrenamiento alpha\n",
    "            # Cuidado con las multiplicaciones matriciales, su órden y dimensiones\n",
    "            \n",
    "            if j > 0:\n",
    "                # Regulariza todo coeficiente excepto el del parámetro bias (primer coef.)\n",
    "                pass\n",
    "            \n",
    "            theta_iter[j] = theta[j] - [...]\n",
    "            \n",
    "        theta = theta_iter\n",
    "        \n",
    "        cost = cost_function([...])    # Calcula el coste para la iteración de theta actual\n",
    "        \n",
    "        j_hist[...]    # Añade el coste de la iteración actual al histórico de costes\n",
    "        \n",
    "        # Comprueba si la diferencia entre el coste de la iteración actual y el de la última iteración en valor\n",
    "        # absoluto son menores que la diferencia mínima para declarar convergencia, e\n",
    "        if k > 0 and [...]:\n",
    "            print('Converge en la iteración nº: ', k)\n",
    "            \n",
    "            break\n",
    "    else:\n",
    "        print('Nº máx. de iteraciones alcanzado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e39cc-536d-46fa-9ec1-e7fa4b654803",
   "metadata": {},
   "source": [
    "*Nota*: Recuerda que las plantillas de código son sólo una ayuda. En ocasiones, puede que quieras usar un código diferente con la misma funcionalidad, p. ej. que itere sobre los elementos de otra forma, etc. ¡Síentete libre de modificarlos a tu antojo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfea771-4dc6-45ed-824d-194542993aa2",
   "metadata": {},
   "source": [
    "## Comprobación del gradient descent regularizado\n",
    "\n",
    "Para comprobar tu implementación de nuevo, comprueba con *lambda* a 0 usando varios valores de *theta_ini*, tanto con la *Theta_verd* como valores cada vez más alejados de ella, y comprueba que finalmente el modelo converge a la *Theta_verd*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d91e9-3a92-4a13-91aa-261f2284c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba tu implementación entrenando un modelo sobre el dataset sintético creado previamente\n",
    "\n",
    "# Crea una theta inicial con un valor dado, aleatorio o escogido a mano\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = 0.\n",
    "e = 1e-3\n",
    "iter_ = 1e3    # Comprueba que tu función puede admitir valores float o modifícalo\n",
    "\n",
    "print('Hiper-arámetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = regularized_gradient_descent([...])\n",
    "\n",
    "print('Tiempo de entrenamiento (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores de la función de coste')\n",
    "print(j_hist[...])\n",
    "print('\\Coste final:')\n",
    "print(j_hist[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdaderos de Theta y diferencia con valores entrenados:')\n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4affe7-77f8-475f-9f65-6bdf11d7a05d",
   "metadata": {},
   "source": [
    "Ahora comprueba de nuevo el entrenamiento de un modelo en algunas de las circunstancias anteriores:\n",
    "1. Usando una *theta_ini* aleatoria y con *lambda* a 0, el coste final debe seguir siendo cercano a 0 y la *theta* final cercana a *Theta_verd*.\n",
    "1. Usando una *theta_ini* aleatoria y con *lambda* pequeña y distinta de 0, el coste final debe ser cercano a 0, aunque el modelo puede empezar a tener peor precisión.\n",
    "1. Según aumenta el valor de *lambda*, el modelo perderá más precisión.\n",
    "\n",
    "Para ello recuerda que puedes modificar los valores de las celdas y reejecutarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677020e7-cc8d-4cb8-830a-58ac8aef8fa7",
   "metadata": {},
   "source": [
    "Anota tus experimentos y resultados en esta celda (en Markdown o código):\n",
    "1. Experimento 1\n",
    "1. Experimento 2\n",
    "1. Experimento 3\n",
    "1. Experimento 4\n",
    "1. Experimento 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ccd072-2f3d-46d1-8663-5ec01b5ad3c7",
   "metadata": {},
   "source": [
    "## ¿Por qué necesitábamos utilizar regularización?\n",
    "\n",
    "El objetivo de la regularización era penalizar el modelo cuando sufre sobre-ajuste, cuando el modelo comienza a memorizar resultados más que aprender a generalizar.\n",
    "\n",
    "Ésto supone un problema cuando los datos de entrenamiento y sobre los que debemos hacer predicciones en producción siguen distribuciones significativamente diferentes.\n",
    "\n",
    "Para comprobar nuestro entrenamiento con descenso de gradiente regularizado, vuelve al apartado de generación del dataset y genera uno con un ratio de ejemplos a características bastante menor y con un ratio de error bastante superior.\n",
    "\n",
    "Comienza a jugar con dichos valores y luego ve modificando la *lambda* del modelo para ver si un valor de *lambda* diferente a 0 comienza a tener más precisión que *lambda* = 0."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
