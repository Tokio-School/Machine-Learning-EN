{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b71ed5-8c08-462d-a3f3-bdfaa4b03bb5",
   "metadata": {},
   "source": [
    "# Linear Regression: Regularisation\n",
    "M2U3 - Exercise 2\n",
    "\n",
    "## What are we going to do?\n",
    "- We will implement a regularised cost function for multivariate linear regression\n",
    "- We will implement the regularisation for gradient descent\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b9db2-3c8c-4412-8a3e-059b0a4bf801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8fa69-2cd8-4002-ab2c-13a5ae4ff396",
   "metadata": {},
   "source": [
    "## Creation of a synthetic dataset\n",
    "\n",
    "To test your implementation of a regularised gradient descent and cost function, retrieve your cells from the previous notebooks on synthetic datasets and generate a dataset for this exercise.\n",
    "\n",
    "Don't forget to add a bias term to *X* and an error term to *Y*, initialized to 0 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b400b5-27b7-4ed9-b950-1475b0d722a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Manually generate a synthetic dataset, with a bias term and an error term initialised to 0\n",
    "\n",
    "m = 1000\n",
    "n = 3\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_true = [...]\n",
    "\n",
    "error = 0.\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta to be estimated and its dimensions:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1e349-ffff-4df9-9597-17e6b71a7806",
   "metadata": {},
   "source": [
    "## Regularised cost function\n",
    "\n",
    "We will now modify our implementation of the cost function from the previous exercise to add the regularisation term.\n",
    "\n",
    "Recall that the regularised cost function is:\n",
    "\n",
    "$$ h_\\theta(x^i) = Y = X \\times \\Theta^T $$\n",
    "$$J_\\theta = \\frac{1}{2m} [\\sum\\limits_{i=0}^{m} (h_\\theta(x^i)-y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f2237-8286-40db-bf9c-14e5da6b298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the regularised cost function according to the following template\n",
    "\n",
    "def regularized_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computes the cost function for the considered dataset and coefficients.\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the values of the independent variables from the examples, of size m x n\n",
    "    y -- Numpy 1D array with the dependent/target variable, of size m x 1\n",
    "    theta -- Numpy 1D array with the weights of the model coefficients, of size 1 x n (row vector)\n",
    "    \n",
    "    Named arguments:\n",
    "    lambda -- float with the regularisation parameter\n",
    "    \n",
    "    Return:\n",
    "    j -- float with the cost for this theta array\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Remember to check the dimensions of the matrix multiplication to perform it correctly\n",
    "    # Remember not to regularize the coefficient of the bias parameter (first value of theta)\n",
    "    j = [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd42e2-4677-4aab-b76c-f48a2d89c52b",
   "metadata": {},
   "source": [
    "*NOTE:* Check that the function simply returns a float value, and not an array or matrix. Use the `ndarray.resize((size0, size1))` method if you need to change the dimensions of any array before you multiply it with `np.matmul()` and make sure the result dimensions match, or returns `j[0,0]` as the `float` value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71362cca-7836-4e76-b05e-1f8036b2566a",
   "metadata": {},
   "source": [
    "As the synthetic dataset has the error term set at 0, the result of the cost function for the *Theta_true* with parameter *lambda* = 0 must be exactly 0.\n",
    "\n",
    "As before, as we move away with different values of θ, the cost should increase. Similarly, the higher the *lambda* regularisation parameter, the higher the penalty and cost, and the higher the *Theta* value, the higher the penalty and cost as well.\n",
    "\n",
    "Check your implementation in these 5 scenarios:\n",
    "1. Using *Theta_true* and with *lambda* at 0, the cost should still be 0.\n",
    "1. With *lambda* still at 0, as the value of *theta* moves away from *Theta_true*, the cost should increase.\n",
    "1. Using *Theta_true* and with a *lambda* other than 0, the cost must now be greater than 0.\n",
    "1. With a *lambda* other than 0, for a *theta* other than *Theta_true*, the cost must be higher than with *lambda* equal to 0.\n",
    "1. With a *lambda* other than 0, the higher the values of the coefficients of *theta* (positive or negative), the higher the penalty and the higher the cost.\n",
    "\n",
    "Recall that the value of lambda must always be positive and generally less than 0: `[0, 1e-1, 3e-1, 1e-2, 3e-2, ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11d5ff-ea2c-4dbb-b8f0-b01801dec7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check the implementation of your regularised cost function in these scenarios\n",
    "\n",
    "theta = Theta_true    # Modify and test various values of theta\n",
    "\n",
    "j = regularized_cost_function(X, Y, theta)\n",
    "\n",
    "print('Cost of the model:')\n",
    "print(j)\n",
    "print('Tested Theta and actual Theta:')\n",
    "print(theta)\n",
    "print(Theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3de9e-cea7-4544-bc01-d36e56fb8a00",
   "metadata": {},
   "source": [
    "Record your experiments and results in this cell (in Markdown or code):\n",
    "1. Experiment 1\n",
    "1. Experiment 2\n",
    "1. Experiment 3\n",
    "1. Experiment 4\n",
    "1. Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9b45f-a805-471e-bcf2-ed4b978e77a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regularised gradient descent\n",
    "\n",
    "Now we will also regularise the training by gradient descent. We will modify the *Theta* updates so that they now also contain the *lambda* regularisation parameter:\n",
    "\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i + \\frac{\\lambda}{m} \\theta_j]; \\space j \\in [1, n] $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$\n",
    "\n",
    "Remember to build again on your previous implementation of the gradient descent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bbcb9-5f11-4284-8d6c-2c673ef60051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the function that trains the regularised gradient descent model\n",
    "\n",
    "def regularized_gradient_descent(x, y, theta, alpha, lambda_=0., e, iter_):\n",
    "    \"\"\" Trains the model by optimising its cost function using gradient descent\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the values of the independent variables from the examples, of size m x n\n",
    "    y -- Numpy 1D array with the dependent/target variable, of size m x 1\n",
    "    theta -- Numpy 1D array with the weights of the model coefficients, of size 1 x n (row vector)\n",
    "    alpha -- float, training rate\n",
    "    \n",
    "    Named arguments (keyword):\n",
    "    lambda -- float with the regularisation parameter\n",
    "    e -- float, minimum difference between iterations to declare that the training has finally converged\n",
    "    iter_ -- int/float, nº of iterations\n",
    "    \n",
    "    Return:\n",
    "    j_hist -- list/array with the evolution of the cost function during the training\n",
    "    theta -- Numpy array with the value of theta at the last iteration\n",
    "    \"\"\"\n",
    "    # TODO: enters default values for e and iter_ in the function keyword arguments\n",
    "    \n",
    "    iter_ = int(iter_)    # If you have entered iter_ in scientific notation (1e3) or float (1000.), convert it\n",
    "    \n",
    "    # Initialise j_hist as a list or a Numpy array. Remember that we do not know what size it will eventually be\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...]    # Obtain m and n from the dimensions of X\n",
    "    \n",
    "    for k in [...]:    # Iterate over the maximum nº of iterations\n",
    "        # Declare a theta for each iteration as a \"deep copy\" of theta, since we must update it value by value\n",
    "        theta_iter = [...]\n",
    "        \n",
    "        for j in [...]:    # Iterate over the nº of features\n",
    "            # Update theta_iter for each feature, according to the derivative of the cost function\n",
    "            # Include the training rate alpha\n",
    "            # Careful with the matrix multiplication, its order and dimensions\n",
    "            \n",
    "            if j > 0:\n",
    "                # Regularise all coefficients except for the bias parameter (first coef.)\n",
    "                pass\n",
    "            \n",
    "            theta_iter[j] = theta[j] - [...]\n",
    "            \n",
    "        theta = theta_iter\n",
    "        \n",
    "        cost = cost_function([...])    # Calculates the cost for the current theta iteration\n",
    "        \n",
    "        j_hist[...]    # Adds the cost of the current iteration to the cost history.\n",
    "        \n",
    "        # Check if the difference between the cost of the current iteration and that of the last iteration in absolute value\n",
    "        # is less than the minimum difference to declare convergence, e\n",
    "        if k > 0 and [...]:\n",
    "            print('Converge at iteration nº: ', k)\n",
    "            \n",
    "            break\n",
    "    else:\n",
    "        print('Max n1 of iterations reached')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e39cc-536d-46fa-9ec1-e7fa4b654803",
   "metadata": {},
   "source": [
    "*Note*: Remember that the code templates are only an aid. Sometimes, you may want to use different code with the same functionality, e.g., iterate over elements in a different way, etc. Feel free to modify them as you wish!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfea771-4dc6-45ed-824d-194542993aa2",
   "metadata": {},
   "source": [
    "## Checking the regularised gradient descent\n",
    "\n",
    "To check your implementation again, check with *lambda* at 0 using various values of *theta_ini*, both with the *Theta_true* and values further and further away from it, and check that eventually the model converges to the *Theta_true*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d91e9-3a92-4a13-91aa-261f2284c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test your implementation by training a model on the previously created synthetic dataset\n",
    "\n",
    "# Create an initial theta with a given, random, or hand-picked value\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = 0.\n",
    "e = 1e-3\n",
    "iter_ = 1e3    # Comprueba que tu función puede admitir valores float o modifícalo\n",
    "\n",
    "print('Hiper-arámetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = regularized_gradient_descent([...])\n",
    "\n",
    "print('Tiempo de entrenamiento (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores de la función de coste')\n",
    "print(j_hist[...])\n",
    "print('\\Coste final:')\n",
    "print(j_hist[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdaderos de Theta y diferencia con valores entrenados:')\n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4affe7-77f8-475f-9f65-6bdf11d7a05d",
   "metadata": {},
   "source": [
    "Ahora comprueba de nuevo el entrenamiento de un modelo en algunas de las circunstancias anteriores:\n",
    "1. Usando una *theta_ini* aleatoria y con *lambda* a 0, el coste final debe seguir siendo cercano a 0 y la *theta* final cercana a *Theta_verd*.\n",
    "1. Usando una *theta_ini* aleatoria y con *lambda* pequeña y distinta de 0, el coste final debe ser cercano a 0, aunque el modelo puede empezar a tener peor precisión.\n",
    "1. Según aumenta el valor de *lambda*, el modelo perderá más precisión.\n",
    "\n",
    "Para ello recuerda que puedes modificar los valores de las celdas y reejecutarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677020e7-cc8d-4cb8-830a-58ac8aef8fa7",
   "metadata": {},
   "source": [
    "Anota tus experimentos y resultados en esta celda (en Markdown o código):\n",
    "1. Experimento 1\n",
    "1. Experimento 2\n",
    "1. Experimento 3\n",
    "1. Experimento 4\n",
    "1. Experimento 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ccd072-2f3d-46d1-8663-5ec01b5ad3c7",
   "metadata": {},
   "source": [
    "## ¿Por qué necesitábamos utilizar regularización?\n",
    "\n",
    "El objetivo de la regularización era penalizar el modelo cuando sufre sobre-ajuste, cuando el modelo comienza a memorizar resultados más que aprender a generalizar.\n",
    "\n",
    "Ésto supone un problema cuando los datos de entrenamiento y sobre los que debemos hacer predicciones en producción siguen distribuciones significativamente diferentes.\n",
    "\n",
    "Para comprobar nuestro entrenamiento con descenso de gradiente regularizado, vuelve al apartado de generación del dataset y genera uno con un ratio de ejemplos a características bastante menor y con un ratio de error bastante superior.\n",
    "\n",
    "Comienza a jugar con dichos valores y luego ve modificando la *lambda* del modelo para ver si un valor de *lambda* diferente a 0 comienza a tener más precisión que *lambda* = 0."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
