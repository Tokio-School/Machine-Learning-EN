{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e3e64e-14a0-4779-83f8-c94bf9837a8e",
   "metadata": {},
   "source": [
    "# Decision Trees: Scikit-Learn\n",
    "M2U5 - Exercise 1\n",
    "\n",
    "## What are we going to do?\n",
    "- We will train a linear regression model using decision trees\n",
    "- We will check to see if there is any deviation or overfitting in the model\n",
    "- We will optimise the hyperparameters with validation\n",
    "- We will evaluate the model on the test subset\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "We are going to solve a multivariate linear regression problem similar to the previous exercises, but this time using a decision tree for linear regression.\n",
    "\n",
    "An example that you can use as a reference for this exercise: [Decision Tree Regression](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95184942-7bea-4ee5-8d0c-656fb4c48320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import all the necessary modules into this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1c0ae-9e95-43d4-8957-75d94c6d8f8f",
   "metadata": {},
   "source": [
    "## Generate a synthetic dataset\n",
    "\n",
    "Generate a synthetic dataset with a fairly large error term and few features, manually or with Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ad192-bcbe-4785-88a9-3b5d93f7fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a synthetic dataset, with few features and a significant error term\n",
    "# Do not add a bias term to X\n",
    "\n",
    "m = 1000\n",
    "n = 2\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_true = [...]\n",
    "\n",
    "error = 0.3\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta and its dimensions to be estimated:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74217537-2ce8-4899-a3a2-4a4167403df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Graphically represent the dataset in 3D to ensure that the error term is sufficiently high\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "plt.title()\n",
    "plt.xlabel()\n",
    "plt.ylabel()\n",
    "\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc5107-d438-4274-ae4b-276caacf5717",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "- Randomly reorder the data.\n",
    "- Normalise the data.\n",
    "- Divide the dataset into training and test subsets.\n",
    "\n",
    "*Note*: We will use K-fold again for the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089aed7-ffe9-4fca-876b-1309f1c2bae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the data, normalise the examples, and divide them into training and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838bfae5-5e7d-4d73-8248-078475322835",
   "metadata": {},
   "source": [
    "## Train an initial model\n",
    "\n",
    "We will begin exploring decision tree models for regression with an initial model.\n",
    "\n",
    "To do this, train a [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) model on the training subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143acbfd-4652-44b5-ba0f-6b4c776c35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a regression tree on the training subset with a max. depth of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694b7ab-2c5f-4e86-b054-f92c1be46861",
   "metadata": {},
   "source": [
    "Now check the suitability of the model by evaluating it on the test subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b9a6f-ca82-41ff-b085-41b772ef4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model with MSE, RMSE and R^2 on the test subset\n",
    "\n",
    "y_test_pred = [...]\n",
    "\n",
    "mse = [...]\n",
    "rmse = [...]\n",
    "r2_score = [...]\n",
    "print('mean square error: {%.2f}'.format(mse))\n",
    "print('Root mean square error: {%.2f}'.format(rmse))\n",
    "print('Coefficient of determination: {%.2f}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a626f-e438-47c6-b378-9f94e7c592f1",
   "metadata": {},
   "source": [
    "*QUESTION:*\n",
    "*Do you think there is deviation or overfitting in this model?*\n",
    "\n",
    "To find out, compare its accuracy with that calculated on the training subset and answer in this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92e6999-1e04-4944-b9bc-e21b94af45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now evaluate the model with MSE, RMSE and R^2 on the training subset\n",
    "\n",
    "y_train_pred = [...]\n",
    "\n",
    "mse = [...]\n",
    "rmse = [...]\n",
    "r2_score = [...]\n",
    "print('mean square error: {%.2f}'.format(mse))\n",
    "print('Root mean square error: {%.2f}'.format(rmse))\n",
    "print('Coefficient of determination: {%.2f}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb79b07-efd4-4f9c-a8ce-68d1b1fb23d6",
   "metadata": {},
   "source": [
    "As mentioned above, decision trees tend to overfit, to over-adjust to the data used to train them, and sometimes fail to predict well on new examples.\n",
    "\n",
    "We are going to check this graphically by training another model with a much larger maximum depth of 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4b2b5-06d3-4aa3-91e6-b7f5be9ddf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train another regression tree on the training subset with max. depth of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6fdca-0857-4f92-9947-8bfdf36645b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now evaluate the model with MSE, RMSE, and R^2 on the training subset\n",
    "\n",
    "y_train_pred = [...]\n",
    "\n",
    "mse = [...]\n",
    "rmse = [...]\n",
    "r2_score = [...]\n",
    "print('mean square error: {%.2f}'.format(mse))\n",
    "print('Root mean square error: {%.2f}'.format(rmse))\n",
    "print('Coefficient of determination: {%.2f}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17f0d8-cf13-4c8f-b728-aeedf14e2cc2",
   "metadata": {},
   "source": [
    "Compare the training accuracy of this model with the previous one (on the training subset).\n",
    "\n",
    "*QUESTION:* Is the accuracy greater or lesser as the maximum depth of the tree increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d66ae-7c62-4855-bfe3-ea9bce8e1626",
   "metadata": {},
   "source": [
    "We will now plot both models, to check whether they suffer from deviation or overfitting.\n",
    "\n",
    "To do so, you can be guided by the preceding example: [Decision Tree Regression](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e172c-0a55-4d2d-8897-788bdea1521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Graphically represent the predictions of both models\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.title([...])\n",
    "plt.xlabel([...])\n",
    "plt.ylabel([...])\n",
    "\n",
    "# Plot the training subset for features 1 and 2 (with different shapes) in a dot plot\n",
    "plt.scatter([...])\n",
    "plt.scatter([...])\n",
    "# Plot the test subset for features 1 and 2 (with different shapes) in a dot plot, with a different colour from the training subset\n",
    "plt.scatter([...])\n",
    "plt.scatter([...])\n",
    "\n",
    "# Plot the predictions of the two models on a line graph, with different colours, and a legend to distinguish them\n",
    "# Use a linear space with a large number of elements between the max. and min. value of both X features as the horizontal axis\n",
    "x_axis = [...]\n",
    "\n",
    "plt.plot([...])\n",
    "plt.plot([...])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94159082-275b-41f6-96fc-add6d750ef60",
   "metadata": {},
   "source": [
    "As we have seen, too small a max. depth generally leads to a model with deviation, a model that is not able to fit the curve well enough, while too high a max. depth leads to a model with overfitting, a model that fits the curve very well, but does not have good accuracy on future examples.\n",
    "\n",
    "Therefore, among all regression tree hyperparameters, we have the maximum depth, which we need to optimise using validation. There are also other hyperparameters, such as the criteria for measuring the quality of a split, the strategy for creating that split, the min. number of examples needed to split a node, etc.\n",
    "\n",
    "For the sake of simplicity, let's start by performing a cross-validation just to find the optimal value for the maximum depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232a8a6-5f03-4e35-a1ed-354c6b069bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a different model for each max_depth value considered on a different fold\n",
    "\n",
    "# Values of max_depth to be considered in an integer space [1, 8]\n",
    "max_depths = [...]\n",
    "print('Max. depths to be considered:')\n",
    "print(max_depths)\n",
    "\n",
    "# Create x K-fold splits, one for each value of max_depth to be considered\n",
    "kf = [...]\n",
    "\n",
    "# Iterate on the splits, train your models and evaluate them on the generated CV subset\n",
    "linear_models = []\n",
    "best_model = None\n",
    "for train, cv in kf.split(X):\n",
    "    # Train a model on the training subset\n",
    "    # Evaluate it on the cv subset using its method score()\n",
    "    # Save the model with the best score on the best_model variable and display the alpha of the best model\n",
    "    alpha = [...]\n",
    "    print('Max. depth used:', max_depth)\n",
    "    \n",
    "    linear_models.append([...])\n",
    "    \n",
    "    # If the model is better than the best model so far, update the best model found\n",
    "    best_model = [...]\n",
    "    \n",
    "    print('Max. depth and R^2 of the best tree so far:', max_depth, best_model.score([...]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184ab37-bcf6-47e5-95a3-97af7ef39a4e",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test subset\n",
    "\n",
    "Finally, we are going to evaluate the model on the test subset.\n",
    "\n",
    "To do this, calculate its MSE, RMSE, and R^2 metrics and plot the model predictions and residuals vs. the test subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3137f-e553-41b7-a4b1-bece2e99d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model with MSE, RMSE and R^2 on the test subset\n",
    "\n",
    "y_train_test = [...]\n",
    "\n",
    "mse = [...]\n",
    "rmse = [...]\n",
    "r2_score = [...]\n",
    "print('mean square error: {%.2f}'.format(mse))\n",
    "print('Root mean square error: {%.2f}'.format(rmse))\n",
    "print('Coefficient of determination: {%.2f}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75456c34-bdf9-4d1c-8d92-986575db190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the predictions of the best tree on the test subset and its residuals\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "plt.title([...])\n",
    "plt.xlabel([...])\n",
    "plt.ylabel([...])\n",
    "\n",
    "# Plot the test subset on a dot plot, representing both features with different shapes\n",
    "plt.scatter([...])\n",
    "\n",
    "# Plot the model's predictions on a line graph\n",
    "# Use a linear space with a large number of elements between the max. and min. value of the X_test features as the horizontal axis\n",
    "x_axis = [...]\n",
    "\n",
    "plt.plot([...])\n",
    "\n",
    "# Calculate the residuals and plot them as a bar chart on the horizontal axis\n",
    "residuals = [...]\n",
    "\n",
    "plt.bar([...]\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
