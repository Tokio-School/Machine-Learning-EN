{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d9c6ef-79eb-4635-9509-2936e0720f69",
   "metadata": {},
   "source": [
    "# Logistic Regression: Multiclass classification\n",
    "M2U5 - Exercise 6\n",
    "\n",
    "## What are we going to do?\n",
    "- We will create a synthetic dataset for multiclass logistic regression\n",
    "- We will preprocess the data\n",
    "- We will train the model on the training subset and check its suitability\n",
    "- We will find the optimal lambda regularization parameter using CV\n",
    "- We will make predictions about new examples\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "Having implemented the full training of a regularised logistic regression model for binary (2 classes) classification, we will repeat the same example for multiclass classification (3+ classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b59c0-28c9-4d2d-a8a5-ae0009f3ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f11be-ac10-4360-b017-a42fcf07398c",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset for multiclass logistic regression\n",
    "\n",
    "We will create a synthetic 3-class dataset for this complete implementation.\n",
    "\n",
    "To do this, manually create a synthetic dataset for logistic regression with bias and error term (to have *Theta_true* available) with a slightly different code template than the one you used in the last exercise.\n",
    "\n",
    "For the multiclass classification we will calculate Y in a different way: And it will have 2D (m x classes) dimensions, to represent all possible classes. We call this encoding of e.g. [0, 0, 1] for the 3/3 class \"one-hot encoding\":\n",
    "\n",
    "- For each example and class, calculate *Y* with the sigmoid with *Theta_true* and *X*.\n",
    "- Transform the values of *Y* to be `0` o `1` according to the max. value of the sigmoid of all the classes.\n",
    "- Finally, transform the value of the class to 1 with a maximum value of the sigmoid, and the values of the other classes to 0, with a final ndarray for each example.\n",
    "\n",
    "To introduce an error term, it runs through all *Y* values and changes the class of that example to a random class with a random error rate.\n",
    "\n",
    "*NOTE:* Investigate how a synthetic dataset for multiclass classification could be achieved using Scikit-learn methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a9e47-7d6e-42be-afff-022e87606153",
   "metadata": {},
   "source": [
    "### Implement the sigmoid activation function\n",
    "\n",
    "Copy your function from previous exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f07fc-a338-48f7-970d-c68f0fd86cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0394f2b-0bbe-411c-99b9-a6dbf31b266b",
   "metadata": {},
   "source": [
    "Create the synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3ee5d-0f0a-43d5-8055-94dbfa9fce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Manually generate a synthetic dataset with a bias term and an error term\n",
    "# Since we are going to train so many models, generate a \"small\" dataset in order to train them quickly\n",
    "# If you need to, you can make it even smaller, or if you want more accuracy and a more realistic challenge, make it bigger\n",
    "m = 1000\n",
    "n = 2\n",
    "classes = 3\n",
    "\n",
    "# Generate a 2D m x n array with random values between -1 and 1\n",
    "# Insert a bias term as a first column of 1s\n",
    "X = [...]\n",
    "\n",
    "# Generate a 2D theta array with (classes x n + 1) random values\n",
    "Theta_true = [...]\n",
    "\n",
    "# Y shall have 2D dimensions of (m x classes)\n",
    "# Calculate Y with the sigmoid and transform its values to 0 or 1 and then to one-hot encoding\n",
    "for i in range(m):\n",
    "    for c in range(classes):\n",
    "        sigmoid_example = sigmoid([...])\n",
    "        # Assign the only class corresponding to the example according to the max. value of the sigmoid\n",
    "        Y[...] = [...]\n",
    "\n",
    "# To introduce an error term, go through all the Y values and change\n",
    "# the class chosen from that example to another random class with a random % error\n",
    "# Note: make sure that the other random class representing the error is different from the original one\n",
    "error = 0.15\n",
    "\n",
    "for j in range(m):\n",
    "    # If a random number is less than or equal to the error\n",
    "    if [...]:\n",
    "        # Assign a randomly selected class\n",
    "        Y[...] = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta to be estimated:')\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d203bc-1c5c-4993-93fe-750697a7b9cd",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "As we did for linear regression, we will preprocess the data completely, following the usual 3 steps:\n",
    "\n",
    "- Randomly reorder the data..\n",
    "- Normalise the data..\n",
    "- Divide the dataset into training, validation, and test subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c92dd-161b-4e9e-8f07-fc508137220e",
   "metadata": {},
   "source": [
    "### Randomly reorder the dataset\n",
    "\n",
    "Reorder the data in the *X* and *Y* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fccf14-2ca5-41f7-afb0-ccd737d1ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the dataset\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Reorder X and Y:')\n",
    "# Use an initial random state of 42, in order to maintain reproducibility\n",
    "X, Y = [...]\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468764a7-e182-4026-aa89-4ae068b5c4ec",
   "metadata": {},
   "source": [
    "### Normalise the dataset\n",
    "\n",
    "Implement the normalisation function and normalize the dataset of *X* examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecec13-4a02-48ef-8907-59a8b8c0ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the dataset with the normalisation function\n",
    "\n",
    "# Copy the normalisation function you used in the linear regression exercise\n",
    "def normalize(x, mu, std):\n",
    "    pass\n",
    "\n",
    "# Find the mean and standard deviation of the features of X (columns), except the first column (bias)\n",
    "mu = [...]\n",
    "std = [...]\n",
    "\n",
    "print('X original:')\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('Mean and standard deviation of the features:')\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('Normalized X:')\n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std)    # Normalise only column 1 and the subsequent columns, not column 0\n",
    "print(X_norm)\n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657316f-9a8a-40cc-9551-fb6945acc02c",
   "metadata": {},
   "source": [
    "*Note*: If you had modified your normalize function to calculate and return the values of mu and std, you can modify this cell to include your custom code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abdd699-3d04-4018-b485-de77e3f7d277",
   "metadata": {},
   "source": [
    "### Divide the dataset into training, validation, and test subsets\n",
    "\n",
    "Divide the *X* and *Y* dataset into 3 subsets with the usual ratio, 60%/20%/20%.\n",
    "\n",
    "If your number of examples is much higher or lower, you can always modify this ratio to another ratio such as 50/25/25 or 80/10/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5743ce5-b995-4c15-8879-0f044a3e77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide the X and Y dataset into 3 subsets following the indicated ratios\n",
    "\n",
    "ratio = [60, 20, 20]\n",
    "print('Ratio:\\n', ratio, ratio[0] + ratio[1] + ratio[2])\n",
    "\n",
    "r = [0, 0]\n",
    "# Tip: the round() function and the x.shape attribute may be useful to you\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('Cutoff indices:\\n', r)\n",
    "\n",
    "# Tip: the np.array_split() function may be useful to you\n",
    "X_train, X_val, X_test = [...]\n",
    "Y_train, Y_val, Y_test = [...]\n",
    "\n",
    "print('Size of the subsets:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12561eea-36a8-4089-a48e-3a4581772b73",
   "metadata": {},
   "source": [
    "## Train an initial model for each class\n",
    "\n",
    "For multiclass classification, we must train a different model for each class. Therefore, if we have 3 classes we must train 3 different models.\n",
    "\n",
    "Each model will only consider the values of the target variable relative to its class in a binary way, classifying examples as either belonging to its class or not belonging.\n",
    "\n",
    "To do this, we will only provide you with the *Y* values for that class or column. E.g., for `Y = [[1, 0, 1], [0, 1, 0], [0, 0, 1]]`:\n",
    "- *Y* for model 1: `[1, 0, 0]`\n",
    "- *Y* for model 2: `[0, 1, 0]`\n",
    "- *Y* for model 3: `[0, 0, 1]`\n",
    "\n",
    "As we did in previous exercises, we will train initial models to check that our implementation is correct:\n",
    "- Train an initial model without regularisation for each class.\n",
    "- Plot the history of the cost function to check its evolution for each model.\n",
    "- If necessary, modify any hyperparameters, such as the training rate, and retrain the models. You will use these hyperparameters in the following steps.\n",
    "\n",
    "Copy the cells from previous exercises where you implemented the regularised cost function and gradient descent for logistic regression and the cell where you trained the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545782d-7cc8-467e-8adf-18941068fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cells with the cost and gradient descent functions for regularised classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650cd71-3b18-4490-802e-22a8dc645261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your models on the unregularised training subset\n",
    "\n",
    "# Create an initial theta with a given value, which may or may not be the same for all the models\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta initial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = 0.\n",
    "e = 1e-3\n",
    "iter_ = 1e3\n",
    "\n",
    "print('Hyperparameters used:')\n",
    "print('Alpha:', alpha, 'Max error:', e, 'Nº iter', iter_)\n",
    "\n",
    "# Initialise some variables to store the output of each model with the appropriate dimensions\n",
    "# Caution: the models may require a different number of iterations before they converge\n",
    "# Give j_train a size to store up to the max. number of iterations, even if not all the elements are filled in\n",
    "j_train_ini = [...]\n",
    "theta_train = [...]\n",
    "\n",
    "t = time.time()\n",
    "for c in [...]:    # Iterate over the nº of classes\n",
    "    print('\\nModel for class nº:', c)\n",
    "    \n",
    "    theta_train = [...]    # Deep copy of theta_ini to remain unchanged\n",
    "    \n",
    "    t_model = time.time()\n",
    "    j_train_ini[...], theta_train[...] = regularized_logistic_gradient_descent([...])\n",
    "    \n",
    "    print('Training time for model (s):', time.time() - t_model)\n",
    "    \n",
    "print('Total training time (s):', time.time() - t)\n",
    "\n",
    "print('\\nFinal cost of the model for each class:')\n",
    "print()\n",
    "\n",
    "print('\\nFinal theta of the model for each class:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976f61e-0957-4b58-942c-e48a6d65cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the evolution of the cost function vs. the number of iterations for each model\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "plt.title('Cost function for each class'')\n",
    "\n",
    "for c in range(classes):\n",
    "    plt.subplot(classes, 1, c + 1)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Class cost {}'.format(c))\n",
    "    plt.plot(j_train_ini[...])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee15b79-9f67-4c69-8aed-451f5f69551b",
   "metadata": {},
   "source": [
    "### Test the suitability of the models\n",
    "\n",
    "Check the accuracy of your models and modify the parameters to retrain them if necessary.\n",
    "\n",
    "Remember that if your dataset is \"too accurate\" you can go back to the original cell and enter a higher error term.\n",
    "\n",
    "Due to the complexity of multiclass classification, we will not ask you on this occasion to check whether the models may be suffering from deviation or overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd6a11-0929-48e5-8d35-240a019d0923",
   "metadata": {},
   "source": [
    "## Find the optimal *lambda* hyperparameter using validation\n",
    "\n",
    "As we have done in previous exercises, we will optimise our regularisation parameter by validation for each of the classes and models.\n",
    "\n",
    "Now, in order to find the optimal lambda, we will train a different model on the training subset for each *lambda* value to be considered and check its accuracy on the validation subset.\n",
    "\n",
    "Again, we will plot the error of each model vs. the *lambda* value used and implement a code that automatically chooses the most optimal model for each class.\n",
    "\n",
    "Remember to train all your models under equal conditions, with the same hyperparameters.\n",
    "\n",
    "Therefore, you must now modify the preceding cell’s code so that you do not train one model like before, but rather one model per class and for each of the *lambda* values to be considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8fab78-1942-43b8-aa3c-2f202bd5d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por cada valor de lambda diferente sobre X_train y evalúalo sobre X_val\n",
    "\n",
    "# Usa de nuevo un espacio logarítmico entre 10 y 10^3 de 5 elementos con valores que comiencen por un decimal no-cero 1 o 3\n",
    "# Al entrenar más modelos, podemos evaluar menos valores de lambda para reducir el tiempo de entrenamiento\n",
    "lambdas = [...]\n",
    "\n",
    "# Completa el código para entrenar un modelo diferente para cada clase y valor de lambda sobre X_train\n",
    "# Almacena sus thetas y costes finales\n",
    "# Posteriormente, evalúa sus costes totales en el subset de validación\n",
    "\n",
    "# Almacena dicha información en los siguientes arrays\n",
    "# Cuidado con sus dimensiones necesarias\n",
    "j_train = [...]\n",
    "j_val = [...]\n",
    "theta_val = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db658f-cf0b-4271-b93f-3c3835dce70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente el error final para cada valor de lambda con una gráfica por clase\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Completa con tu código\n",
    "for c in range(clases):\n",
    "    plt.subplot(clases, 1, c + 1)\n",
    "    \n",
    "    plt.title('Clase:', c)\n",
    "    plt.xlabel('Lambda')\n",
    "    plt.ylabel('Coste final')\n",
    "    plt.plot(j_train[...])\n",
    "    plt.plot(j_val[...])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d6b5b5-5288-472f-83f5-fe0b5191a358",
   "metadata": {},
   "source": [
    "### Escoger el mejor modelo para cada clase\n",
    "\n",
    "Copia el código de ejercicios anteriores y modifícalo para escoger el modelo con mayor precisión sobre el subset de validación para cada clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288db2e0-2697-4569-ab34-68baf0606efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escoge los modelos y valores de lambda óptimos, con el menor error sobre el subset de validación\n",
    "\n",
    "# Itera sobre todas las combinaciones de theta y lambda y escoge los modelos de menor coste en el subset de validación para cada clase\n",
    "\n",
    "j_final = [...]\n",
    "theta_final = [...]\n",
    "lambda_final = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71f987-c768-4668-928b-d83783d238a7",
   "metadata": {},
   "source": [
    "## Evaluar los modelos sobre el subset de test\n",
    "\n",
    "Finalmente, vamos a evaluar el modelo de cada clase sobre un subset de datos que no hemos usado para entrenarlos ni para escoger ningún hiper-parámetro.\n",
    "\n",
    "Para ello, vamos a calcular el coste o error total sobre el subset de test y comprobar gráficamente los residuos sobre el mismo.\n",
    "\n",
    "Recuerda usar sólo las columnas de la *Y* que \"vería\" cada modelo, puesto que clasifica los ejemplos en función de si pertenecen a su clase o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a255d-2ab8-4f10-8a5e-ff2928ee9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula el error de los modelos sobre el subset de test usando la función de coste\n",
    "# Utiliza la theta y lambda del modelo específico de la clase correspondiente a dicho ejemplo\n",
    "\n",
    "j_test = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c9941-fe87-4f09-9ecd-4a3536f48cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula las predicciones de los modelos sobre el subset de test, calcula los residuos y represéntalos\n",
    "\n",
    "# Recuerda usar la función sigmoide para transformar las predicciones y escoger la clase según el valor máx. del sigmoide\n",
    "Y_test_pred = [...]\n",
    "\n",
    "residuos = [...]\n",
    "\n",
    "plt.figure(4)\n",
    "\n",
    "# Completa con tu código\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095fd04-6f43-4a4b-9c63-9af9d9455114",
   "metadata": {},
   "source": [
    "## Realizar predicciones sobre nuevos ejemplos\n",
    "\n",
    "Con nuestros modelos ya entrenados, optimizados y evaluados, lo único que nos queda es utilizarlos realizando predicciones con nuevos ejemplos.\n",
    "\n",
    "Para ello, vamos a:\n",
    "- Generar un nuevo ejemplo, siguiendo el mismo patrón que el dataset original.\n",
    "- Normalizar sus características antes de poder realizar predicciones sobre ellos.\n",
    "- Generar una predicción para dicho nuevo ejemplo para cada una de las clases, para cada uno de los 3 modelos.\n",
    "- Escoger la clase final como la clase con mayor valor de *Y* tras el sigmoide, aunque varios modelos predijeran `Y >= 0.0; Y = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e3488-0e99-4479-9807-fd584a025eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un nuevo ejemplo siguiendo el patrón original, con término de bias\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "# Para comparar, antes de normalizar los datos, usa la Theta_verd para ver cuál sería la clase real asociada\n",
    "Y_verd = [...]\n",
    "\n",
    "# Normaliza sus características (excepto el término de bias) con las medias y desviaciones típicas del subset de entrenamiento\n",
    "X_pred = [...]\n",
    "\n",
    "# Genera una predicción para dicho ejemplo para cada modelo usando el sigmoide\n",
    "Y_pred = [...]\n",
    "\n",
    "# Escoge la clase final como la de mayor valor tras el sigmoide y transfórmala a un vector one-hot encoding de 0 y 1\n",
    "Y_pred = [...]\n",
    "\n",
    "# Compara la clase real asociada a dicho nuevo ejemplo y la clase predicha\n",
    "print('Clase real del nuevo ejemplo y clase predicha:')\n",
    "print(Y_verd)\n",
    "print(Y_pred)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
