{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d9c6ef-79eb-4635-9509-2936e0720f69",
   "metadata": {},
   "source": [
    "# Logistic Regression: Multiclass classification\n",
    "M2U5 - Exercise 6\n",
    "\n",
    "## What are we going to do?\n",
    "- We will create a synthetic dataset for multiclass logistic regression\n",
    "- We will preprocess the data\n",
    "- We will train the model on the training subset and check its suitability\n",
    "- We will find the optimal lambda regularization parameter using CV\n",
    "- We will make predictions about new examples\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "Having implemented the full training of a regularised logistic regression model for binary (2 classes) classification, we will repeat the same example for multiclass classification (3+ classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b59c0-28c9-4d2d-a8a5-ae0009f3ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f11be-ac10-4360-b017-a42fcf07398c",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset for multiclass logistic regression\n",
    "\n",
    "We will create a synthetic 3-class dataset for this complete implementation.\n",
    "\n",
    "To do this, manually create a synthetic dataset for logistic regression with bias and error term (to have *Theta_true* available) with a slightly different code template than the one you used in the last exercise.\n",
    "\n",
    "For the multiclass classification we will calculate Y in a different way: And it will have 2D (m x classes) dimensions, to represent all possible classes. We call this encoding of e.g. [0, 0, 1] for the 3/3 class \"one-hot encoding\":\n",
    "\n",
    "- For each example and class, calculate *Y* with the sigmoid with *Theta_true* and *X*.\n",
    "- Transform the values of *Y* to be `0` o `1` according to the max. value of the sigmoid of all the classes.\n",
    "- Finally, transform the value of the class to 1 with a maximum value of the sigmoid, and the values of the other classes to 0, with a final ndarray for each example.\n",
    "\n",
    "To introduce an error term, it runs through all *Y* values and changes the class of that example to a random class with a random error rate.\n",
    "\n",
    "*NOTE:* Investigate how a synthetic dataset for multiclass classification could be achieved using Scikit-learn methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a9e47-7d6e-42be-afff-022e87606153",
   "metadata": {},
   "source": [
    "### Implement the sigmoid activation function\n",
    "\n",
    "Copy your function from previous exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f07fc-a338-48f7-970d-c68f0fd86cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0394f2b-0bbe-411c-99b9-a6dbf31b266b",
   "metadata": {},
   "source": [
    "Create the synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3ee5d-0f0a-43d5-8055-94dbfa9fce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Manually generate a synthetic dataset with a bias term and an error term\n",
    "# Since we are going to train so many models, generate a \"small\" dataset in order to train them quickly\n",
    "# If you need to, you can make it even smaller, or if you want more accuracy and a more realistic challenge, make it bigger\n",
    "m = 1000\n",
    "n = 2\n",
    "classes = 3\n",
    "\n",
    "# Generate a 2D m x n array with random values between -1 and 1\n",
    "# Insert a bias term as a first column of 1s\n",
    "X = [...]\n",
    "\n",
    "# Generate a 2D theta array with (classes x n + 1) random values\n",
    "Theta_true = [...]\n",
    "\n",
    "# Y shall have 2D dimensions of (m x classes)\n",
    "# Calculate Y with the sigmoid and transform its values to 0 or 1 and then to one-hot encoding\n",
    "for i in range(m):\n",
    "    for c in range(classes):\n",
    "        sigmoid_example = sigmoid([...])\n",
    "        # Assign the only class corresponding to the example according to the max. value of the sigmoid\n",
    "        Y[...] = [...]\n",
    "\n",
    "# To introduce an error term, go through all the Y values and change\n",
    "# the class chosen from that example to another random class with a random % error\n",
    "# Note: make sure that the other random class representing the error is different from the original one\n",
    "error = 0.15\n",
    "\n",
    "for j in range(m):\n",
    "    # If a random number is less than or equal to the error\n",
    "    if [...]:\n",
    "        # Assign a randomly selected class\n",
    "        Y[...] = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta to be estimated:')\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d203bc-1c5c-4993-93fe-750697a7b9cd",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "As we did for linear regression, we will preprocess the data completely, following the usual 3 steps:\n",
    "\n",
    "- Randomly reorder the data..\n",
    "- Normalise the data..\n",
    "- Divide the dataset into training, validation, and test subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c92dd-161b-4e9e-8f07-fc508137220e",
   "metadata": {},
   "source": [
    "### Randomly reorder the dataset\n",
    "\n",
    "Reorder the data in the *X* and *Y* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fccf14-2ca5-41f7-afb0-ccd737d1ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the dataset\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Reorder X and Y:')\n",
    "# Use an initial random state of 42, in order to maintain reproducibility\n",
    "X, Y = [...]\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468764a7-e182-4026-aa89-4ae068b5c4ec",
   "metadata": {},
   "source": [
    "### Normalise the dataset\n",
    "\n",
    "Implement the normalisation function and normalize the dataset of *X* examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecec13-4a02-48ef-8907-59a8b8c0ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the dataset with the normalisation function\n",
    "\n",
    "# Copy the normalisation function you used in the linear regression exercise\n",
    "def normalize(x, mu, std):\n",
    "    pass\n",
    "\n",
    "# Find the mean and standard deviation of the features of X (columns), except the first column (bias)\n",
    "mu = [...]\n",
    "std = [...]\n",
    "\n",
    "print('X original:')\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('Mean and standard deviation of the features:')\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('Normalized X:')\n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std)    # Normalise only column 1 and the subsequent columns, not column 0\n",
    "print(X_norm)\n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657316f-9a8a-40cc-9551-fb6945acc02c",
   "metadata": {},
   "source": [
    "*Note*: If you had modified your normalize function to calculate and return the values of mu and std, you can modify this cell to include your custom code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abdd699-3d04-4018-b485-de77e3f7d277",
   "metadata": {},
   "source": [
    "### Divide the dataset into training, validation, and test subsets\n",
    "\n",
    "Divide the *X* and *Y* dataset into 3 subsets with the usual ratio, 60%/20%/20%.\n",
    "\n",
    "If your number of examples is much higher or lower, you can always modify this ratio to another ratio such as 50/25/25 or 80/10/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5743ce5-b995-4c15-8879-0f044a3e77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide the X and Y dataset into 3 subsets following the indicated ratios\n",
    "\n",
    "ratio = [60, 20, 20]\n",
    "print('Ratio:\\n', ratio, ratio[0] + ratio[1] + ratio[2])\n",
    "\n",
    "r = [0, 0]\n",
    "# Tip: the round() function and the x.shape attribute may be useful to you\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('Cutoff indices:\\n', r)\n",
    "\n",
    "# Tip: the np.array_split() function may be useful to you\n",
    "X_train, X_val, X_test = [...]\n",
    "Y_train, Y_val, Y_test = [...]\n",
    "\n",
    "print('Size of the subsets:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12561eea-36a8-4089-a48e-3a4581772b73",
   "metadata": {},
   "source": [
    "## Train an initial model for each class\n",
    "\n",
    "For multiclass classification, we must train a different model for each class. Therefore, if we have 3 classes we must train 3 different models.\n",
    "\n",
    "Each model will only consider the values of the target variable relative to its class in a binary way, classifying examples as either belonging to its class or not belonging.\n",
    "\n",
    "To do this, we will only provide you with the *Y* values for that class or column. E.g., for `Y = [[1, 0, 1], [0, 1, 0], [0, 0, 1]]`:\n",
    "- *Y* for model 1: `[1, 0, 0]`\n",
    "- *Y* for model 2: `[0, 1, 0]`\n",
    "- *Y* for model 3: `[0, 0, 1]`\n",
    "\n",
    "As we did in previous exercises, we will train initial models to check that our implementation is correct:\n",
    "- Train an initial model without regularisation for each class.\n",
    "- Plot the history of the cost function to check its evolution for each model.\n",
    "- If necessary, modify any hyperparameters, such as the training rate, and retrain the models. You will use these hyperparameters in the following steps.\n",
    "\n",
    "Copy the cells from previous exercises where you implemented the regularised cost function and gradient descent for logistic regression and the cell where you trained the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545782d-7cc8-467e-8adf-18941068fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cells with the cost and gradient descent functions for regularised classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650cd71-3b18-4490-802e-22a8dc645261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your models on the unregularised training subset\n",
    "\n",
    "# Create an initial theta with a given value, which may or may not be the same for all the models\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta initial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = 0.\n",
    "e = 1e-3\n",
    "iter_ = 1e3\n",
    "\n",
    "print('Hyperparameters used:')\n",
    "print('Alpha:', alpha, 'Max error:', e, 'Nº iter', iter_)\n",
    "\n",
    "# Initialise some variables to store the output of each model with the appropriate dimensions\n",
    "# Caution: the models may require a different number of iterations before they converge\n",
    "# Give j_train a size to store up to the max. number of iterations, even if not all the elements are filled in\n",
    "j_train_ini = [...]\n",
    "theta_train = [...]\n",
    "\n",
    "t = time.time()\n",
    "for c in [...]:    # Iterate over the nº of classes\n",
    "    print('\\nModel for class nº:', c)\n",
    "    \n",
    "    theta_train = [...]    # Deep copy of theta_ini to remain unchanged\n",
    "    \n",
    "    t_model = time.time()\n",
    "    j_train_ini[...], theta_train[...] = regularized_logistic_gradient_descent([...])\n",
    "    \n",
    "    print('Training time for model (s):', time.time() - t_model)\n",
    "    \n",
    "print('Total training time (s):', time.time() - t)\n",
    "\n",
    "print('\\nFinal cost of the model for each class:')\n",
    "print()\n",
    "\n",
    "print('\\nFinal theta of the model for each class:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976f61e-0957-4b58-942c-e48a6d65cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the evolution of the cost function vs. the number of iterations for each model\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "plt.title('Cost function for each class'')\n",
    "\n",
    "for c in range(classes):\n",
    "    plt.subplot(classes, 1, c + 1)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Class cost {}'.format(c))\n",
    "    plt.plot(j_train_ini[...])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee15b79-9f67-4c69-8aed-451f5f69551b",
   "metadata": {},
   "source": [
    "### Test the suitability of the models\n",
    "\n",
    "Check the accuracy of your models and modify the parameters to retrain them if necessary.\n",
    "\n",
    "Remember that if your dataset is \"too accurate\" you can go back to the original cell and enter a higher error term.\n",
    "\n",
    "Due to the complexity of multiclass classification, we will not ask you on this occasion to check whether the models may be suffering from deviation or overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd6a11-0929-48e5-8d35-240a019d0923",
   "metadata": {},
   "source": [
    "## Find the optimal *lambda* hyperparameter using validation\n",
    "\n",
    "As we have done in previous exercises, we will optimise our regularisation parameter by validation for each of the classes and models.\n",
    "\n",
    "Now, in order to find the optimal lambda, we will train a different model on the training subset for each *lambda* value to be considered and check its accuracy on the validation subset.\n",
    "\n",
    "Again, we will plot the error of each model vs. the *lambda* value used and implement a code that automatically chooses the most optimal model for each class.\n",
    "\n",
    "Remember to train all your models under equal conditions, with the same hyperparameters.\n",
    "\n",
    "Therefore, you must now modify the preceding cell’s code so that you do not train one model like before, but rather one model per class and for each of the *lambda* values to be considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8fab78-1942-43b8-aa3c-2f202bd5d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a model on X_train for each different lambda value and evaluate it on X_val\n",
    "\n",
    "# Use a logarithmic space between 10 and 10^3 with 5 elements with non-zero decimal values starting with a 1 or a 3\n",
    "# By training more models, we can evaluate fewer lambda values to reduce training time\n",
    "lambdas = [...]\n",
    "\n",
    "# Complete the code to train a different model for each class and value of lambda on X_train\n",
    "# Store your thetas and final costs\n",
    "# Afterwards, evaluate its total cost on the validation subset\n",
    "\n",
    "# Store this information in the following arrays\n",
    "# Careful with its essential dimensions\n",
    "j_train = [...]\n",
    "j_val = [...]\n",
    "theta_val = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db658f-cf0b-4271-b93f-3c3835dce70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the final error for each lambda value with one plot per class\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Fill in your code\n",
    "for c in range(classes):\n",
    "    plt.subplot(classes, 1, c + 1)\n",
    "    \n",
    "    plt.title('Class:', c)\n",
    "    plt.xlabel('Lambda')\n",
    "    plt.ylabel('Final cost')\n",
    "    plt.plot(j_train[...])\n",
    "    plt.plot(j_val[...])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d6b5b5-5288-472f-83f5-fe0b5191a358",
   "metadata": {},
   "source": [
    "### Choosing the best model for each class\n",
    "\n",
    "Copy the code from previous exercises and modify it to choose the most accurate model on the validation subset for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288db2e0-2697-4569-ab34-68baf0606efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose the optimal models and lambda values, with the lowest error on the validation subset\n",
    "\n",
    "# Iterate over all the combinations of theta and lambda and choose the lowest cost models on the validation subset for each class\n",
    "\n",
    "j_final = [...]\n",
    "theta_final = [...]\n",
    "lambda_final = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71f987-c768-4668-928b-d83783d238a7",
   "metadata": {},
   "source": [
    "## Evaluate the models on the test subset.\n",
    "\n",
    "Finally, we will evaluate the model of each class on a subset of data that we have not used for training nor for choosing any hyperparameters.\n",
    "\n",
    "Therefore, we will calculate the total cost or error on the test subset and graphically check the residuals of the model on it.\n",
    "\n",
    "Remember to use only the *Y* columns that each model would \"see\", as it classifies examples according to whether they belong to its class or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a255d-2ab8-4f10-8a5e-ff2928ee9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the error of the models on the test subset using the cost function\n",
    "# Use the theta and lambda of the specific model of the class corresponding to that example\n",
    "\n",
    "j_test = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c9941-fe87-4f09-9ecd-4a3536f48cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the predictions of the models on the test subset, calculate the residuals and plot them\n",
    "\n",
    "# Remember to use the sigmoid function to transform the predictions and choose the class according to the maximum value of the sigmoid\n",
    "Y_test_pred = [...]\n",
    "\n",
    "residuals = [...]\n",
    "\n",
    "plt.figure(4)\n",
    "\n",
    "# Fill in your code\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095fd04-6f43-4a4b-9c63-9af9d9455114",
   "metadata": {},
   "source": [
    "## Make predictions about new future examples\n",
    "\n",
    "With our model trained, optimised, and evaluated, all that remains is to put it to work by making predictions with new examples.\n",
    "\n",
    "To do this, we will:\n",
    "- Generate a new example, which follows the same pattern as the original dataset.\n",
    "- Normalise its features before making predictions about them.\n",
    "- Generate a prediction for this new example for each of the classes, for each of the 3 models.\n",
    "- Choose the class with the highest Y value after the sigmoid as the final class, even though several models predicted `Y >= 0.0; Y = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e3488-0e99-4479-9807-fd584a025eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a new example following the original pattern, with a bias term\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "# For comparison, before normalising the data, use Theta_true to see what the actual associated class would be\n",
    "Y_true = [...]\n",
    "\n",
    "# Normalise its features (except for the bias term) with the means and standard deviations of the training subset\n",
    "X_pred = [...]\n",
    "\n",
    "# Generate a prediction for this new example for each model using the sigmoid\n",
    "Y_pred = [...]\n",
    "\n",
    "# Choose the highest value after the sigmoid as the final class and transform it to a one-hot encoding vector of 0 and 1\n",
    "Y_pred = [...]\n",
    "\n",
    "# Compare the actual class associated with this new example and the predicted class\n",
    "print('Actual class of the new example and predicted class:')\n",
    "print(Y_true)\n",
    "print(Y_pred)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
