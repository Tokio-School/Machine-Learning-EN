{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4551f26-d023-4bd9-8ec1-9a43d2fa387e",
   "metadata": {},
   "source": [
    "# Decision trees vs. Linear regression\n",
    "M2U5 - Exercise 2\n",
    "\n",
    "## What are we going to do?\n",
    "- We will compare the accuracy and behaviour of decision trees versus traditional linear regression\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "It is sometimes felt that regression trees may not be as accurate and fall into more overfitting when compared to traditional linear regression, especially when there are a high number of features.\n",
    "\n",
    "In this exercise, we will follow the usual steps to train 2 linear regression models: a decision tree, and a Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d17daa-c83e-412e-a54f-239be941af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import all the necessary modules into this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1167a2c5-b0c7-4c20-b132-b4384b4858e2",
   "metadata": {},
   "source": [
    "## Generate a synthetic dataset\n",
    "\n",
    "Generate a synthetic dataset with a fairly large error term and few features, manually or with Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb071ed8-13f5-4d76-b0f2-718502b9647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a synthetic dataset with few features and a significant error term\n",
    "# Do not add a bias term to X\n",
    "\n",
    "m = 1000\n",
    "n = 9\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_true = [...]\n",
    "\n",
    "error = 0.3\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta and its dimensions to be estimated:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86c2fb-f9a1-44d6-bad3-fd6f7493e044",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "- Randomly reorder the data.\n",
    "- Normalise the data.\n",
    "- Divide the dataset into training and test subsets.\n",
    "\n",
    "*Note*: We will use K-fold again for the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d42e9-6c01-4c53-b149-cdf8c8053d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the data, normalise the examples, and divide them into training and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e140a60-9a78-46b9-b0dc-76ab0aa1ba44",
   "metadata": {},
   "source": [
    "## Optimise the models using cross-validation\n",
    "\n",
    "- Train a model for each regularisation value or max. depth to be considered.\n",
    "- Train and evaluate them on a K-fold training subset division.\n",
    "- Choose the optimal model and its regularisation.\n",
    "\n",
    "Consider similar parameters to those from previous exercises:\n",
    "- Maximum depth in the range `[1, 8]`\n",
    "- L2 *alpha* regularisation parameters in the logarithmic range `[0, 0.1]: 0.1, 0.01, 0.001, 0.0001`, etc.\n",
    "\n",
    "You can copy the cells from previous exercises and modify them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43793063-c934-4c8c-a205-26470fb7edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a different model on a different K-fold for the regression tree and the Lasso\n",
    "\n",
    "# Iterate over the necessary K-fold splits, train your models and evaluate them on the CV subset\n",
    "mejor_tree = [...]\n",
    "mejor_lasso = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701fdad6-3161-4927-bb93-17d573673c1a",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test subset\n",
    "\n",
    "Finally, we are going to evaluate the best decision tree and Lasso model on the test subset.\n",
    "\n",
    "To do this, calculate their MSE, RMSE, and R^2 score metrics and plot the model predictions and residuals vs. the test subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c65c62-3d3a-4eab-ac26-b45d2968567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model with MSE, RMSE, and R^2 on the test subset for the best tree and Lasso\n",
    "\n",
    "y_train_test = [...]\n",
    "\n",
    "mse = [...]\n",
    "rmse = [...]\n",
    "r2_score = [...]\n",
    "print('mean square error: {%.2f}'.format(mse))\n",
    "print('Root mean square error: {%.2f}'.format(rmse))\n",
    "print('Coefficient of determination: {%.2f}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12355c1-9685-4b4d-af04-a755d191ccea",
   "metadata": {},
   "source": [
    "Finally, check their possible deviation or overfitting and final accuracy by plotting the residuals of both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dfd58-fe9a-4a1d-a98d-423d05e789e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the residuals of both models as line graphs with different colours, vs. the index of the examples (m)\n",
    "\n",
    "residuals_tree = [...]\n",
    "residuals_lasso = [...]\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "plt.title([...])\n",
    "plt.xlabel([...])\n",
    "plt.ylabel([...])\n",
    "\n",
    "plt.plot([...])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63dc89-5b2d-45cf-9d6b-2cb31afb98e6",
   "metadata": {},
   "source": [
    "*Are there significant differences between the two models? What happens if we vary the error or the number of features in the original dataset, how do both types of models respond?*\n",
    "\n",
    "In the case of the regression tree, we may not have made the fairest comparison, since there are still other hyperparameters that we can modify: [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "\n",
    "## *Bonus*: Optimisation of all the decision tree hyperparameters\n",
    "\n",
    "*Do you have the courage to use [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) not only to optimise max_depth, but for all the hyperparameters of the regression tree*\n",
    "\n",
    "There is an example on the GridSearchCV documentation page you can refer to."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
