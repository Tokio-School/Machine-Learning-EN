{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf574b3-aadd-43f6-aa67-ab82651f0145",
   "metadata": {},
   "source": [
    "# Logistic Regression: Regularisation\n",
    "M2U5 - Exercise 5\n",
    "\n",
    "## What are we going to do?\n",
    "- We will implement the regularised cost and gradient descent functions\n",
    "- We will check the training by plotting the evolution of the cost function\n",
    "- We will find the optimal *lambda* regularisation parameter using validation\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "Once the unregularised cost function and gradient descent are implemented, we will regularise them and train a full logistic regression model, checking it by validation and evaluating it on a test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371fe604-a17c-4c68-9415-c516cfd380e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f7c59-0a6a-440c-80ac-5d172adaba24",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset for logistic regression\n",
    "\n",
    "We will create a synthetic dataset with only 2 classes (0 and 1) to test this implementation of a fully trained binary classification model, step by step.\n",
    "\n",
    "To do this, manually create a synthetic dataset for logistic regression with bias and error term (to have *Theta_true* available) with the code you used in the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40f576-6ce6-493a-81d7-6e7134f200bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Manually generate a synthetic dataset with a bias term and an error term\n",
    "m = 100\n",
    "n = 1\n",
    "\n",
    "# Generate a 2D m x n array with random values between -1 and 1\n",
    "# Insert a bias term as a first column of 1s\n",
    "X = [...]\n",
    "\n",
    "# Generate a theta array with n + 1 random values between [0, 1)\n",
    "Theta_true = [...]\n",
    "\n",
    "# Calculate Y as a function of X and *Theta_true*\n",
    "# Transform Y to values of 1 and 0 (float) when Y â‰¥ 0.0\n",
    "# Using a probability as the error term, iterate over Y and change the assigned class to its opposite, 1 to 0, and 0 to 1\n",
    "error = 0.15\n",
    "\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta and its dimensions to be estimated:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2aed3-82ba-4547-a2ae-50dcf3d4cea9",
   "metadata": {},
   "source": [
    "## Implement the sigmoid activation function\n",
    "\n",
    "Copy your cell with the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a747e-c79e-4a24-b8e6-a8a8573e21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac98be5-1acb-4510-8c66-e58424203d63",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "As we did for linear regression, we will preprocess the data completely, following the usual 3 steps:\n",
    "\n",
    "- Randomly reorder the data.\n",
    "- Normalise the data.\n",
    "- Divide the dataset into training, validation, and test subsets.\n",
    "\n",
    "You can do this manually or with Scikit-learn's auxiliary functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1f14c-40a6-4c34-a455-25c3ff250484",
   "metadata": {},
   "source": [
    "### Randomly rearrange the dataset\n",
    "\n",
    "Reorder the data in the *X* and *Y* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf1e1e-6330-4e27-8e5a-be7beba21b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the dataset\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Reorder X and Y:')\n",
    "# Use an initial random state of 42, in order to maintain reproducibility\n",
    "X, Y = [...]\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943d24b-c888-4412-bab5-8033e478e681",
   "metadata": {},
   "source": [
    "### Normalise the dataset\n",
    "\n",
    "Implement the normalisation function and normalise the dataset of *X* examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e6a10-0c2b-4a41-9fc9-39a88b922ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the dataset with a normalisation function\n",
    "\n",
    "# Copy the normalisation function you used in the linear regression exercise\n",
    "def normalize(x, mu, std):\n",
    "    pass\n",
    "\n",
    "# Find the mean and standard deviation of the features of X (columns), except the first column (bias)\n",
    "mu = [...]\n",
    "std = [...]\n",
    "\n",
    "print('Original X:')\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('Mean and standard deviation of the features:')\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('Normalized X:')\n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std)    # Normalize only column 1 and the subsequent columns, not column 0\n",
    "print(X_norm)\n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b79868-e71c-4193-80b6-3132daac1823",
   "metadata": {},
   "source": [
    "*Note*: If you had modified your normalize function to calculate and return the values of mu and std, you can modify this cell to include your custom code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a1f3f-6347-4296-88c4-20f786029c9a",
   "metadata": {},
   "source": [
    "### Divide the dataset into training, validation, and test subsets\n",
    "\n",
    "Divide the *X* and *Y* dataset into 3 subsets with the usual ratio, 60%/20%/20%.\n",
    "\n",
    "If your number of examples is much higher or lower, you can always modify this ratio to another ratio such as 50/25/25 or 80/10/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e00607-f834-464e-99af-6b9e0146cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide the X and Y dataset into the 3 subsets following the indicated ratios\n",
    "\n",
    "ratio = [60, 20, 20]\n",
    "print('Ratio:\\n', ratio, ratio[0] + ratio[1] + ratio[2])\n",
    "\n",
    "r = [0, 0]\n",
    "# Tip: the round() function and the x.shape attribute may be useful to you\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('Cutoff indices:\\n', r)\n",
    "\n",
    "# Tip: the np.array_split() function may be useful to you\n",
    "X_train, X_val, X_test = [...]\n",
    "Y_train, Y_val, Y_test = [...]\n",
    "\n",
    "print('Size of the subsets:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85ef3b-dd5c-4c1e-a4df-d5937e31f4a8",
   "metadata": {},
   "source": [
    "## Implement the sigmoid activation function\n",
    "\n",
    "Copy your cell with the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0c3a9-9f0e-4832-b795-4a3e349faafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b0553-219a-4f0d-89f0-11f4ea4e6c25",
   "metadata": {},
   "source": [
    "## Implement the regularised cost function\n",
    "\n",
    "We are going to implement the regularised cost function. This function will be similar to the one we implemented for linear regression in a previous exercise.\n",
    "\n",
    "Regularised cost function:\n",
    "\n",
    "$$ Y = h_\\Theta(x) = g(X \\times \\Theta^T) $$\n",
    "$$ J(\\Theta) = - [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (y^i log(h_\\theta(x^i)) + (1 - y^i) log(1 - h_\\theta(x^i))] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\Theta_j^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dd0bc-2987-4319-886c-01ce50e7133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the regularised cost function for logistic regression\n",
    "\n",
    "def regularized_logistic_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computes the cost function for the considered dataset and coefficients\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- ndarray 2D with the values of the independent variables from the examples, of size m x n\n",
    "    y -- ndarray 1D with the dependent/target variable, of size m x 1 and values of 0 or 1\n",
    "    theta -- ndarray 1D with the weights of the model coefficients, of size 1 x n (row vector)\n",
    "    lambda_ -- regularisation factor, by default 0.\n",
    "    \n",
    "    Return:\n",
    "    j -- float with the cost for this theta array\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Remember to check the dimensions of the matrix multiplication to perform it correctly\n",
    "    j = [...]\n",
    "    \n",
    "    # Regularise for all Theta except the bias term (the first value)\n",
    "    j += [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce13caa-cd63-4754-9bfa-692427d9581e",
   "metadata": {},
   "source": [
    "Now let's check your implementation in the following scenarios:\n",
    "1. For *lambda* = 0:\n",
    "    1. Using *Theta_true*, the cost should be 0.\n",
    "    1. As the value of *theta* moves away from *Theta_true*, the cost should increase.\n",
    "1. For *lambda* != 0:\n",
    "    1. Using *Theta_true*, the cost should be greater than 0.\n",
    "    1. The higher the *lambda*, the higher the cost.\n",
    "    1. The increase in cost as a function of lambda is exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7caff62-b979-43c3-acb7-ab1536115902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test your implementation on the dataset\n",
    "\n",
    "theta = Theta_true    # Modify and test several values of theta\n",
    "\n",
    "j = logistic_cost_function(X, Y, theta)\n",
    "\n",
    "print('Cost of the model:')\n",
    "print(j)\n",
    "print('Checked theta and Actual theta:')\n",
    "print(theta)\n",
    "print(Theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab374ab0-348f-47b3-b19f-92b690610d49",
   "metadata": {},
   "source": [
    "Record your experiments and results in this cell (in Markdown or code):\n",
    "\n",
    "1. Experiment 1\n",
    "1. Experiment 2\n",
    "1. Experiment 3\n",
    "1. Experiment 4\n",
    "1. Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17253ad8-9665-4190-8fb5-da043f92900f",
   "metadata": {},
   "source": [
    "## Train an initial model on the training subset\n",
    "\n",
    "As we did in previous exercises, we will train an initial model to check that our implementation and the dataset work correctly, and then we will be able to train a model with validation without any problem.\n",
    "\n",
    "To do this, follow the same steps as you did for linear regression:\n",
    "- Train an initial model without regularisation.\n",
    "- Plot the history of the cost function to check its evolution.\n",
    "- If necessary, modify any of the parameters and retrain the model. You will use these parameters in the following steps.\n",
    "\n",
    "Copy the cells from previous exercises where you implemented the cost function in unregularised logistic regression and the cell where you trained the model, and modify them for regularised logistic regression.\n",
    "\n",
    "Recall the gradient descent functions for regularised logistic regression:\n",
    "\n",
    "$$ Y = h_\\Theta(x) = g(X \\times \\Theta^T) $$\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i + \\frac{\\lambda}{m} \\theta_j]; \\space j \\in [1, n] $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e25d51-0191-47ec-9e35-0fe2df104a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cell with the gradient descent for unregularised logistic regression and modify it to implement the regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7e8f5-e69c-44c1-b918-e875509a1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cell where we trained the model\n",
    "# Train your model on the unregularised training subset and check that it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af06ed-1362-42df-b9ab-eda151c3bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the evolution of the cost function vs. the number of iterations\n",
    "\n",
    "plt.figure(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e710b7-7e10-48b4-b110-ce1f880af6f4",
   "metadata": {},
   "source": [
    "### Comprobar la implementaciÃ³n\n",
    "\n",
    "Comprueba de nuevo tu implementaciÃ³n, al igual que hiciste en el ejercicio anterior.\n",
    "\n",
    "En esta ocasiÃ³n, ademÃ¡s, comprueba cÃ³mo con una *lambda* distinta a 0 la penalizaciÃ³n hace que el coste sea mayor cuanto mayor sea esta *lambda*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29135cf0-3451-48a0-9d84-d5ce27c46105",
   "metadata": {},
   "source": [
    "### Comprobar si existe desviaciÃ³n o sobreajuste\n",
    "\n",
    "Al igual que hacÃ­amos en la regresiÃ³n lineal, vamos a comprobar si existe sobreajuste comparando el coste del modelo en el dataset de entrenamiento y de validaciÃ³n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26ac15-42a2-4f3d-bbb5-50128460f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba el coste del modelo sobre el dataset de entrenamiento y validaciÃ³n\n",
    "# Utiliza la Theta_final del modelo entrenado en ambos casos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294ecc3-1ab7-4c93-9f80-de6990e1ffb3",
   "metadata": {},
   "source": [
    "Recuerda que con un dataset sintÃ©tico aleatorio es difÃ­cil que se diera un caso u otro, pero de esta forma podrÃ­amos apreciar dichos problemas de la siguiente forma:\n",
    "\n",
    "- Si el coste final en ambos subsets es alto, puede haber un problema de desviaciÃ³n o *bias*.\n",
    "- Si el coste final en ambos subsets es muy diferente entre sÃ­, puede haber un problema de sobreajuste o *varianza*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6381277-2d83-48e8-ad23-f09a131c1b7f",
   "metadata": {},
   "source": [
    "## Hallar el hiper-parÃ¡metro *lambda* Ã³ptimo por validaciÃ³n\n",
    "\n",
    "Del mismo modo que hemos hecho en ejercicios anteriores, vamos a optimizar nuestro parÃ¡metro de regularizaciÃ³n por validaciÃ³n.\n",
    "\n",
    "Para ello vamos a entrenar un modelo diferente por cada valor de *lambda* a considerar sobre el subset de entrenamiento, y evaluar su error o coste final sobre el subset de validaciÃ³n.\n",
    "\n",
    "Vamos a representar grÃ¡ficamente el error de cada modelo vs el valor de *lambda* usado e implementar un cÃ³digo que elegirÃ¡ automÃ¡ticamente el modelo mÃ¡s Ã³ptimo de entre todos.\n",
    "\n",
    "Recuerda entrenar todos tus modelos en igualdad de condiciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20de71-246c-49e0-907d-2290019a6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por cada valor de lambda diferente sobre X_train y evalÃºalo sobre X_val\n",
    "\n",
    "# Usa de nuevo un espacio logarÃ­tmico entre 10 y 10^3 de 10 elementos con valores que comiencen por un decimal no-cero 1 o 3\n",
    "lambdas = [...]\n",
    "\n",
    "# Completa el cÃ³digo para entrenar un modelo diferente para cada valor de lambda sobre X_train\n",
    "# Almacena su theta y error/coste final\n",
    "# Posteriormente, evalÃºa su coste total en el subset de validaciÃ³n\n",
    "\n",
    "# Almacena dicha informaciÃ³n en los siguientes ndarrays, del mismo tamaÃ±o que lambdas\n",
    "j_train = [...]\n",
    "j_val = [...]\n",
    "theta_val = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53229f4a-5ce9-4b1d-9613-179a2aaa6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa grÃ¡ficamente el error final para cada valor de lambda\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "# Completa con tu cÃ³digo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50326c71-7a1c-4931-a057-edbf9e129194",
   "metadata": {},
   "source": [
    "### Escoger el mejor modelo\n",
    "\n",
    "Copia el cÃ³digo de ejercicios anteriores, modificÃ¡ndolo si es necesario, para escoger el modelo con mayor precisiÃ³n sobre el subset de validaciÃ³n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2146e3-1801-4307-a06c-a528c28474c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escoge el modelo y el valor de lambda Ã³ptimos, con el menor error sobre el subset de CV\n",
    "\n",
    "# Itera sobre todas las combinaciones de theta y lambda y escoge las de menor coste en el subset de CV\n",
    "\n",
    "j_final = [...]\n",
    "theta_final = [...]\n",
    "lambda_final = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95dbd8-c54a-4252-b76f-bb1898b8e321",
   "metadata": {},
   "source": [
    "## Evaluar el modelo sobre el subset de test\n",
    "\n",
    "Finalmente, vamos a evaluar el modelo sobre un subset de datos que no hemos usado para entrenarlo ni para escoger ningÃºn hiper-parÃ¡metro.\n",
    "\n",
    "Para ello, vamos a calcular el coste o error total sobre el subset de test y comprobar grÃ¡ficamente los residuos sobre el mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd9907-032f-4198-b6d5-0c384ed159c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula el error del modelo sobre el subset de test usando la funciÃ³n de coste con las correspondientes theta y lambda\n",
    "\n",
    "j_test = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c216f-eace-44c5-aa46-764cb3f91750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula las predicciones del modelo sobre el subset de test, calcula los residuos y represÃ©ntalos frente al Ã­ndice de ejemplos (m)\n",
    "\n",
    "# Recuerda usar la funciÃ³n sigmoide para transformar las predicciones\n",
    "Y_test_pred = [...]\n",
    "\n",
    "residuos = [...]\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "# Completa con tu cÃ³digo\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea8c8f-849c-477b-9bf5-82904d149ac9",
   "metadata": {},
   "source": [
    "## Realizar predicciones sobre nuevos ejemplos\n",
    "\n",
    "Con nuestro modelo ya entrenado, optimizado y evaluado, lo Ãºnico que nos queda es ponerlo en funcionamiento realizando predicciones con nuevos ejemplos.\n",
    "\n",
    "Para ello, vamos a:\n",
    "- Generar un nuevo ejemplo, siguiendo el mismo patrÃ³n que el dataset original.\n",
    "- Normalizar sus caracterÃ­sticas antes de poder realizar predicciones sobre ellos.\n",
    "- Generar una predicciÃ³n para dicho nuevo ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f3b8d-e416-4dd6-bebb-7517f6b69a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un nuevo ejemplo siguiendo el patrÃ³n original, con tÃ©rmino de bias y error aleatorio\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "# Normaliza sus caracterÃ­sticas (excepto el tÃ©rmino de bias) con las medias y desviaciones tÃ­picas originales\n",
    "X_pred = [...]\n",
    "\n",
    "# Genera una predicciÃ³n para dicho ejemplo\n",
    "Y_pred = [...]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
