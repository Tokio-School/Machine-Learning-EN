{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf574b3-aadd-43f6-aa67-ab82651f0145",
   "metadata": {},
   "source": [
    "# Logistic Regression: Regularisation\n",
    "M2U5 - Exercise 5\n",
    "\n",
    "## What are we going to do?\n",
    "- We will implement the regularised cost and gradient descent functions\n",
    "- We will check the training by plotting the evolution of the cost function\n",
    "- We will find the optimal *lambda* regularisation parameter using validation\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "Once the unregularised cost function and gradient descent are implemented, we will regularise them and train a full logistic regression model, checking it by validation and evaluating it on a test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371fe604-a17c-4c68-9415-c516cfd380e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f7c59-0a6a-440c-80ac-5d172adaba24",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset for logistic regression\n",
    "\n",
    "We will create a synthetic dataset with only 2 classes (0 and 1) to test this implementation of a fully trained binary classification model, step by step.\n",
    "\n",
    "To do this, manually create a synthetic dataset for logistic regression with bias and error term (to have *Theta_true* available) with the code you used in the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40f576-6ce6-493a-81d7-6e7134f200bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Manually generate a synthetic dataset with a bias term and an error term\n",
    "m = 100\n",
    "n = 1\n",
    "\n",
    "# Generate a 2D m x n array with random values between -1 and 1\n",
    "# Insert a bias term as a first column of 1s\n",
    "X = [...]\n",
    "\n",
    "# Generate a theta array with n + 1 random values between [0, 1)\n",
    "Theta_true = [...]\n",
    "\n",
    "# Calculate Y as a function of X and *Theta_true*\n",
    "# Transform Y to values of 1 and 0 (float) when Y ≥ 0.0\n",
    "# Using a probability as the error term, iterate over Y and change the assigned class to its opposite, 1 to 0, and 0 to 1\n",
    "error = 0.15\n",
    "\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta and its dimensions to be estimated:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2aed3-82ba-4547-a2ae-50dcf3d4cea9",
   "metadata": {},
   "source": [
    "## Implement the sigmoid activation function\n",
    "\n",
    "Copy your cell with the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a747e-c79e-4a24-b8e6-a8a8573e21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac98be5-1acb-4510-8c66-e58424203d63",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "As we did for linear regression, we will preprocess the data completely, following the usual 3 steps:\n",
    "\n",
    "- Randomly reorder the data.\n",
    "- Normalise the data.\n",
    "- Divide the dataset into training, validation, and test subsets.\n",
    "\n",
    "You can do this manually or with Scikit-learn's auxiliary functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1f14c-40a6-4c34-a455-25c3ff250484",
   "metadata": {},
   "source": [
    "### Randomly rearrange the dataset\n",
    "\n",
    "Reorder the data in the *X* and *Y* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf1e1e-6330-4e27-8e5a-be7beba21b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the dataset\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Reorder X and Y:')\n",
    "# Use an initial random state of 42, in order to maintain reproducibility\n",
    "X, Y = [...]\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943d24b-c888-4412-bab5-8033e478e681",
   "metadata": {},
   "source": [
    "### Normalise the dataset\n",
    "\n",
    "Implement the normalisation function and normalise the dataset of *X* examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e6a10-0c2b-4a41-9fc9-39a88b922ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the dataset with a normalisation function\n",
    "\n",
    "# Copy the normalisation function you used in the linear regression exercise\n",
    "def normalize(x, mu, std):\n",
    "    pass\n",
    "\n",
    "# Find the mean and standard deviation of the features of X (columns), except the first column (bias)\n",
    "mu = [...]\n",
    "std = [...]\n",
    "\n",
    "print('Original X:')\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('Mean and standard deviation of the features:')\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('Normalized X:')\n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std)    # Normalize only column 1 and the subsequent columns, not column 0\n",
    "print(X_norm)\n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b79868-e71c-4193-80b6-3132daac1823",
   "metadata": {},
   "source": [
    "*Note*: If you had modified your normalize function to calculate and return the values of mu and std, you can modify this cell to include your custom code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a1f3f-6347-4296-88c4-20f786029c9a",
   "metadata": {},
   "source": [
    "### Divide the dataset into training, validation, and test subsets\n",
    "\n",
    "Divide the *X* and *Y* dataset into 3 subsets with the usual ratio, 60%/20%/20%.\n",
    "\n",
    "If your number of examples is much higher or lower, you can always modify this ratio to another ratio such as 50/25/25 or 80/10/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e00607-f834-464e-99af-6b9e0146cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide the X and Y dataset into the 3 subsets following the indicated ratios\n",
    "\n",
    "ratio = [60, 20, 20]\n",
    "print('Ratio:\\n', ratio, ratio[0] + ratio[1] + ratio[2])\n",
    "\n",
    "r = [0, 0]\n",
    "# Tip: the round() function and the x.shape attribute may be useful to you\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('Cutoff indices:\\n', r)\n",
    "\n",
    "# Tip: the np.array_split() function may be useful to you\n",
    "X_train, X_val, X_test = [...]\n",
    "Y_train, Y_val, Y_test = [...]\n",
    "\n",
    "print('Size of the subsets:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85ef3b-dd5c-4c1e-a4df-d5937e31f4a8",
   "metadata": {},
   "source": [
    "## Implement the sigmoid activation function\n",
    "\n",
    "Copy your cell with the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0c3a9-9f0e-4832-b795-4a3e349faafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b0553-219a-4f0d-89f0-11f4ea4e6c25",
   "metadata": {},
   "source": [
    "## Implement the regularised cost function\n",
    "\n",
    "We are going to implement the regularised cost function. This function will be similar to the one we implemented for linear regression in a previous exercise.\n",
    "\n",
    "Regularised cost function:\n",
    "\n",
    "$$ Y = h_\\Theta(x) = g(X \\times \\Theta^T) $$\n",
    "$$ J(\\Theta) = - [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (y^i log(h_\\theta(x^i)) + (1 - y^i) log(1 - h_\\theta(x^i))] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\Theta_j^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dd0bc-2987-4319-886c-01ce50e7133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the regularised cost function for logistic regression\n",
    "\n",
    "def regularized_logistic_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computes the cost function for the considered dataset and coefficients\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- ndarray 2D with the values of the independent variables from the examples, of size m x n\n",
    "    y -- ndarray 1D with the dependent/target variable, of size m x 1 and values of 0 or 1\n",
    "    theta -- ndarray 1D with the weights of the model coefficients, of size 1 x n (row vector)\n",
    "    lambda_ -- regularisation factor, by default 0.\n",
    "    \n",
    "    Return:\n",
    "    j -- float with the cost for this theta array\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Remember to check the dimensions of the matrix multiplication to perform it correctly\n",
    "    j = [...]\n",
    "    \n",
    "    # Regularise for all Theta except the bias term (the first value)\n",
    "    j += [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce13caa-cd63-4754-9bfa-692427d9581e",
   "metadata": {},
   "source": [
    "Now let's check your implementation in the following scenarios:\n",
    "1. For *lambda* = 0:\n",
    "    1. Using *Theta_true*, the cost should be 0.\n",
    "    1. As the value of *theta* moves away from *Theta_true*, the cost should increase.\n",
    "1. For *lambda* != 0:\n",
    "    1. Using *Theta_true*, the cost should be greater than 0.\n",
    "    1. The higher the *lambda*, the higher the cost.\n",
    "    1. The increase in cost as a function of lambda is exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7caff62-b979-43c3-acb7-ab1536115902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test your implementation on the dataset\n",
    "\n",
    "theta = Theta_true    # Modify and test several values of theta\n",
    "\n",
    "j = logistic_cost_function(X, Y, theta)\n",
    "\n",
    "print('Cost of the model:')\n",
    "print(j)\n",
    "print('Checked theta and Actual theta:')\n",
    "print(theta)\n",
    "print(Theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab374ab0-348f-47b3-b19f-92b690610d49",
   "metadata": {},
   "source": [
    "Record your experiments and results in this cell (in Markdown or code):\n",
    "\n",
    "1. Experiment 1\n",
    "1. Experiment 2\n",
    "1. Experiment 3\n",
    "1. Experiment 4\n",
    "1. Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17253ad8-9665-4190-8fb5-da043f92900f",
   "metadata": {},
   "source": [
    "## Train an initial model on the training subset\n",
    "\n",
    "As we did in previous exercises, we will train an initial model to check that our implementation and the dataset work correctly, and then we will be able to train a model with validation without any problem.\n",
    "\n",
    "To do this, follow the same steps as you did for linear regression:\n",
    "- Train an initial model without regularisation.\n",
    "- Plot the history of the cost function to check its evolution.\n",
    "- If necessary, modify any of the parameters and retrain the model. You will use these parameters in the following steps.\n",
    "\n",
    "Copy the cells from previous exercises where you implemented the cost function in unregularised logistic regression and the cell where you trained the model, and modify them for regularised logistic regression.\n",
    "\n",
    "Recall the gradient descent functions for regularised logistic regression:\n",
    "\n",
    "$$ Y = h_\\Theta(x) = g(X \\times \\Theta^T) $$\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i + \\frac{\\lambda}{m} \\theta_j]; \\space j \\in [1, n] $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e25d51-0191-47ec-9e35-0fe2df104a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cell with the gradient descent for unregularised logistic regression and modify it to implement the regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7e8f5-e69c-44c1-b918-e875509a1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cell where we trained the model\n",
    "# Train your model on the unregularised training subset and check that it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af06ed-1362-42df-b9ab-eda151c3bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the evolution of the cost function vs. the number of iterations\n",
    "\n",
    "plt.figure(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e710b7-7e10-48b4-b110-ce1f880af6f4",
   "metadata": {},
   "source": [
    "### Comprobar la implementación\n",
    "\n",
    "Comprueba de nuevo tu implementación, al igual que hiciste en el ejercicio anterior.\n",
    "\n",
    "En esta ocasión, además, comprueba cómo con una *lambda* distinta a 0 la penalización hace que el coste sea mayor cuanto mayor sea esta *lambda*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29135cf0-3451-48a0-9d84-d5ce27c46105",
   "metadata": {},
   "source": [
    "### Comprobar si existe desviación o sobreajuste\n",
    "\n",
    "Al igual que hacíamos en la regresión lineal, vamos a comprobar si existe sobreajuste comparando el coste del modelo en el dataset de entrenamiento y de validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26ac15-42a2-4f3d-bbb5-50128460f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba el coste del modelo sobre el dataset de entrenamiento y validación\n",
    "# Utiliza la Theta_final del modelo entrenado en ambos casos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294ecc3-1ab7-4c93-9f80-de6990e1ffb3",
   "metadata": {},
   "source": [
    "Recuerda que con un dataset sintético aleatorio es difícil que se diera un caso u otro, pero de esta forma podríamos apreciar dichos problemas de la siguiente forma:\n",
    "\n",
    "- Si el coste final en ambos subsets es alto, puede haber un problema de desviación o *bias*.\n",
    "- Si el coste final en ambos subsets es muy diferente entre sí, puede haber un problema de sobreajuste o *varianza*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6381277-2d83-48e8-ad23-f09a131c1b7f",
   "metadata": {},
   "source": [
    "## Hallar el hiper-parámetro *lambda* óptimo por validación\n",
    "\n",
    "Del mismo modo que hemos hecho en ejercicios anteriores, vamos a optimizar nuestro parámetro de regularización por validación.\n",
    "\n",
    "Para ello vamos a entrenar un modelo diferente por cada valor de *lambda* a considerar sobre el subset de entrenamiento, y evaluar su error o coste final sobre el subset de validación.\n",
    "\n",
    "Vamos a representar gráficamente el error de cada modelo vs el valor de *lambda* usado e implementar un código que elegirá automáticamente el modelo más óptimo de entre todos.\n",
    "\n",
    "Recuerda entrenar todos tus modelos en igualdad de condiciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20de71-246c-49e0-907d-2290019a6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por cada valor de lambda diferente sobre X_train y evalúalo sobre X_val\n",
    "\n",
    "# Usa de nuevo un espacio logarítmico entre 10 y 10^3 de 10 elementos con valores que comiencen por un decimal no-cero 1 o 3\n",
    "lambdas = [...]\n",
    "\n",
    "# Completa el código para entrenar un modelo diferente para cada valor de lambda sobre X_train\n",
    "# Almacena su theta y error/coste final\n",
    "# Posteriormente, evalúa su coste total en el subset de validación\n",
    "\n",
    "# Almacena dicha información en los siguientes ndarrays, del mismo tamaño que lambdas\n",
    "j_train = [...]\n",
    "j_val = [...]\n",
    "theta_val = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53229f4a-5ce9-4b1d-9613-179a2aaa6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente el error final para cada valor de lambda\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "# Completa con tu código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50326c71-7a1c-4931-a057-edbf9e129194",
   "metadata": {},
   "source": [
    "### Escoger el mejor modelo\n",
    "\n",
    "Copia el código de ejercicios anteriores, modificándolo si es necesario, para escoger el modelo con mayor precisión sobre el subset de validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2146e3-1801-4307-a06c-a528c28474c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escoge el modelo y el valor de lambda óptimos, con el menor error sobre el subset de CV\n",
    "\n",
    "# Itera sobre todas las combinaciones de theta y lambda y escoge las de menor coste en el subset de CV\n",
    "\n",
    "j_final = [...]\n",
    "theta_final = [...]\n",
    "lambda_final = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95dbd8-c54a-4252-b76f-bb1898b8e321",
   "metadata": {},
   "source": [
    "## Evaluar el modelo sobre el subset de test\n",
    "\n",
    "Finalmente, vamos a evaluar el modelo sobre un subset de datos que no hemos usado para entrenarlo ni para escoger ningún hiper-parámetro.\n",
    "\n",
    "Para ello, vamos a calcular el coste o error total sobre el subset de test y comprobar gráficamente los residuos sobre el mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd9907-032f-4198-b6d5-0c384ed159c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula el error del modelo sobre el subset de test usando la función de coste con las correspondientes theta y lambda\n",
    "\n",
    "j_test = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c216f-eace-44c5-aa46-764cb3f91750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula las predicciones del modelo sobre el subset de test, calcula los residuos y represéntalos frente al índice de ejemplos (m)\n",
    "\n",
    "# Recuerda usar la función sigmoide para transformar las predicciones\n",
    "Y_test_pred = [...]\n",
    "\n",
    "residuos = [...]\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "# Completa con tu código\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea8c8f-849c-477b-9bf5-82904d149ac9",
   "metadata": {},
   "source": [
    "## Realizar predicciones sobre nuevos ejemplos\n",
    "\n",
    "Con nuestro modelo ya entrenado, optimizado y evaluado, lo único que nos queda es ponerlo en funcionamiento realizando predicciones con nuevos ejemplos.\n",
    "\n",
    "Para ello, vamos a:\n",
    "- Generar un nuevo ejemplo, siguiendo el mismo patrón que el dataset original.\n",
    "- Normalizar sus características antes de poder realizar predicciones sobre ellos.\n",
    "- Generar una predicción para dicho nuevo ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f3b8d-e416-4dd6-bebb-7517f6b69a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un nuevo ejemplo siguiendo el patrón original, con término de bias y error aleatorio\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "# Normaliza sus características (excepto el término de bias) con las medias y desviaciones típicas originales\n",
    "X_pred = [...]\n",
    "\n",
    "# Genera una predicción para dicho ejemplo\n",
    "Y_pred = [...]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
