{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf574b3-aadd-43f6-aa67-ab82651f0145",
   "metadata": {},
   "source": [
    "# Logistic Regression: Regularisation\n",
    "M2U5 - Exercise 5\n",
    "\n",
    "## What are we going to do?\n",
    "- We will implement the regularised cost and gradient descent functions\n",
    "- We will check the training by plotting the evolution of the cost function\n",
    "- We will find the optimal *lambda* regularisation parameter using validation\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "Once the unregularised cost function and gradient descent are implemented, we will regularise them and train a full logistic regression model, checking it by validation and evaluating it on a test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371fe604-a17c-4c68-9415-c516cfd380e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f7c59-0a6a-440c-80ac-5d172adaba24",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset for logistic regression\n",
    "\n",
    "We will create a synthetic dataset with only 2 classes (0 and 1) to test this implementation of a fully trained binary classification model, step by step.\n",
    "\n",
    "To do this, manually create a synthetic dataset for logistic regression with bias and error term (to have *Theta_true* available) with the code you used in the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40f576-6ce6-493a-81d7-6e7134f200bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Manually generate a synthetic dataset with a bias term and an error term\n",
    "m = 100\n",
    "n = 1\n",
    "\n",
    "# Generate a 2D m x n array with random values between -1 and 1\n",
    "# Insert a bias term as a first column of 1s\n",
    "X = [...]\n",
    "\n",
    "# Generate a theta array with n + 1 random values between [0, 1)\n",
    "Theta_true = [...]\n",
    "\n",
    "# Calculate Y as a function of X and *Theta_true*\n",
    "# Transform Y to values of 1 and 0 (float) when Y ≥ 0.0\n",
    "# Using a probability as the error term, iterate over Y and change the assigned class to its opposite, 1 to 0, and 0 to 1\n",
    "error = 0.15\n",
    "\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta and its dimensions to be estimated:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2aed3-82ba-4547-a2ae-50dcf3d4cea9",
   "metadata": {},
   "source": [
    "## Implement the sigmoid activation function\n",
    "\n",
    "Copy your cell with the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a747e-c79e-4a24-b8e6-a8a8573e21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac98be5-1acb-4510-8c66-e58424203d63",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "As we did for linear regression, we will preprocess the data completely, following the usual 3 steps:\n",
    "\n",
    "- Randomly reorder the data.\n",
    "- Normalise the data.\n",
    "- Divide the dataset into training, validation, and test subsets.\n",
    "\n",
    "You can do this manually or with Scikit-learn's auxiliary functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1f14c-40a6-4c34-a455-25c3ff250484",
   "metadata": {},
   "source": [
    "### Randomly rearrange the dataset\n",
    "\n",
    "Reorder the data in the *X* and *Y* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf1e1e-6330-4e27-8e5a-be7beba21b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the dataset\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Reorder X and Y:')\n",
    "# Use an initial random state of 42, in order to maintain reproducibility\n",
    "X, Y = [...]\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943d24b-c888-4412-bab5-8033e478e681",
   "metadata": {},
   "source": [
    "### Normalise the dataset\n",
    "\n",
    "Implement the normalisation function and normalise the dataset of *X* examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e6a10-0c2b-4a41-9fc9-39a88b922ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the dataset with a normalisation function\n",
    "\n",
    "# Copy the normalisation function you used in the linear regression exercise\n",
    "def normalize(x, mu, std):\n",
    "    pass\n",
    "\n",
    "# Find the mean and standard deviation of the features of X (columns), except the first column (bias)\n",
    "mu = [...]\n",
    "std = [...]\n",
    "\n",
    "print('Original X:')\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('Mean and standard deviation of the features:')\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('Normalized X:')\n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std)    # Normalize only column 1 and the subsequent columns, not column 0\n",
    "print(X_norm)\n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b79868-e71c-4193-80b6-3132daac1823",
   "metadata": {},
   "source": [
    "*Note*: If you had modified your normalize function to calculate and return the values of mu and std, you can modify this cell to include your custom code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a1f3f-6347-4296-88c4-20f786029c9a",
   "metadata": {},
   "source": [
    "### Divide the dataset into training, validation, and test subsets\n",
    "\n",
    "Divide the *X* and *Y* dataset into 3 subsets with the usual ratio, 60%/20%/20%.\n",
    "\n",
    "If your number of examples is much higher or lower, you can always modify this ratio to another ratio such as 50/25/25 or 80/10/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e00607-f834-464e-99af-6b9e0146cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide the X and Y dataset into the 3 subsets following the indicated ratios\n",
    "\n",
    "ratio = [60, 20, 20]\n",
    "print('Ratio:\\n', ratio, ratio[0] + ratio[1] + ratio[2])\n",
    "\n",
    "r = [0, 0]\n",
    "# Tip: the round() function and the x.shape attribute may be useful to you\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('Cutoff indices:\\n', r)\n",
    "\n",
    "# Tip: the np.array_split() function may be useful to you\n",
    "X_train, X_val, X_test = [...]\n",
    "Y_train, Y_val, Y_test = [...]\n",
    "\n",
    "print('Size of the subsets:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85ef3b-dd5c-4c1e-a4df-d5937e31f4a8",
   "metadata": {},
   "source": [
    "## Implement the sigmoid activation function\n",
    "\n",
    "Copy your cell with the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0c3a9-9f0e-4832-b795-4a3e349faafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b0553-219a-4f0d-89f0-11f4ea4e6c25",
   "metadata": {},
   "source": [
    "## Implement the regularised cost function\n",
    "\n",
    "We are going to implement the regularised cost function. This function will be similar to the one we implemented for linear regression in a previous exercise.\n",
    "\n",
    "Regularised cost function:\n",
    "\n",
    "$$ Y = h_\\Theta(x) = g(X \\times \\Theta^T) $$\n",
    "$$ J(\\Theta) = - [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (y^i log(h_\\theta(x^i)) + (1 - y^i) log(1 - h_\\theta(x^i))] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\Theta_j^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dd0bc-2987-4319-886c-01ce50e7133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the regularised cost function for logistic regression\n",
    "\n",
    "def regularized_logistic_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computes the cost function for the considered dataset and coefficients\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- ndarray 2D with the values of the independent variables from the examples, of size m x n\n",
    "    y -- ndarray 1D with the dependent/target variable, of size m x 1 and values of 0 or 1\n",
    "    theta -- ndarray 1D with the weights of the model coefficients, of size 1 x n (row vector)\n",
    "    lambda_ -- regularisation factor, by default 0.\n",
    "    \n",
    "    Return:\n",
    "    j -- float with the cost for this theta array\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Remember to check the dimensions of the matrix multiplication to perform it correctly\n",
    "    j = [...]\n",
    "    \n",
    "    # Regularise for all Theta except the bias term (the first value)\n",
    "    j += [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce13caa-cd63-4754-9bfa-692427d9581e",
   "metadata": {},
   "source": [
    "Now let's check your implementation in the following scenarios:\n",
    "1. For *lambda* = 0:\n",
    "    1. Using *Theta_true*, the cost should be 0.\n",
    "    1. As the value of *theta* moves away from *Theta_true*, the cost should increase.\n",
    "1. For *lambda* != 0:\n",
    "    1. Using *Theta_true*, the cost should be greater than 0.\n",
    "    1. The higher the *lambda*, the higher the cost.\n",
    "    1. The increase in cost as a function of lambda is exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7caff62-b979-43c3-acb7-ab1536115902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test your implementation on the dataset\n",
    "\n",
    "theta = Theta_true    # Modify and test several values of theta\n",
    "\n",
    "j = logistic_cost_function(X, Y, theta)\n",
    "\n",
    "print('Cost of the model:')\n",
    "print(j)\n",
    "print('Checked theta and Actual theta:')\n",
    "print(theta)\n",
    "print(Theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab374ab0-348f-47b3-b19f-92b690610d49",
   "metadata": {},
   "source": [
    "Record your experiments and results in this cell (in Markdown or code):\n",
    "\n",
    "1. Experiment 1\n",
    "1. Experiment 2\n",
    "1. Experiment 3\n",
    "1. Experiment 4\n",
    "1. Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17253ad8-9665-4190-8fb5-da043f92900f",
   "metadata": {},
   "source": [
    "## Train an initial model on the training subset\n",
    "\n",
    "As we did in previous exercises, we will train an initial model to check that our implementation and the dataset work correctly, and then we will be able to train a model with validation without any problem.\n",
    "\n",
    "To do this, follow the same steps as you did for linear regression:\n",
    "- Train an initial model without regularisation.\n",
    "- Plot the history of the cost function to check its evolution.\n",
    "- If necessary, modify any of the parameters and retrain the model. You will use these parameters in the following steps.\n",
    "\n",
    "Copy the cells from previous exercises where you implemented the cost function in unregularised logistic regression and the cell where you trained the model, and modify them for regularised logistic regression.\n",
    "\n",
    "Recall the gradient descent functions for regularised logistic regression:\n",
    "\n",
    "$$ Y = h_\\Theta(x) = g(X \\times \\Theta^T) $$\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i + \\frac{\\lambda}{m} \\theta_j]; \\space j \\in [1, n] $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e25d51-0191-47ec-9e35-0fe2df104a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cell with the gradient descent for unregularised logistic regression and modify it to implement the regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7e8f5-e69c-44c1-b918-e875509a1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cell where we trained the model\n",
    "# Train your model on the unregularised training subset and check that it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af06ed-1362-42df-b9ab-eda151c3bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the evolution of the cost function vs. the number of iterations\n",
    "\n",
    "plt.figure(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e710b7-7e10-48b4-b110-ce1f880af6f4",
   "metadata": {},
   "source": [
    "### Check the implementation\n",
    "\n",
    "Check your implementation again, as you did in the previous exercise.\n",
    "\n",
    "On this occasion, it also shows that for a *lambda* other than 0, the higher the *lambda* the higher the cost will be, due to the penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29135cf0-3451-48a0-9d84-d5ce27c46105",
   "metadata": {},
   "source": [
    "### Check for deviation or overfitting\n",
    "\n",
    "As we did in linear regression, we will check for overfitting by comparing the cost of the model on the training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26ac15-42a2-4f3d-bbb5-50128460f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check the cost of the model on the training and validation datasets\n",
    "# Use the Theta_final of the trained model in both cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294ecc3-1ab7-4c93-9f80-de6990e1ffb3",
   "metadata": {},
   "source": [
    "Remember that with a random synthetic dataset it is difficult to have one or the other, but by proceeding in this way we will be able to identify the following problems:\n",
    "\n",
    "- If the final cost in both subsets is high, there may be a problem with deviation or *bias*.\n",
    "- If the final costs in both subsets are very different from each other, there may be a problem with overfitting or *variance*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6381277-2d83-48e8-ad23-f09a131c1b7f",
   "metadata": {},
   "source": [
    "## Find the optimal *lambda* hyperparameter using validation\n",
    "\n",
    "As we have done in previous exercises, we will optimise our regularisation parameter by validation.\n",
    "\n",
    "To do this, we will train a different model on the training subset for each lambda value to be considered, and evaluate its error or final cost on the validation subset.\n",
    "\n",
    "We will plot the error of each model vs. the *lambda* value used and implement a code that will automatically choose the most optimal model among all of them.\n",
    "\n",
    "Remember to train all your models under equal conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20de71-246c-49e0-907d-2290019a6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a model on X_train for each different lambda value and evaluate it on X_val\n",
    "\n",
    "# Use a logarithmic space between 10 and 10^3 with 10 elements with non-zero decimal values starting with a 1 or a 3\n",
    "lambdas = [...]\n",
    "\n",
    "# Complete the code to train a different model for each class and value of lambda on X_train\n",
    "# Store your theta and final cost/error\n",
    "# Afterwards, evaluate its total cost on the validation subset\n",
    "\n",
    "# Store this information in the following arrays, which are the same size as lambda’s arrays\n",
    "j_train = [...]\n",
    "j_val = [...]\n",
    "theta_val = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53229f4a-5ce9-4b1d-9613-179a2aaa6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the final error for each value of lambda\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "# Fill in your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50326c71-7a1c-4931-a057-edbf9e129194",
   "metadata": {},
   "source": [
    "### Choosing the best model\n",
    "\n",
    "Copy the code from previous exercises and modify it to choose the most accurate model on the validation subset for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2146e3-1801-4307-a06c-a528c28474c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose the optimal model and lambda value, with the lowest error on the CV subset\n",
    "\n",
    "# Iterate over all the combinations of theta and lambda and choose the one with the lowest cost on the CV subset\n",
    "\n",
    "j_final = [...]\n",
    "theta_final = [...]\n",
    "lambda_final = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95dbd8-c54a-4252-b76f-bb1898b8e321",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test subset\n",
    "\n",
    "Finally, we will evaluate the model on a subset of data that we have not used for its training nor to choose any of its hyperparameters.\n",
    "\n",
    "Therefore, we will calculate the total error or cost on the test subset and graphically check the residuals of the model on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd9907-032f-4198-b6d5-0c384ed159c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the model error on the test subset using the cost function with the corresponding theta and lambda\n",
    "\n",
    "j_test = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c216f-eace-44c5-aa46-764cb3f91750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the predictions of the model on the test subset, calculate the residuals and plot them against the index of examples (m)\n",
    "\n",
    "# Remember to use the sigmoid function to transform the predictions\n",
    "Y_test_pred = [...]\n",
    "\n",
    "residuals = [...]\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "# Fill in your code\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea8c8f-849c-477b-9bf5-82904d149ac9",
   "metadata": {},
   "source": [
    "## Make predictions about new examples\n",
    "\n",
    "With our model trained, optimised, and evaluated, all that remains is to put it to work by making predictions with new examples.\n",
    "\n",
    "To do this, we will:\n",
    "- Generate a new example, which follows the same pattern as the original dataset.\n",
    "- Normalise its features before making predictions about them.\n",
    "- Generate a prediction for this new example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f3b8d-e416-4dd6-bebb-7517f6b69a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a new example following the original pattern, with a bias term and a random error term.\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "# Normalise its features (except the bias term) with the original means and standard deviations\n",
    "X_pred = [...]\n",
    "\n",
    "# Generate a prediction for this new example\n",
    "Y_pred = [...]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
