{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c3d42e-4278-4e97-80b1-29c7bd9dca9e",
   "metadata": {},
   "source": [
    "# Logistic Regression: Training and predictions\n",
    "M2U5 - Exercise 4\n",
    "\n",
    "## What are we going to do?\n",
    "- We will create a synthetic dataset for logistic regression\n",
    "- We will preprocess the data\n",
    "- We will train the model using gradient descent\n",
    "- We will check the training by plotting the evolution of the cost function\n",
    "- We will make predictions about new examples\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "Once the cost function is implemented, we will train a gradient descent logistic regression model, testing our training, evaluating it on a test subset and finally, making predictions on it.\n",
    "\n",
    "This time we will work with a binary logistic regression, while in other exercises we will consider a multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75001ae7-ec7a-404c-b18e-626111215ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c8385-e9ef-4891-a6e1-5a32415301d0",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset for logistic regression\n",
    "\n",
    "We will create a synthetic dataset with only 2 classes (0 and 1) to test this implementation of a fully trained binary classification model, step by step.\n",
    "\n",
    "To do this, manually create a synthetic dataset for logistic regression with bias and error term (to have *Theta_true* available) with the code you used in the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e34da-36ff-4d10-82aa-a23368c62f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Manually generate a synthetic dataset with a bias term and an error term\n",
    "m = 100\n",
    "n = 1\n",
    "\n",
    "# Generate a 2D m x n array with random values between -1 and 1\n",
    "# Insert a bias term as a first column of 1s\n",
    "X = [...]\n",
    "\n",
    "# Generate a theta array with n + 1 random values between [0, 1)\n",
    "Theta_true = [...]\n",
    "\n",
    "# Calculate Y as a function of X and Theta_true\n",
    "# Transform Y to values of 1 and 0 (float) when Y â‰¥ 0.0\n",
    "# Using a probability as the error term, iterate over Y and change the assigned class to its opposite, 1 to 0, and 0 to 1\n",
    "error = 0.15\n",
    "\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta and its dimensions to be estimated:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104d14f-d030-4080-8857-f5b87b7ec9bf",
   "metadata": {},
   "source": [
    "## Implement the sigmoid activation function\n",
    "\n",
    "Copy your cell with the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c1160-8942-4119-93a1-c9fc9e51842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af47ac7-c02e-4e8c-8aa1-ec4ce7fa2109",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "As we did for linear regression, we will preprocess the data completely, following the usual 3 steps:\n",
    "\n",
    "- Randomly reorder the data.\n",
    "- Normalise the data.\n",
    "- Divide the dataset into training and test subsets.\n",
    "\n",
    "You can do this manually or with Scikit-learn's auxiliary functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f6925-be52-4e3f-afe5-145319c40611",
   "metadata": {},
   "source": [
    "### Randomly reorder the dataset\n",
    "\n",
    "Reorder the data in the *X* and *Y* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a26f38-d730-456d-9154-7500c2d86b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Randomly reorder the dataset\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Reorder X and Y:')\n",
    "# Use an initial random state of 42, in order to maintain reproducibility\n",
    "X, Y = [...]\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659a404-06e7-4d25-b542-35c1a36332bb",
   "metadata": {},
   "source": [
    "### Normalise the dataset\n",
    "\n",
    "Implement the normalisation function and normalise the dataset of *X* examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28586e-67db-4555-8e9f-606a17871561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the dataset with a normalisation function\n",
    "\n",
    "# Copy the normalisation function you used in the linear regression exercise\n",
    "def normalize(x, mu, std):\n",
    "    pass\n",
    "\n",
    "# Find the mean and standard deviation of the X features (columns), except for the first one (bias)\n",
    "mu = [...]\n",
    "std = [...]\n",
    "\n",
    "print('Original X:')\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('Mean and standard deviation of the features:')\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('Normalised X:')\n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std)    # Normalise only column 1 and the subsequent columns, not column 0\n",
    "print(X_norm)\n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325eadb3-b554-48f1-8502-80f3bc962a5f",
   "metadata": {},
   "source": [
    "### Divide the dataset into training and test subsets\n",
    "\n",
    "Divide the *X* and *Y* dataset into 2 subsets with the usual ratio of 70%/30%.\n",
    "\n",
    "If your number of examples is much higher or lower, you can always modify this ratio accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a5f7f-443b-43f9-8dd9-e18391a647d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide the X and Y dataset into the 2 subsets according to the indicated ratio\n",
    "\n",
    "ratio = [70, 30]\n",
    "print('Ratio:\\n', ratio, ratio[0] + ratio[1])\n",
    "\n",
    "# Cutoff index\n",
    "# Tip: the round() function and the x.shape attribute may be useful to you\n",
    "r = [...]\n",
    "print('Cutoff indices:\\n', r)\n",
    "\n",
    "# Tip: the np.array_split() function may be useful to you\n",
    "X_train, X_test = [...]\n",
    "Y_train, Y_test = [...]\n",
    "\n",
    "print('Size of the subsets:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc8050-7a75-4e4d-b118-495a09e6d759",
   "metadata": {},
   "source": [
    "## Train an initial model on the training subset\n",
    "\n",
    "As we did in previous exercises, we will train an initial model to check that our implementation and the dataset work correctly, and then we will be able to train a model with validation without any problem.\n",
    "\n",
    "To do this, follow the same steps as you did for linear regression:\n",
    "- Train an initial model without implementing regularisation.\n",
    "- Plot the history of the cost function to check its evolution.\n",
    "- If necessary, modify any of the parameters and retrain the model. You will use these parameters in the following steps.\n",
    "\n",
    "Copy the cells from previous exercises where you implemented the cost function for logistic regression, the unregularised gradient descent for linear regression, and the cell where you trained the regression model, and modify them for logistic regression.\n",
    "\n",
    "Recall the gradient descent functions for logistic regression:\n",
    "\n",
    "$$ Y = h_\\Theta(x) = g(X \\times \\Theta^T) $$\n",
    "$$ \\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630ea2f-e1c7-477f-865f-f09bb26f4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cell with the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f44be2-e863-4856-b08a-04f639092ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cell with the unregularised gradient descent function for linear regression and adapt it for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21bacf5-2af3-4900-b810-2ff9edd7f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the cell where we trained the model\n",
    "# Train your model on the unregularised training subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be70b0-10fe-482b-aa24-b2dcd4f38000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the evolution of the cost function vs. the number of iterations\n",
    "\n",
    "plt.figure(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9108c89-0d80-4001-87d6-b21bd30c32b0",
   "metadata": {},
   "source": [
    "Check your implementation in the following scenarios:\n",
    "1. Using *Theta_true*, the final cost should be practically 0 and converge in a couple of iterations.\n",
    "1. As the value of *theta* moves away from *Theta_true*, it should need more iterations to converge, and *theta_final* should be very similar to *Theta_true*.\n",
    "\n",
    "To do this, remember that you can modify the values of the cells and re-execute them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1022f9-323e-4ff3-bcdd-030558f2f966",
   "metadata": {},
   "source": [
    "Record your experiments and results in this cell (in Markdown or code):\n",
    "1. Experiment 1\n",
    "1. Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad87473-1622-4e42-adb3-507d60607562",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test subset\n",
    "\n",
    "Finally, we will evaluate the model on a subset of data that we have not used to train it.\n",
    "\n",
    "Therefore, we will calculate the total cost or error on the test subset and graphically check the residuals of the model on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10047f-8f27-4994-aac1-045b1a030f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the error of the model on the test subset using the cost function with the corresponding theta\n",
    "\n",
    "j_test = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc3271-f2b7-4672-9e1b-570647bbee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the predictions of the model on the test subset, calculate the residuals and plot them against the index of examples (m)\n",
    "\n",
    "# Remember to use the sigmoid function to transform the predictions\n",
    "Y_test_pred = [...]\n",
    "\n",
    "residuals = [...]\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "# Fill in your code\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c322b-bbbe-4536-a62f-d460c20cdc43",
   "metadata": {},
   "source": [
    "## Make predictions about new examples\n",
    "\n",
    "With our model trained, optimised, and evaluated, all that remains is to put it to work by making predictions with new examples.\n",
    "\n",
    "To do this, we will:\n",
    "- Generate a new example, following the same pattern as the original dataset.\n",
    "- Normalise its features before making predictions about them.\n",
    "- Generate a prediction for this new example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4170c69-9850-4069-89da-70d715e6b4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Generate a new example following the original pattern, with a bias term and a random error term\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "# Normalise its features (except the bias term) with the original means and standard deviations\n",
    "X_pred = [...]\n",
    "\n",
    "# Generate a prediction for this new example\n",
    "Y_pred = [...]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
