{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a483fea-5052-486a-a650-5801d6883aa1",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression: Gradient descent\n",
    "M2U2 - Exercise 2\n",
    "\n",
    "## What are we going to do?\n",
    "- Implement the optimization of the cost function using gradient descent, or in other words, training the model\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This exercise is a continuation of the previous exercise \"Cost function\", so you should build on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43af36-a639-4381-8424-72241955e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed96c4-774a-4e0b-88a2-9c23c7f9181b",
   "metadata": {},
   "source": [
    "## Task 1: Implement the cost function for multivariate linear regression\n",
    "\n",
    "In this task, you must copy the corresponding cell from the previous exercise, bringing your code to implement the vectorised cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300d424-c3ba-47d5-9254-65716fbd1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the vectorised cost function following the template below\n",
    "\n",
    "def cost_function(x, y, theta):\n",
    "    \"\"\" Compute the cost function for the considered dataset and coefficients.\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the values of the independent variables from the examples, of size m x n\n",
    "    y -- Numpy 1D array with the dependent/target variable, of size m x 1\n",
    "    theta -- Numpy 1D array with the weights of the model coefficients, of size 1 x n (row vector)\n",
    "    \n",
    "    Return:\n",
    "    j -- float with the cost for this theta array\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Remember to check the dimensions of the matrix multiplication to do it correctly\n",
    "    j = [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09977c-81e4-44d3-babf-e4a6ee0a9fb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2: Implement the optimisation of this cost function using gradient descent\n",
    "\n",
    "We are now going to solve the optimisation of this cost function to train the model, using the vectorised gradient descent method. The model will be considered trained when its cost function has reached a minimum, stable value.\n",
    "\n",
    "$$Y = h_\\Theta(X) = X \\times \\Theta^T$$\n",
    "\n",
    "$$J_\\theta = \\frac{1}{2m} \\sum_{i = 0}^{m} (h_\\theta(x^i) - y^i)^2$$\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i = 0}^{m}{(h_\\theta(x^i) - y^i) x_j^i}]$$\n",
    "\n",
    "To do this, once again, fill in the code template in the next cell.\n",
    "\n",
    "Tips:\n",
    "- If you prefer, you can first implement the function with loops and iterations, and finally in a vectorised way\n",
    "- Remember the dimensions of each vector/matrix\n",
    "- Again, record the operations in step-by-step order on a sheet or in an auxiliary cell\n",
    "- At each step, write down the dimensions of your result, which you can also check in your code\n",
    "- Use numpy.matmul() for matrix multiplication\n",
    "- At the start of each training iteration, you must copy all $\\Theta$ values, since you are going to iterate by updating each of its values based on the entire vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0980f-0796-4f79-aed6-d02f9e95d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the function that trains the model using gradient descent\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha, e, iter_):\n",
    "    \"\"\" Trains the model by optimising its gradient descent cost function\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the values of the independent variables from the examples, of size m x n\n",
    "    y -- Numpy 1D array with the dependent/target variable, of size m x 1\n",
    "    theta -- Numpy 1D array with the weights of the model coefficients, of size 1 x n (row vector)\n",
    "    alpha -- float, training rate\n",
    "    \n",
    "    Named arguments (keyword):\n",
    "    e -- float, minimum difference between iterations to declare that the training has finally converged\n",
    "    iter_ -- int/float, nº of iterations\n",
    "    \n",
    "    Return:\n",
    "    j_hist -- list/array with the evolution of the cost function during the training\n",
    "    theta -- NumPy array with the value of theta at the last iteration\n",
    "    \"\"\"\n",
    "    # TODO: enters default values for e and iter_ in the function keyword arguments\n",
    "    \n",
    "    iter_ = int(iter_)    # If you have entered iter_ in scientific notation (1E3) or float (1000.), converts it\n",
    "    \n",
    "    # Initialises j_hist as a list or a NumPy array. Remember that we do not know what size it will eventually be\n",
    "    # Your max. nº of elements will be the max. nº of iterations\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...]    # Obtain m and n from the dimensions of X\n",
    "    \n",
    "    for k in [...]:    # Iterate over the maximum nº of iterations\n",
    "        theta_iter = [...]    # Copy the theta for each iteration with \"deep copy\" since we have to update it\n",
    "        \n",
    "        for j in [...]:    # Iterate over the nº of features\n",
    "            # Update theta_iter for each feature, according to the derivative of the cost function\n",
    "            # Include the training rate alpha\n",
    "            # Careful with the matrix multiplication, its order and dimensions\n",
    "            theta_iter[j] = theta[j] - [...]\n",
    "            \n",
    "        theta = theta_iter    # Updates the entire theta, ready for the next iteration\n",
    "        \n",
    "        cost = cost_function([...])    # Calculates the cost for the current iteration of theta\n",
    "        \n",
    "        j_hist[...]    # Adds the cost of the current iteration to the cost history\n",
    "        \n",
    "        # Check if the difference between the cost of the current iteration and that of the last iteration in absolute value\n",
    "        # is less than the minimum difference to declare convergence, e, for all iterations\n",
    "        # except the first\n",
    "        if k > 0 and [...]:\n",
    "            print('Converge at iteration nº: ', k)\n",
    "            \n",
    "            break\n",
    "    else:\n",
    "        print('Max. nº of iterations reached')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b469bd3-80f2-4cbe-baa5-6a10c06f4ca1",
   "metadata": {},
   "source": [
    "## Task 3: Check the implementation of gradient descent\n",
    "\n",
    "To check your implementation, again, use the same cell, varying its parameters several times, plotting the evolution of the cost function and seeing how its value approaches 0.\n",
    "\n",
    "In each case, check that the initial and final $\\Theta$ are very similar in the following scenarios:\n",
    "1. It generates several synthetic datasets, testing each of them\n",
    "1. It modifies the nº of examples and features, m and n\n",
    "1. It modifies the error parameter, which may cause the initial and final $\\Theta$ to not quite match, and the greater the error, the more difference there may be\n",
    "1. Check the max. nº of iterations or the training rate α hyperparameters, which will make the model take more or less time to train, within minimum and maximum values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609719f-aab2-4ea3-94c5-6c05c72deabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un dataset sintético, con término de error, de la forma que escojas, con Numpy o Scikit-learn\n",
    "\n",
    "m = 0\n",
    "n = 0\n",
    "e = 0.\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_verd = [...]\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Comprueba los valores y dimensiones (forma o \"shape\") de los vectores\n",
    "print('Theta real a estimar:')\n",
    "print()\n",
    "print('shape')\n",
    "\n",
    "print('Primeras 10 filas y 5 columnas de X e Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensiones de X e Y:')\n",
    "print('shape', 'shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d7f8c-ef7e-4358-b027-32f0fac388ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba tu implementación entrenando un modelo sobre el dataset sintético creado previamente\n",
    "\n",
    "# Utiliza una theta iniciada aleatoriamente o la Theta_verd, en función del escenario a comprobar\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "e = 1e-3\n",
    "iter_ = 1e3    # Comprueba que tu función puede admitir valores float o modifícalo\n",
    "\n",
    "print('Hiper-arámetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = gradient_descent([...])\n",
    "\n",
    "print('Tiempo de entrenamiento (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores de la función de coste')\n",
    "print(j_hist[...])\n",
    "print('\\nCoste final:')\n",
    "print(j_hist[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdaderos de Theta y diferencia con valores entrenados:')\n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26243d6-8737-417c-acbb-1ae426dd05d0",
   "metadata": {},
   "source": [
    "Representa gráficamente el histórico de la función de coste para comprobar tu implementación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdecba-cf90-4d35-99b7-5da9cc2b00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente la función de coste vs el nº de iteraciones\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Función de coste')\n",
    "plt.xlabel('nº iteraciones')\n",
    "plt.ylabel('coste')\n",
    "\n",
    "plt.plot([...])    # Completar\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3a720-c473-4e29-b3c7-0bda5bea5bbb",
   "metadata": {},
   "source": [
    "Para comprobar completamente la implementación de dichas funciones, modifica el dataset sintético original para comprobar que la función de coste y el entrenamiento por gradient descent siguen funcionando correctamente.\n",
    "\n",
    "P. ej., modifica el nº de ejemplos y el nº de características.\n",
    "\n",
    "También añádele de nuevo un término de error a la Y. En este caso, puede que la Theta inicial y la final no concuerden del todo, ya que hemos introducido error o \"ruido\" en el dataset de entrenamiento.\n",
    "\n",
    "Por último, comprueba todos los hiper-parámetros de tu implementación. Utiliza varios valores de alpha, e, nº de iteraciones, etc., y comprueba que los resultados son los esperados."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
