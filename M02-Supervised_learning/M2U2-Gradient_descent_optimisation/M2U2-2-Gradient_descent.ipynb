{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a483fea-5052-486a-a650-5801d6883aa1",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression: Gradient descent\n",
    "M2U2 - Exercise 2\n",
    "\n",
    "## What are we going to do?\n",
    "- Implement the optimization of the cost function using gradient descent, or in other words, training the model\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This exercise is a continuation of the previous exercise \"Cost function\", so you should build on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43af36-a639-4381-8424-72241955e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed96c4-774a-4e0b-88a2-9c23c7f9181b",
   "metadata": {},
   "source": [
    "## Task 1: Implement the cost function for multivariate linear regression\n",
    "\n",
    "In this task, you must copy the corresponding cell from the previous exercise, bringing your code to implement the vectorised cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300d424-c3ba-47d5-9254-65716fbd1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the vectorised cost function following the template below\n",
    "\n",
    "def cost_function(x, y, theta):\n",
    "    \"\"\" Compute the cost function for the considered dataset and coefficients.\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the values of the independent variables from the examples, of size m x n\n",
    "    y -- Numpy 1D array with the dependent/target variable, of size m x 1\n",
    "    theta -- Numpy 1D array with the weights of the model coefficients, of size 1 x n (row vector)\n",
    "    \n",
    "    Return:\n",
    "    j -- float with the cost for this theta array\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Remember to check the dimensions of the matrix multiplication to do it correctly\n",
    "    j = [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09977c-81e4-44d3-babf-e4a6ee0a9fb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2: Implement the optimisation of this cost function using gradient descent\n",
    "\n",
    "We are now going to solve the optimisation of this cost function to train the model, using the vectorised gradient descent method. The model will be considered trained when its cost function has reached a minimum, stable value.\n",
    "\n",
    "$$Y = h_\\Theta(X) = X \\times \\Theta^T$$\n",
    "\n",
    "$$J_\\theta = \\frac{1}{2m} \\sum_{i = 0}^{m} (h_\\theta(x^i) - y^i)^2$$\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i = 0}^{m}{(h_\\theta(x^i) - y^i) x_j^i}]$$\n",
    "\n",
    "To do this, once again, fill in the code template in the next cell.\n",
    "\n",
    "Tips:\n",
    "- If you prefer, you can first implement the function with loops and iterations, and finally in a vectorised way\n",
    "- Remember the dimensions of each vector/matrix\n",
    "- Again, record the operations in step-by-step order on a sheet or in an auxiliary cell\n",
    "- At each step, write down the dimensions of your result, which you can also check in your code\n",
    "- Use numpy.matmul() for matrix multiplication\n",
    "- At the start of each training iteration, you must copy all $\\Theta$ values, since you are going to iterate by updating each of its values based on the entire vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0980f-0796-4f79-aed6-d02f9e95d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa la función que entrena el modelo por gradient descent\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha, e, iter_):\n",
    "    \"\"\" Entrena el modelo optimizando su función de coste por gradient descent\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    alpha -- float, ratio de entrenamiento\n",
    "    \n",
    "    Argumentos nombrados (keyword):\n",
    "    e -- float, diferencia mínima entre iteraciones para declarar que el entrenamiento ha convergido finalmente\n",
    "    iter_ -- int/float, nº de iteraciones\n",
    "    \n",
    "    Devuelve:\n",
    "    j_hist -- list/array con la evolución de la función de coste durante el entrenamiento\n",
    "    theta -- array de Numpy con el valor de theta en la última iteración\n",
    "    \"\"\"\n",
    "    # TODO: declara unos valores por defecto para e e iter_ en los argumentos nombrados (keyword) de la función\n",
    "    \n",
    "    iter_ = int(iter_)    # Si has declarado iter_ en notación científica (1e3) o float (1000.), conviértelo\n",
    "    \n",
    "    # Inicializa j_hist como una list o un array de Numpy. Recuerda que no sabemos qué tamaño tendrá finalmente\n",
    "    # Su nº máx. de elementos será el nº máx. de iteraciones\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...]    # Obtén m y n a partir de las dimensiones de X\n",
    "    \n",
    "    for k in [...]:    # Itera sobre el nº de iteraciones máximo\n",
    "        theta_iter = [...]    # Copia con \"deep copy\" la theta para cada iteración, ya que debemos actualizarla\n",
    "        \n",
    "        for j in [...]:    # Itera sobre el nº de características\n",
    "            # Actualiza theta_iter para cada característica, según la derivada de la función de coste\n",
    "            # Incluye el ratio de entrenamiento alpha\n",
    "            # Cuidado con las multiplicaciones matriciales, su órden y dimensiones\n",
    "            theta_iter[j] = theta[j] - [...]\n",
    "            \n",
    "        theta = theta_iter    # Actualiza toda la theta, lista para la siguiente iteración\n",
    "        \n",
    "        cost = cost_function([...])    # Calcula el coste para la iteración de theta actual\n",
    "        \n",
    "        j_hist[...]    # Añade el coste de la iteración actual al histórico de costes\n",
    "        \n",
    "        # Comprueba si la diferencia entre el coste de la iteración actual y el de la última iteración en valor\n",
    "        # absoluto son menores que la diferencia mínima para declarar convergencia, e, para toda iteración\n",
    "        # excepto la primera\n",
    "        if k > 0 and [...]:\n",
    "            print('Converge en la iteración nº: ', k)\n",
    "            \n",
    "            break\n",
    "    else:\n",
    "        print('Nº máx. de iteraciones alcanzado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b469bd3-80f2-4cbe-baa5-6a10c06f4ca1",
   "metadata": {},
   "source": [
    "## Tarea 3: Comprobar la implementación del gradient descent\n",
    "\n",
    "Para comprobar tu implementación, de nuevo, utiliza la misma celda variando sus parámetros varias veces, representando gráficamente la evolución de la función de coste y viendo cómo su valor va acercándose a 0.\n",
    "\n",
    "En cada caso, comprueba que la $\\Theta$ inicial y final son muy similares en los siguientes escenarios:\n",
    "1. Genera varios datasets sintéticos, comprobando cada uno\n",
    "1. Modifica el nº de ejemplos y características, m y n\n",
    "1. Modifica el parámetro de error, lo que puede hacer que la $\\Theta$ inicial y final no concuerden del todo, y a mayor error más diferencia puede haber\n",
    "1. Comprueba los hiper-parámetros del nº máx. de iteraciones o el ratio de entrenamiento $\\alpha$, que hará que el modelo tarde más o menos en entrenarse, dentro de unos valores mínimos y máximos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609719f-aab2-4ea3-94c5-6c05c72deabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un dataset sintético, con término de error, de la forma que escojas, con Numpy o Scikit-learn\n",
    "\n",
    "m = 0\n",
    "n = 0\n",
    "e = 0.\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_verd = [...]\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Comprueba los valores y dimensiones (forma o \"shape\") de los vectores\n",
    "print('Theta real a estimar:')\n",
    "print()\n",
    "print('shape')\n",
    "\n",
    "print('Primeras 10 filas y 5 columnas de X e Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensiones de X e Y:')\n",
    "print('shape', 'shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d7f8c-ef7e-4358-b027-32f0fac388ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba tu implementación entrenando un modelo sobre el dataset sintético creado previamente\n",
    "\n",
    "# Utiliza una theta iniciada aleatoriamente o la Theta_verd, en función del escenario a comprobar\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "e = 1e-3\n",
    "iter_ = 1e3    # Comprueba que tu función puede admitir valores float o modifícalo\n",
    "\n",
    "print('Hiper-arámetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = gradient_descent([...])\n",
    "\n",
    "print('Tiempo de entrenamiento (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores de la función de coste')\n",
    "print(j_hist[...])\n",
    "print('\\nCoste final:')\n",
    "print(j_hist[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdaderos de Theta y diferencia con valores entrenados:')\n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26243d6-8737-417c-acbb-1ae426dd05d0",
   "metadata": {},
   "source": [
    "Representa gráficamente el histórico de la función de coste para comprobar tu implementación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdecba-cf90-4d35-99b7-5da9cc2b00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente la función de coste vs el nº de iteraciones\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Función de coste')\n",
    "plt.xlabel('nº iteraciones')\n",
    "plt.ylabel('coste')\n",
    "\n",
    "plt.plot([...])    # Completar\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3a720-c473-4e29-b3c7-0bda5bea5bbb",
   "metadata": {},
   "source": [
    "Para comprobar completamente la implementación de dichas funciones, modifica el dataset sintético original para comprobar que la función de coste y el entrenamiento por gradient descent siguen funcionando correctamente.\n",
    "\n",
    "P. ej., modifica el nº de ejemplos y el nº de características.\n",
    "\n",
    "También añádele de nuevo un término de error a la Y. En este caso, puede que la Theta inicial y la final no concuerden del todo, ya que hemos introducido error o \"ruido\" en el dataset de entrenamiento.\n",
    "\n",
    "Por último, comprueba todos los hiper-parámetros de tu implementación. Utiliza varios valores de alpha, e, nº de iteraciones, etc., y comprueba que los resultados son los esperados."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
