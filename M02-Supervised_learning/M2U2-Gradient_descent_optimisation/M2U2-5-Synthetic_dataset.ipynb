{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9afd25-e3cb-4641-883a-a8e708e2a736",
   "metadata": {},
   "source": [
    "# Linear Regression: Synthetic dataset example\n",
    "M2U2 - Exercise 5\n",
    "\n",
    "## What are we going to do?\n",
    "- Use an automatically generated synthetic dataset to check our implementation\n",
    "- Train a multivariate linear regression ML model\n",
    "- Check the training evolution of the model\n",
    "- Evaluate a simple model\n",
    "- Make predictions about new future examples\n",
    "\n",
    "Remember to follow the instructions for the submission of assignments indicated in [Submission Instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8adcb2-aeae-4431-98d0-3f157acf9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12947e67-26b6-448d-9714-5bfa36be3250",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creation of a synthetic dataset\n",
    "\n",
    "We are going to create a synthetic dataset to check our implementation.\n",
    "\n",
    "Following the methods that we have used in previous exercises, create a synthetic dataset using the NumPy method.\n",
    "\n",
    "Include a controllable error term in that dataset, but initialise it to 0, since to make the first implementation of this multivariate linear regression ML model we do not want any error in the data that could hide an error in our model.\n",
    "\n",
    "Afterwards, we will introduce an error term to check that our implementation can also train the model under these more realistic circumstances.\n",
    "\n",
    "### The bias or intercept term\n",
    "\n",
    "This time, we are going to generate the synthetic dataset with a small modification: we are going to add a first column of 1s to X, or a 1. (float) as the first value of the features of each example.\n",
    "\n",
    "Furthermore, since we have added one more feature n to the matrix X, we have also added one more feature or value to the vector $\\Theta$, so we now have n + 1 features.\n",
    "\n",
    "Why do we add this column, this new term or feature?\n",
    "\n",
    "Because this is the simplest way to implement the linear equation in a single linear algebra operation, i.e., to vectorise it.\n",
    "\n",
    "In this way, we thus convert $Y = m \\times X + b$ en $Y = X \\times \\Theta$, saving us an addition operation and implementing the equation in a single matrix multiplication operation.\n",
    "\n",
    "The term *b*, therefore, is incorporated as the first term of the vector $\\Theta$, which when multiplied by the first column of X, which has a value of 1 for all its rows, allows us to add said term *b* to each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87898f6d-d055-47e1-a732-ba32515bcde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a synthetic dataset in whatever way you choose, with error term initially set to 0\n",
    "\n",
    "m = 100\n",
    "n = 3\n",
    "\n",
    "# Create a matrix of random numbers in the interval [-1, 1)\n",
    "X = [...]\n",
    "# Insert a vector of 1s as the 1st column of X\n",
    "# Tips: np.insert(), np.ones(), index 0, axis 1...\n",
    "X = [...]\n",
    "\n",
    "# Generate a vector of random numbers in the interval [0, 1) of size n + 1 (to add the bias term)\n",
    "Theta_true = [...]\n",
    "\n",
    "# Add to the Y vector a random error term in % (0.1= 10%) initialised at 0\n",
    "# Said term represents an error of +/- said percentage, e.g., +/- 5%,+/- 10%, etc., not just to add\n",
    "# The percentage error is calculated on Y, therefore the error would be e.g., +3.14% of Y, or -4.12% of Y....\n",
    "error = 0.\n",
    "\n",
    "Y = np.matmul(X, Theta_true)\n",
    "Y = Y + [...] * error\n",
    "\n",
    "# Check the values and dimensions of the vectors\n",
    "print('Theta to be estimated and its dimensions:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('First 10 rows and 5 columns of X and Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensions of X and Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea2b0c-65c3-44fe-bd4a-7893e439c4ee",
   "metadata": {},
   "source": [
    "Note the matrix multiplication operation implemented: $Y = X \\times \\Theta$\n",
    "\n",
    "Check the dimensions of each vector: X, Y, $\\Theta$.\n",
    "*Do you think this operation is possible according to the rules of linear algebra?*\n",
    "\n",
    "If you have doubts, you can consult the NumPy documentation relating to the np.matmul function.\n",
    "\n",
    "Check the result, perhaps reducing the original number of examples and features, and make sure it is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9c1d5-c55b-4738-b6e7-a311bb352715",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Copy your implementation of the cost function and its optimisation by gradient descent from the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a626cc6-67cb-477f-b624-0908ddd4bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the code of your cost and gradient descent functions\n",
    "\n",
    "def cost_function(x, y, theta):\n",
    "    \"\"\" Computes the cost function for the considered dataset and coefficients.\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the values of the independent variables from the examples, of size m x n +1\n",
    "    y -- Numpy 1D array with the dependent/target variable, of size m x 1\n",
    "    theta -- Numpy 1D array with the weights of the model coefficients, of size 1 x n +1 (row vector)\n",
    "    \n",
    "    Return:\n",
    "    j -- float with the cost for this theta array\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha, e, iter_):\n",
    "    \"\"\" Train the model by optimising its cost function by gradient descent\n",
    "    \n",
    "    Positional arguments:\n",
    "    x -- Numpy 2D array with the values of the independent variables from the examples, of size m x n +1\n",
    "    y -- Numpy 1D array with the dependent/target variable, of size m x 1\n",
    "    theta -- Numpy 1D array with the weights of the model coefficients, of size 1 x n +1 (row vector)\n",
    "    alpha -- float, training rate\n",
    "    \n",
    "    Named arguments (keyword):\n",
    "    e -- float, minimum difference between iterations to declare that the training has finally converged\n",
    "    iter_ -- int/float, nº of iterations\n",
    "    \n",
    "    Return:\n",
    "    j_hist -- list/array with the evolution of the cost function during training, of size nº of iterations that the model has used\n",
    "    theta -- NumPy array with the value of theta at the last iteration, of size 1 x n + 1\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b83ca-7bdb-42e6-9320-7a0f3eeec93c",
   "metadata": {},
   "source": [
    "We will use these functions to train our ML model.\n",
    "\n",
    "Let's remind you of the steps we will follow:\n",
    "- Start $\\Theta$ with random values\n",
    "- Optimise $\\Theta$ by reducing the cost associated with each iteration of its values\n",
    "- When we have found the minimum value of the cost function, take its associated $\\Theta$ as the coefficients of our model\n",
    "\n",
    "To do this, fill in the code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652fdc77-da49-471a-b986-73be8ec6d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your ML model by optimising its Theta coefficients using gradient descent\n",
    "\n",
    "# Initialise theta with n + 1 random values\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta initial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "e = 1e-4\n",
    "iter_ = 1e5\n",
    "\n",
    "print('Hyperparameters to be used:')\n",
    "print('Alpha: {}, e: {}, max nº. iter: {}'.format(alpha, e, iter_))\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta = gradient_descent([...])\n",
    "\n",
    "print('Training time (s):', time.time() - t)\n",
    "\n",
    "# TODO: complete\n",
    "print('\\nLast 10 values of the cost function')\n",
    "print(j_hist[...])\n",
    "print('\\Final cost:')\n",
    "print(j_hist[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta)\n",
    "\n",
    "print('True values of Theta and difference with trained values:')\n",
    "print(Theta_true)\n",
    "print(theta - Theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c814b-faf2-488b-add1-5448b5d3c793",
   "metadata": {},
   "source": [
    "Check that the initial $\\Theta$ has not been modified. Your implementation must copy a new Python object at each iteration and not modify it during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df9748-858d-4ac8-b3b0-17140e921436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check that the initial Theta has not been modified\n",
    "\n",
    "print('Theta initial y theta final:')\n",
    "print(theta_ini)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50ac66-c917-4e73-a6d1-c7216938e9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check the training of the model\n",
    "\n",
    "To check the training of the model, we will graphically represent the evolution of the cost function, to ensure that there has not been any great jump and that it has been steadily moving towards a minimum value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e993a2b-95ce-435e-a5e0-7aa5ab1ad898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the evolution of the cost function vs. the number of iterations\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "plt.title('Función de coste')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Coste')\n",
    "\n",
    "plt.plot([...])    # Completa los argumentos\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f2ca5-665f-4808-9bb0-ec212e033fd7",
   "metadata": {},
   "source": [
    "## Realizar predicciones\n",
    "\n",
    "Vamos a utilizar la $\\Theta$, el resultado de nuestro proceso de entrenamiento del modelo, para realizar predicciones sobre nuevos ejemplos que llegaran en el futuro.\n",
    "\n",
    "Generaremos un nuevo conjunto de datos X siguiendo los mismos pasos que hemos seguido anteriormente. Por tanto, si X tiene el mismo nº de características (n + 1) y sus valores están en el mismo rango de la X generada previamente, se comportarán igual que los datos usados para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c39c63-05a6-4c85-89e5-16187c4a59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Realiza predicciones usando la theta calculada\n",
    "\n",
    "# Genera una nueva matriz X con nuevos ejemplos. Usa el mismo nº de características y el mismo rango de valores\n",
    "# aleatorios, pero un nº de ejemplos menor (p. ej., 25% del original)\n",
    "# Recuerda añadir el término bias, o una primera columna de 1s a la matriz, de tamaño m x n + 1\n",
    "X_pred = [...]\n",
    "\n",
    "# Calcula las predicciones para dichos nuevos datos\n",
    "y_pred = [...]    # Pista: de nuevo, matmul\n",
    "\n",
    "print('Predicciones:')\n",
    "print(y_pred)    # Puedes imprimir todo el vector o sólo los primeros valores, si es demasiado largo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e231f49-9179-411c-b34e-4548f3be6f01",
   "metadata": {},
   "source": [
    "## Evaluación del modelo\n",
    "\n",
    "Para evaluar el modelo tenemos varias opciones. En este punto, vamos a hacer una evaluación más simple, rápida e informal del mismo. En siguientes módulos del curso veremos cómo evaluar nuestros modelos de una forma más formal y precisa.\n",
    "\n",
    "Vamos a hacer una evaluación gráfica, para comprobar simplemente que nuestra implementación funciona como esperamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33bd75-c4a1-41b1-b363-d91333bacb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente los residuos entre la Y inicial y la Y predicha para los mismos ejemplos\n",
    "\n",
    "# Realiza predicciones para cada valor de la X original con la theta entrenada por el modelo\n",
    "Y_pred = [...]\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.title('Dataset original y predicciones')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Residuos')\n",
    "\n",
    "# Calcula los residuos para cada ejemplo\n",
    "# Recuerda que son la diferencia en valor absoluto entre la Y real y la Y predicha para cada ejemplo\n",
    "residuos = [...]\n",
    "\n",
    "# Usa una gráfica con series diferentes: Y de entrenamiento, Y predicha y residuos\n",
    "# Usa una gráfica de puntos para la Y de entrenamiento, de línea para la Y predicha y de barra para los residuos, superpuestas\n",
    "plt.scatter([...])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807967f-7d35-456b-8cf7-a76e25bbc041",
   "metadata": {},
   "source": [
    "Si nuestra implementación es correcta, nuestro modelo debe haber podido entrenarse correctamente y tener unos resíduos prácticamente nulos, una diferencia prácticamente nula entre los resultados originales (Y) y los resultados que calcularía nuestro modelo.\n",
    "\n",
    "Sin embargo, como recordamos, en el primer punto hemos creado un dataset con el término de error a 0. Por tanto, cada valor de Y no tiene ninguna diferencia o variación aleatoria sobre su valor real.\n",
    "\n",
    "En la vida real, sea porque no hemos tenido en cuenta todas las características que afectarían a nuestra variable objetivo, sea porque los datos contienen algún pequeño error, o sea porque, por lo general, los datos no siguen un comportamiento completamente preciso, siempre tendremos algún término de error, más o menos aleatorio.\n",
    "\n",
    "Por tanto, *¿y si vuelves a la primera celda y modificas tu término de error, y ejecutas de nuevo las siguientes para entrenar y evaluar un nuevo modelo de regresión lineal sobre datos más parecidos a la realidad?*\n",
    "\n",
    "Comprueba de dicha forma la robustez de tu implementación."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
