{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb77f4a-5b33-473c-83d3-dfff59c18787",
   "metadata": {},
   "source": [
    "# Gradient descent by batch vs. mini-batch vs. stochastic\n",
    "M5U5 - Exercise 2\n",
    "\n",
    "## What are we going to do?\n",
    "- Modify our batch gradient descent implementation from batch to mini-batch and stochastic\n",
    "- Test the differences between the training according to the 3 methods\n",
    "\n",
    "Remember to follow the instructions for the practice deliverables given in [Submission instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "So far we have talked about training models or optimising functions by gradient descent. However, we have omitted to refer to \"batch\" gradient descent, which can be distinguished from mini-batch and stochastic gradient descent.\n",
    "\n",
    "For a detailed comparison of the 3 types you can refer to the course content. Just a reminder:\n",
    "- \"Batch\" = training data set for 1 iteration or \"epoch\".\n",
    "- Iteration or \"epoch\": iteration over training, loop after which the weights of $\\Theta$.\n",
    "- Batch: iteration or \"epoch\" over all training data before updating $\\Theta$.\n",
    "    - Slow but steady, eventually converges.\n",
    "- Stochastic: One iteration per training example.\n",
    "    - Fast at start but very unstable, takes much longer to converge. Cannot be parallelised.\n",
    "    - \"Stochastic\" as it is much more random in its path.\n",
    "- Mini-batch: iteration per partition of training data, e.g. 10% of data or 10 partitions.\n",
    "    - Best of both worlds: faster than batch, more stable than stochastic, converges and can be parallelised.\n",
    "\n",
    "We will implement all 3 types, either manually or customised with Numpy or Scikit-learn, and compare their characteristics, in this case for **linear regression**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95baf0b-004f-4d83-b3c0-adeeb8ef64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import all necessary libraries into this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c018a1-e07f-4ad9-8999-8a5bfa265d4c",
   "metadata": {},
   "source": [
    "## Synthetic dataset generation and data processing\n",
    "\n",
    "Retrieve your cells to create a synthetic dataset for linear regression, with Numpy or Scikit-learn methods:\n",
    "- Create a dataset with no error term\n",
    "- Rearrange the data randomly\n",
    "- Normalise the data if necessary\n",
    "- Split the dataset into training and test subsets, we will not do validation or regularisation in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8039c-69a3-427d-8739-47b92294479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a synthetic dataset for linear regression with no error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5134b084-dcc4-4d8c-837c-a78580973233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rearrange the data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c1240-5fa7-4b50-b239-b24d8172b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749814f-62d3-433b-a551-cf64f82a4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide the dataset into training and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7eacb-b653-4d21-a916-08e54a92ef35",
   "metadata": {},
   "source": [
    "## Customised gradient descent\n",
    "\n",
    "### Batch gradient descent\n",
    "\n",
    "Recall the cost function and gradient descent equations for the regularised batch gradient descent:\n",
    "\n",
    "$$ h_\\theta(x^i) = Y = X \\times \\Theta^T $$\n",
    "$$ J_\\theta = \\frac{1}{2m} [\\sum\\limits_{i=0}^{m} (h_\\theta(x^i)-y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j] $$\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$\n",
    "\n",
    "We are going to retrieve the batch gradient descent implementation you have used in previous exercises to take it as the basis for the mini-batch or stochastic gradient descent.\n",
    "\n",
    "Start by retrieving the implementation cells of the cost function, its implementation check, the regularised gradient descent, the training of a model and its implementation check.\n",
    "\n",
    "Once retrieved, it executes the cells, adding the suffix `_batch` to the variables of the cost function evolution and final $\\Theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a05ff9-8102-41ce-8663-410a3d91f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Retrieve the cell that implements the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f1660-1207-47b9-8a78-e57de24adbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Retrieve the cell that checks the implementation of the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca227b-443f-44cd-9de7-b38eef903624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Retrieve the cell that implements the gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13618f49-0c9c-46a4-af5a-9ec8ce28d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Retrieve the cell that trains a model with a training dataset and a given hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261b197-0204-4921-b5b5-42b7aca14070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Retrieve the cell that tests the implementation of the gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a76627-b7d9-4db2-93a2-4141bdf45a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Retrieve the cell that graphically represents the evolution of the cost function history vs. iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae4c9f-d67f-475d-9d5a-807338cd27ae",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "In the stochastic gradient descent, we update the values of $\\Theta$ after each example, ending an epoch when we complete one pass through all examples.\n",
    "\n",
    "Therefore, the training algorithm will be:\n",
    "1. Reorder the examples randomly (we have already reordered them).\n",
    "1. Initialise $\\Theta$ to random values.\n",
    "1. For each epoch, up to a maximum number of iterations:\n",
    "    1. For each training example:\n",
    "        1. Compute the prediction or hypothesis $h_\\Theta(x^i)$\n",
    "        1. Compute the cost, loss or error of that prediction\n",
    "        1. Compute the gradients of the coefficients $\\Theta$\n",
    "        1. Update the coefficients $\\Theta$\n",
    "\n",
    "Therefore, the regularised stochastic gradient descent and cost function equations are:\n",
    "\n",
    "$$ h_\\theta(x^i) = y^i = x^i \\times \\Theta^T $$\n",
    "$$ J_\\theta(x^i) = \\frac{1}{2m} [(h_\\theta(x^i) - y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j] $$\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} (h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} (h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c27ff7-04e6-4985-b06a-6ca60b396bef",
   "metadata": {},
   "source": [
    "Now adapt your model training cell for stochastic gradient descent and train a model on the training data:\n",
    "\n",
    "*NOTE:* Try to use the same hyper-parameters and initial Theta for all models, so that you can compare them a posteriori under the same circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cca92-56ef-425e-ab0c-18a6b5721d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adapt the regularised gradient descent function to stochastic\n",
    "# NOTE: Check the implementation first before modifying it. Many changes may not be necessary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8764c690-d0e5-491a-87f1-8b20e30ec620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a stochastic gradient descent model\n",
    "# Add the suffix \"_stochastic\" to the outcome variables to distinguish it from other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c546362-d871-4413-8aa7-62671c3ece97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the implementation of stochastic gradient descent under various circumstances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbff44-48d1-4b22-9334-7c44c4c2ed9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Plot the evolution of the cost function graphically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e20112-4ae5-4939-baca-92ca07ea4576",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "In gradient descent with mini-batches, we update the values of $\\Theta$ after each subset of examples or \"batch\", a partition of the training subset, ending an epoch when we complete one pass through all the \"batches\" or examples.\n",
    "\n",
    "Therefore, the training algorithm will be:\n",
    "1. Reorder the examples randomly (we have already reordered them).\n",
    "1. For each epoch, up to a maximum number of iterations:\n",
    "    1. Initialise $\\Theta$ to random values.\n",
    "    1. Divide the training examples into *k* \"batches\".\n",
    "    1. For each \"batch\":\n",
    "        1. Compute the prediction or hypothesis $h_\\Theta(x^i)$ over the entire \"batch\"\n",
    "        1. Compute the cost, loss or error of the prediction over it.\n",
    "        1. Compute the gradients of the coefficients $\\Theta$\n",
    "        1. Update the coefficients $\\Theta$\n",
    "\n",
    "Therefore, the equations of the cost function and gradient descent with regularised mini-batches are:\n",
    "\n",
    "$$ m_k = \\text{number of examples in the current \"batch\"} $$\n",
    "$$ h_\\theta(x^i) = Y = X \\times \\Theta^T $$\n",
    "$$ J_\\theta = \\frac{1}{2 m_k} [\\sum\\limits_{i=0}^{m_k} (h_\\theta(x^i)-y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j] $$\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m_k} \\sum_{i=0}^{m_k}(h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m_k}) - \\alpha \\frac{1}{m_k} \\sum_{i=0}^{m_k}(h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4373f-8917-4e94-850c-42e70b3bc22e",
   "metadata": {},
   "source": [
    "Now adapt your model training cell for stochastic gradient descent and train a model on the training data:\n",
    "\n",
    "*NOTE:* Try to use the same hyper-parameters and initial Theta for all models, so that you can compare them a posteriori under the same circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18be36-181e-4c72-ac57-2d8eda05cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adapt the regularised gradient descent function to mini-batch\n",
    "# NOTE: Check the implementation first before modifying it. You may not need to make many changes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60973b-058e-420a-8c2b-3aec81195c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a model by gradient descent with mini-batches\n",
    "# Add the suffix \"_mini_batch\" to result variables to distinguish it from other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e6fdc-76b5-4d1a-9517-18bca4e135ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the implementation of gradient descent with mini-batches in various circumstances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0b4a8-98f8-4e0f-a74b-7747f6dd87a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Plot the evolution of the cost function graphically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da8cb5-f932-4be1-ad00-9b654a5878af",
   "metadata": {},
   "source": [
    "## Comparación de métodos\n",
    "\n",
    "Responde a las siguientes preguntas en la siguiente celda:\n",
    "*PREGUNTAS:*\n",
    "1. *¿Cuánto era necesario modificar las funciones de descenso de gradiente?*\n",
    "1. *¿Qué modelo ha tenido menor coste final?*\n",
    "1. *¿Qué modelo ha tardado menos tiempo en entrenarse/converger?*\n",
    "1. *¿Cómo han sido las evoluciones de la función de coste? ¿Han sido comparables en cuanto a estabilidad, p. ej.?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91fd760-2566-4e9b-97ea-07a88e783069",
   "metadata": {},
   "source": [
    "*RESPUESTAS:*\n",
    "1. ...\n",
    "1. ...\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e701d-2581-479e-85f3-c5cc3ec06a5c",
   "metadata": {},
   "source": [
    "### Comparación de residuos y precisión\n",
    "\n",
    "Calcula la precisión como RMSE y representa gráficamente los residuos de los 3 modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce243d-eff2-48be-bbc7-71b225102b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula el RMSE de los 3 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc10e0-17ed-4762-9b39-a7b4ff0274ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente los residuos de los 3 modelos\n",
    "# Usa una gráfica de puntos con 3 series de colores diferentes y su leyenda\n",
    "# Incluye una rejilla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb610a1-2425-4c0c-aeb4-888b304cc27a",
   "metadata": {},
   "source": [
    "*PREGUNTA:* ¿Aprecias diferencias entre ellos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7833fe-ace6-4523-8e05-4be630ab757e",
   "metadata": {},
   "source": [
    "## Gradient descent con Scikit-learn\n",
    "\n",
    "Ahora entrena 3 modelos y compara su rendimiento utilizando los métodos de Scikit-learn, en concreto regresión lineal por [linear_model.SGDRegressor](https://scikit-learn.org/0.15/modules/generated/sklearn.linear_model.SGDRegressor.html) con sus métodos `fit()` y [partial_fit()](https://scikit-learn.org/0.15/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor.partial_fit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18d642-93de-4664-bbf5-2eded37c99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por descenso de gradiente en batch con Scikit-learn\n",
    "# Añade el sufijo \"_batch\" a las variables resultado para distinguirlo de otros modelos\n",
    "# Muestra su tiempo de entrenamiento\n",
    "# Calcula su coste y RMSE final\n",
    "# Representa gráficamente sus residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c47c6-0147-45f0-98f2-5249d470c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por descenso de gradiente estocástico con Scikit-learn\n",
    "# Añade el sufijo \"_estocastico\" a las variables resultado para distinguirlo de otros modelos\n",
    "# Muestra su tiempo de entrenamiento\n",
    "# Calcula su coste y RMSE final\n",
    "# Representa gráficamente sus residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c4f4a-3fe0-47ad-803c-467eb2562274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por descenso de gradiente coni mini-batches con Scikit-learn\n",
    "# Añade el sufijo \"_mini_batch\" a las variables resultado para distinguirlo de otros modelos\n",
    "# Muestra su tiempo de entrenamiento\n",
    "# Calcula su coste y RMSE final\n",
    "# Representa gráficamente sus residuos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40808a1-4c94-4cd3-8df2-2558577e73bf",
   "metadata": {},
   "source": [
    "*PREGUNTA:* ¿Aprecias diferencias entre ellos?"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
