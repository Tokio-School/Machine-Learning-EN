{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671f2138-334b-4dbe-8465-c110632bf188",
   "metadata": {},
   "source": [
    "# Model evaluation and improvement: Choice of polyomial degrees\n",
    "M5U5 - Exercise 1\n",
    "\n",
    "## What are we going to do?\n",
    "- Transform features to apply polynomials of varying degrees\n",
    "- Identify the suitable polynomial degree for each feature\n",
    "- Identify when we suffer deviation or over-fitting due to using the wrong degree polynomial\n",
    "\n",
    "Remember to follow the instructions for the practice deliverables given at [Submission instructions](https://github.com/Tokio-School/Machine-Learning-EN/blob/main/Submission_instructions.md).\n",
    "\n",
    "## Instructions\n",
    "In many cases, various effects of predictor variables in nature have an influence on the target variable that is not linear, but this linearity can be modelled by transforming the original data. Some of these effects, and therefore transformations, are polynomial, square root, logarithmic, etc.\n",
    "\n",
    "E.g. sunlight, temperature, temporal effects with daily cycles, etc., have a polynomial effect on animals, plants, etc.\n",
    "\n",
    "In this exercise we are going to see how we can transform our data to model a system, train a model, of linear type on non-linear data, but which we can convert to be linear, and therefore we can solve for linear models such as linear regression or linear logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3cdc3b-2a3d-4515-84dd-495299872973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import all the necessary libraries in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715da28-1a2d-47c5-8dd4-bdf3c11c97ce",
   "metadata": {},
   "source": [
    "## Polynomial characteristics\n",
    "\n",
    "One of the most common effects are polynomials. Find out more about polynomials and their degrees: [Polinomial](https://en.wikipedia.org/wiki/Polynomial).\n",
    "\n",
    "P. ej., modelo lineal con un Ãºnico predictor *X* modelizado por un polinomio de grado 3: $Y = \\theta_0 + \\theta_1 \\times X + \\theta_2 \\times X^2 + \\theta_3 \\times X^3 $\n",
    "\n",
    "In this case, with a single predictor *X*, instead of taking just that predictor we take other features from it, transforming it by squaring and cubing it. We take one feature, and get 2 more from it.\n",
    "\n",
    "To identify these effects in our datasets, it is important to become familiar with the characteristic graphical form of the most common ones.\n",
    "\n",
    "Plot multi-degree polynomials, play with their parameters and study their resulting characteristic forms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd037a-6079-44a3-9715-5a38bec8427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot multiple polynomial graphs\n",
    "\n",
    "#  Create an ndarray with a linear space of 100 points between [0, 100] which we will use as X, predictor variable, and the horizontal axis of the graph\n",
    "x = [...]\n",
    "\n",
    "# Create ndarrays with the transformations by raising said X to degrees 2 to 6\n",
    "for degree in [...]:\n",
    "    term = [...]    # Calculate the corresponding term by raising x to that degree\n",
    "    # Concatenate that term to x as a new column, horizontally, using np.concatenate()\n",
    "    [...]\n",
    "\n",
    "# Plot such polynomials as dot and line plots as a series of different colours\n",
    "# Add a grid, title and reading for the series\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92bfe3d-e28c-4085-9a8a-82609e750707",
   "metadata": {},
   "source": [
    "## Creating the dataset\n",
    "\n",
    "Once the polynomial effects have been graphically explored, we are going to build a synthetic dataset with high degree polynomial effects, which we will have to solve by transforming our data and testing various polynomial degrees.\n",
    "\n",
    "The process we are going to follow to generate the dataset, therefore, is the following:\n",
    "1. Generate a dataset with 7 features, composed of a pseudo-random *X*, $X^2, X^3, ..., X^6$\n",
    "1. Generate some pseudo-random $\\Theta$ coefficients/weights\n",
    "1. Complete the dataset by generating a Y from some of the features of *X* and $\\Theta$, to a given degree\n",
    "1. Add an error parameter or white/Gaussian noise to *Y*\n",
    "\n",
    "To obtain *Y* we will not use all the $n + 1$ features of *X*, but only up to a given degree, o that we can train several models using more or less characteristics of *X* until we find the optimal polynomial degree, neither too much nor too little.\n",
    "\n",
    "Once generated, as usual, our goal for practice will be to explore how we can transform our data to model an originally non-linear dataset, by linear models, in order to obtain $\\Theta$ and be able to generate new predictions with our model.\n",
    "\n",
    "We generate a dataset with more features than those used to calculate *Y* so that we have the flexibility to use more or less in the future.\n",
    "\n",
    "Build on your manual dataset generation code (not Scikit-learn methods) from previous exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e79f6-8974-460f-9a18-94da80a6a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a dataset with polynomial effects up to degree 6\n",
    "m = 100\n",
    "\n",
    "# Generate an X of m pseudo-random values in the range [0, 1)\n",
    "X_true = [...]\n",
    "\n",
    "# Concatenate 5 new columns/characteristics to X with corresponding degree terms ([2, 6])\n",
    "for grade in [...]:\n",
    "    term = [...]    # Calculates the corresponding term by raising X to that degree\n",
    "    # Concatenate that term to X as a new column, horizontally, using np.concatenate()\n",
    "    [...]\n",
    "\n",
    "# Inserts a column of 1. to the left of X as a bias term\n",
    "X_true = [...]\n",
    "\n",
    "# What would be the n or number of features/dimensions of this dataset?\n",
    "n = [...]\n",
    "\n",
    "# Generate a pseudo-random true Theta ndarray [0, 1) 1D of size (n + 1,)\n",
    "Theta_true = [...]\n",
    "\n",
    "# Calculate the Y corresponding to X and true Theta with the first 4 features of X, i.e. with a polynomial up to degree 3 (b + X + X^2 + X^3)\n",
    "# Use the first 4 columns of X and true Theta\n",
    "Y = [...]\n",
    "\n",
    "# Add a white/Gaussian error term as a +/-e percentage added to Y\n",
    "# Make sure to generate pseudo-random numbers from a normal or Gaussian distribution\n",
    "e = 0.15\n",
    "\n",
    "Y = Y + [...]\n",
    "\n",
    "# Check the values and dimensions of X and Y\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0ff8b-0c2c-4b20-b8e8-27f0ef434c7e",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Once the base dataset is generated, we are going to generate a different training dataset. The reason for generating a different one is to simulate all the steps in the same way we would do in reality, starting from the same point, of only having one predictor or feature for *X* and one *Y*, having to generate new transformed features since we would not know which degree of the polynomial would be the correct one for each feature (here we only consider a base predictor), not even if there is a polynomial effect or not.\n",
    "\n",
    "We will generate an *X* iteratively, testing one degree of polynomial, checking and re-testing a different degree, until we get a transformation that when modelling the model we obtain satisfactory results.\n",
    "\n",
    "To do so, start from $X_{verd1}$ ($X_{verd0} = 1$) and generate a dataset *X* with a number of features given by the degree of the polynomial to be checked.\n",
    "\n",
    "Throughout the exercise, you will return to the next code cell and you can re-run it to test a different polynomial grade.\n",
    "\n",
    "To do this, use Scikit-learn's preprocessing methods:\n",
    "- [preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "- [Polynomial features](https://scikit-learn.org/stable/modules/preprocessing.html#polynomial-features).\n",
    "\n",
    "Generate an *X* from $X_{verd1}$ by playing with polynomial transformations of degree from 2 to, in subsequent iterations, 5 or even 6.\n",
    "\n",
    "*NOTE:* Polynomial effects can be of arbitrarily high degree. However, it is most common in nature to find effects up to degree 4, beyond which they are exceptionally rare and therefore also usually considered too extreme in statistical or scientific models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf9904-7cd3-4915-8ed8-3cf26d3f23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a dataset X from X_verd[:, 1] by polynomial transformation with Scikit-learn\n",
    "# NOTE: Beware of the behaviour of PolynomialFeatures(), which adds bias term and polynomial terms for multiple features\n",
    "grade = 2    # In subsequent iterations, modify the degree number of the polynomial\n",
    "\n",
    "X = [...]\n",
    "\n",
    "# Checks the values and dimensions of X and Y\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2c52e-3007-4661-af07-4b7cd9566095",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "As usual, preprocess your dataset before proceeding by randomly reordering the data, normalising it if necessary and splitting it into training and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e5dd8-fca1-4d55-b670-39967819002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess data by reordering it, normalising it and splitting it into training and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80a4b3-3702-44be-b9ad-6900aa231b9d",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We have started with the hypothesis that we can transform our data with a polynomial of degree 2 for a linear model to obtain satisfactory results.\n",
    "\n",
    "Let's train such a model and evaluate its results.\n",
    "\n",
    "Train a linear regression model by cross-validation with [linear_model.RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) and evaluate it with its coefficient of determination $R^2$ of the `model.score()` method on the test subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f578aac-dee0-411d-aa8e-559b6f062d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model hypothesis by CV and evaluate it on the test subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0f480-ef4d-4b6a-b02f-5e06e4b01770",
   "metadata": {},
   "source": [
    "### Evaluation of the residuals\n",
    "\n",
    "Usually, the best way to assess whether we are hypothesising the correct data pattern, in this case a polynomial effect of degree 2, is to explore the residuals of the model.\n",
    "\n",
    "Calculate your residuals on the test subset and plot them graphically:\n",
    "\n",
    "*NOTE:* Remember the definition of residuals, $\\text{residuals} = (Y_{pred} - Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebadef49-b752-475f-8092-f4b44dc4601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate and plot the residuals of the model against the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4445b04-e42c-4925-88e6-90e81d391841",
   "metadata": {},
   "source": [
    "*Do they look acceptable and do they follow a pattern?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c92dc8-79ab-403c-96c4-f0384679af8f",
   "metadata": {},
   "source": [
    "## Iterate until the solution is found\n",
    "\n",
    "We have hypothesised that the optimal degree of polynomial transformation would be 2, but we have not obtained satisfactory results. Therefore, we must iterate, go back, make a new hypothesis of a higher degree, re-run the cells and check the results.\n",
    "\n",
    "In science in general, data science and ML, we must always pose multiple hypotheses, test them and iteratively accept or discard them. To do this, it is essential to document the experiments we have been running and their results.\n",
    "\n",
    "Record the results of your experiments in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848b060-b1d2-4ae8-a074-db20b963b56f",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "1. Polynomial of degree 2: $R^2$ = ...\n",
    "1. Polynomial of degree 3: $R^2$ = ...\n",
    "1. Polynomial of degree 4: $R^2$ = ...\n",
    "1. Polynomial of degree 5: $R^2$ = ...\n",
    "1. Polynomial of degree 6: $R^2$ = ..."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
